{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f8f4d2",
   "metadata": {},
   "source": [
    "# Optimized SBERT_dd.ipynb for memory management (processing in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Optimized Dask Client\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "\n",
    "# Start a local Dask cluster with constrained resources\n",
    "# Reserve ~75% of available RAM for Dask, leaving room for other processes\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,  # Adjust based on your CPU cores\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='4GB'  # 16GB total across 4 workers, leaving 4GB for system\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Theme Dictionary & Optimize Theme Embeddings\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get theme embeddings for specific app IDs\n",
    "# This avoids loading all embeddings at once\n",
    "def get_theme_embeddings(app_ids):\n",
    "    \"\"\"Get theme embeddings for a specific set of app IDs\"\"\"\n",
    "    embeddings = {}\n",
    "    for appid in app_ids:\n",
    "        if appid not in embeddings and appid in GAME_THEMES:\n",
    "            emb_list = []\n",
    "            for theme, seeds in GAME_THEMES[appid].items():\n",
    "                seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "                emb_list.append(seed_emb.mean(axis=0))\n",
    "            embeddings[appid] = np.vstack(emb_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Read Parquet Files with Optimized Blocksize\n",
    "# Read with explicit blocksize optimization\n",
    "ddf = dd.read_parquet(\n",
    "    'parquet_output_theme_combo/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up'],\n",
    "    blocksize='64MB'  # Adjust based on available RAM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11279213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Optimized Partition-wise Topic Assignment\n",
    "def assign_topic(df_partition):\n",
    "    \"\"\"Assign topics using only theme embeddings for app IDs in this partition\"\"\"\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "    \n",
    "    # Get unique app IDs in this partition\n",
    "    app_ids = df_partition['steam_appid'].unique().tolist()\n",
    "    app_ids = [int(appid) for appid in app_ids]\n",
    "    \n",
    "    # Get embeddings only for app IDs in this partition\n",
    "    local_theme_embeddings = get_theme_embeddings(app_ids)\n",
    "    \n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go with batching\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        appid = int(appid)\n",
    "        if appid in local_theme_embeddings:\n",
    "            theme_embs = local_theme_embeddings[appid]\n",
    "            sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "            topic_ids.append(int(sims.argmax()))\n",
    "        else:\n",
    "            # Default topic if theme embeddings not available\n",
    "            topic_ids.append(0)\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Aggregate Counts, Likes, and Collect Reviews per Theme\n",
    "# Process in smaller chunks to avoid memory issues\n",
    "# Get unique app IDs\n",
    "unique_app_ids = ddf['steam_appid'].unique().compute()\n",
    "\n",
    "# Initialize empty dataframes for results\n",
    "all_agg_dfs = []\n",
    "all_review_dfs = []\n",
    "\n",
    "# Process in batches of app IDs\n",
    "batch_size = 5  # Adjust based on your memory constraints\n",
    "for i in tqdm(range(0, len(unique_app_ids), batch_size)):\n",
    "    batch_app_ids = unique_app_ids[i:i+batch_size]\n",
    "    \n",
    "    # Filter data for this batch of app IDs\n",
    "    batch_ddf = ddf_with_topic[ddf_with_topic['steam_appid'].isin(batch_app_ids)]\n",
    "    \n",
    "    # Aggregate for this batch\n",
    "    agg = batch_ddf.groupby(['steam_appid', 'topic_id']).agg(\n",
    "        review_count=('review', 'count'),\n",
    "        likes_sum=('voted_up', 'sum')\n",
    "    )\n",
    "    \n",
    "    # Collect reviews for this batch\n",
    "    reviews_series = batch_ddf.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "        .apply(lambda x: list(x), meta=('review', object))\n",
    "    \n",
    "    # Compute both in parallel\n",
    "    agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    agg_df = agg_df.reset_index()\n",
    "    reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "    \n",
    "    # Append to results\n",
    "    all_agg_dfs.append(agg_df)\n",
    "    all_review_dfs.append(reviews_df)\n",
    "\n",
    "# Combine results\n",
    "agg_df = pd.concat(all_agg_dfs)\n",
    "reviews_df = pd.concat(all_review_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e00f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Construct Final Report DataFrame\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    \n",
    "    # Check if appid exists in GAME_THEMES\n",
    "    if appid in GAME_THEMES:\n",
    "        theme_keys = list(GAME_THEMES[appid].keys())\n",
    "        # Check if tid is a valid index\n",
    "        if tid < len(theme_keys):\n",
    "            theme_name = theme_keys[tid]\n",
    "        else:\n",
    "            theme_name = f\"Unknown Theme {tid}\"\n",
    "    else:\n",
    "        theme_name = f\"Unknown Theme {tid}\"\n",
    "    \n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Save intermediate results to avoid recomputation if summarization fails\n",
    "final_report.to_csv('output_csvs/SBERT_DD_new_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: View the Report\n",
    "# Print preview of the DataFrame (excluding the Reviews column as it contains lists)\n",
    "print(\"Final report preview (Reviews column contains lists of review texts):\")\n",
    "print(final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio']].head())\n",
    "\n",
    "# Verify that Reviews column contains lists\n",
    "sample_reviews = final_report['Reviews'].iloc[0]\n",
    "print(f\"\\nSample from first Reviews entry (showing first review only):\")\n",
    "if isinstance(sample_reviews, list) and len(sample_reviews) > 0:\n",
    "    print(f\"Number of reviews in list: {len(sample_reviews)}\")\n",
    "    print(f\"First review (truncated): {sample_reviews[0][:100]}...\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 (FULLY OPTIMIZED - FIXED): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Start a local Dask cluster\n",
    "n_workers = 4  # Adjust based on your CPU core count\n",
    "cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# Define model parameters \n",
    "MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "# First, load the data once and distribute it to avoid repetition\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx):\n",
    "    \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "    # Get just this partition\n",
    "    return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Prepare partitions with delayed\n",
    "partition_size = len(final_report) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx))\n",
    "\n",
    "# The main processing function - FIXED: Removed dependency on datasets library\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition of the data on a worker with batch processing\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Load tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model with device_map=\"auto\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with model AND tokenizer\n",
    "    summarizer = pipeline(\n",
    "        task='summarization',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework='pt',\n",
    "        model_kwargs={\"use_cache\": True}\n",
    "    )\n",
    "    \n",
    "    # Report worker GPU status\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "    # FIXED: Process chunks in batches without requiring the datasets library\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in batches for efficient GPU utilization\"\"\"\n",
    "        # Process in large batches to utilize GPU effectively\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Process in batches of MAX_GPU_BATCH_SIZE\n",
    "        for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "            batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "            batch_summaries = summarizer(\n",
    "                batch,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "            all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "        return all_summaries\n",
    "    \n",
    "    # Define the hierarchical summary function with batch processing\n",
    "    def hierarchical_summary(reviews, chunk_size=200):\n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= chunk_size:\n",
    "            doc = \"\\n\\n\".join(reviews)\n",
    "            return summarizer(\n",
    "                doc,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with batched processing\n",
    "        intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "        # Summarize the intermediate summaries\n",
    "        joined = \" \".join(intermediate_summaries)\n",
    "        return summarizer(\n",
    "            joined,\n",
    "            max_length=60,\n",
    "            min_length=20,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # Process the partition with a progress bar\n",
    "    results = []\n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            summary = hierarchical_summary(row['Reviews'], chunk_size=200)\n",
    "            results.append((idx, summary))\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Clean up every few iterations\n",
    "            if len(results) % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Clean up at the end\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    del summarizer\n",
    "    \n",
    "    # Return the results for this partition\n",
    "    return results\n",
    "\n",
    "# Schedule the tasks with the delayed partitions\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# Create a main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater\n",
    "def update_main_progress(future):\n",
    "    # Update main progress bar based on worker progress\n",
    "    completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "    main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "    main_progress.refresh()\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "import threading\n",
    "stop_flag = False\n",
    "\n",
    "def progress_monitor():\n",
    "    while not stop_flag:\n",
    "        update_main_progress(futures)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=progress_monitor)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for computation to complete - FIXED: Added more reliable computation approach\n",
    "try:\n",
    "    print(\"Computing all partitions...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    # Fallback to direct computation if future gathering fails\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join()\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(final_report)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# Flatten the nested list of results\n",
    "all_results = []\n",
    "for worker_results in results:\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "# Sort by index\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "final_report.to_csv('output_csvs/SBERT_dd_clai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde0016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
