{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 01:48:30,000 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d67ad601b3661759d2d36491f6e3937b initialized by task ('shuffle-transfer-d67ad601b3661759d2d36491f6e3937b', 0) executed on worker tcp://127.0.0.1:46267\n",
      "2025-05-07 01:48:30,428 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d67ad601b3661759d2d36491f6e3937b deactivated due to stimulus 'task-finished-1746596910.4273107'\n",
      "2025-05-07 01:48:48,937 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 3b998867d817d70d14a6fccc198201c6 initialized by task ('shuffle-transfer-3b998867d817d70d14a6fccc198201c6', 0) executed on worker tcp://127.0.0.1:46425\n",
      "2025-05-07 01:49:38,228 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 3b998867d817d70d14a6fccc198201c6 deactivated due to stimulus 'task-finished-1746596978.2278636'\n",
      "2025-05-07 01:49:56,334 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44391 (pid=86593) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:49:56,837 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46267 (pid=86589) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:49:58,810 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46267'.\n",
      "2025-05-07 01:49:59,029 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:49:59,032 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46425 (pid=86582) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:49:59,033 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39315 (pid=86587) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:49:59,037 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:49:59,143 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:49:59,191 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:50:10,177 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39155 (pid=87136) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:50:10,578 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35151 (pid=87140) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:50:12,502 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:50:12,503 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46533 (pid=87132) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:50:12,507 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:50:12,525 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:50:16,575 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:32827 (pid=87148) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:50:16,728 - distributed.scheduler - ERROR - Task ('from_sequence-process_summary_item-42e21853e75f5bee5f1ce4c84bd89b70', 9) marked as failed because 4 workers died while trying to run it\n",
      "2025-05-07 01:50:16,728 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:32827' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('from_sequence-process_summary_item-42e21853e75f5bee5f1ce4c84bd89b70', 0)} (stimulus_id='handle-worker-cleanup-1746597016.7280507')\n",
      "2025-05-07 01:50:20,667 - distributed.scheduler - ERROR - Task ('from_sequence-process_summary_item-42e21853e75f5bee5f1ce4c84bd89b70', 1) marked as failed because 4 workers died while trying to run it\n",
      "2025-05-07 01:50:20,668 - distributed.scheduler - ERROR - Task ('from_sequence-process_summary_item-42e21853e75f5bee5f1ce4c84bd89b70', 8) marked as failed because 4 workers died while trying to run it\n",
      "2025-05-07 01:50:20,779 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-07 01:50:24,063 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42767 (pid=87242) exceeded 95% memory budget. Restarting...\n",
      "2025-05-07 01:50:24,403 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Optimized Dask Client\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "\n",
    "# Start a local Dask cluster with constrained resources\n",
    "# Reserve ~75% of available RAM for Dask, leaving room for other processes\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,  # Adjust based on your CPU cores\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='4GB'  # 16GB total across 4 workers, leaving 4GB for system\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c4f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Theme Dictionary & Optimize Theme Embeddings\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get theme embeddings for specific app IDs\n",
    "# This avoids loading all embeddings at once\n",
    "def get_theme_embeddings(app_ids):\n",
    "    \"\"\"Get theme embeddings for a specific set of app IDs\"\"\"\n",
    "    embeddings = {}\n",
    "    for appid in app_ids:\n",
    "        if appid not in embeddings and appid in GAME_THEMES:\n",
    "            emb_list = []\n",
    "            for theme, seeds in GAME_THEMES[appid].items():\n",
    "                seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "                emb_list.append(seed_emb.mean(axis=0))\n",
    "            embeddings[appid] = np.vstack(emb_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf95f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Read Parquet Files with Optimized Blocksize\n",
    "# Read with explicit blocksize optimization\n",
    "ddf = dd.read_parquet(\n",
    "    'parquet_output_theme_combo/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up'],\n",
    "    blocksize='64MB'  # Adjust based on available RAM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902a237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11279213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Optimized Partition-wise Topic Assignment\n",
    "def assign_topic(df_partition):\n",
    "    \"\"\"Assign topics using only theme embeddings for app IDs in this partition\"\"\"\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "    \n",
    "    # Get unique app IDs in this partition\n",
    "    app_ids = df_partition['steam_appid'].unique().tolist()\n",
    "    app_ids = [int(appid) for appid in app_ids]\n",
    "    \n",
    "    # Get embeddings only for app IDs in this partition\n",
    "    local_theme_embeddings = get_theme_embeddings(app_ids)\n",
    "    \n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go with batching\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        appid = int(appid)\n",
    "        if appid in local_theme_embeddings:\n",
    "            theme_embs = local_theme_embeddings[appid]\n",
    "            sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "            topic_ids.append(int(sims.argmax()))\n",
    "        else:\n",
    "            # Default topic if theme embeddings not available\n",
    "            topic_ids.append(0)\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fe5a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460f48fbd3a543f1983123c006f156e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 87.21 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Aggregate Counts, Likes, and Collect Reviews per Theme\n",
    "# Process in smaller chunks to avoid memory issues\n",
    "# Get unique app IDs\n",
    "unique_app_ids = ddf['steam_appid'].unique().compute()\n",
    "\n",
    "# Initialize empty dataframes for results\n",
    "all_agg_dfs = []\n",
    "all_review_dfs = []\n",
    "\n",
    "# Process in batches of app IDs\n",
    "batch_size = 5  # Adjust based on your memory constraints\n",
    "for i in tqdm(range(0, len(unique_app_ids), batch_size)):\n",
    "    batch_app_ids = unique_app_ids[i:i+batch_size]\n",
    "    \n",
    "    # Filter data for this batch of app IDs\n",
    "    batch_ddf = ddf_with_topic[ddf_with_topic['steam_appid'].isin(batch_app_ids)]\n",
    "    \n",
    "    # Aggregate for this batch\n",
    "    agg = batch_ddf.groupby(['steam_appid', 'topic_id']).agg(\n",
    "        review_count=('review', 'count'),\n",
    "        likes_sum=('voted_up', 'sum')\n",
    "    )\n",
    "    \n",
    "    # Collect reviews for this batch\n",
    "    reviews_series = batch_ddf.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "        .apply(lambda x: list(x), meta=('review', object))\n",
    "    \n",
    "    # Compute both in parallel\n",
    "    agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    agg_df = agg_df.reset_index()\n",
    "    reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "    \n",
    "    # Append to results\n",
    "    all_agg_dfs.append(agg_df)\n",
    "    all_review_dfs.append(reviews_df)\n",
    "\n",
    "# Combine results\n",
    "agg_df = pd.concat(all_agg_dfs)\n",
    "reviews_df = pd.concat(all_review_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e00f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Construct Final Report DataFrame\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    \n",
    "    # Check if appid exists in GAME_THEMES\n",
    "    if appid in GAME_THEMES:\n",
    "        theme_keys = list(GAME_THEMES[appid].keys())\n",
    "        # Check if tid is a valid index\n",
    "        if tid < len(theme_keys):\n",
    "            theme_name = theme_keys[tid]\n",
    "        else:\n",
    "            theme_name = f\"Unknown Theme {tid}\"\n",
    "    else:\n",
    "        theme_name = f\"Unknown Theme {tid}\"\n",
    "    \n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Save intermediate results to avoid recomputation if summarization fails\n",
    "final_report.to_csv('output_csvs/SBERT_DD_new_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6495d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final report preview (Reviews column contains lists of review texts):\n",
      "   steam_appid        Theme  #Reviews LikeRatio\n",
      "0           10    community      2511     96.2%\n",
      "1           10   anti_cheat      3654     93.7%\n",
      "2           10  performance      2527     91.7%\n",
      "3           10  competitive      9644     98.1%\n",
      "4           10     gameplay      2416     96.9%\n",
      "\n",
      "Sample from first Reviews entry (showing first review only):\n",
      "Number of reviews in list: 2511\n",
      "First review (truncated): Actually the best game in this world. It still doesnt matter if u have NASA PC or you are playing on...\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: View the Report\n",
    "# Print preview of the DataFrame (excluding the Reviews column as it contains lists)\n",
    "print(\"Final report preview (Reviews column contains lists of review texts):\")\n",
    "print(final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio']].head())\n",
    "\n",
    "# Verify that Reviews column contains lists\n",
    "sample_reviews = final_report['Reviews'].iloc[0]\n",
    "print(f\"\\nSample from first Reviews entry (showing first review only):\")\n",
    "if isinstance(sample_reviews, list) and len(sample_reviews) > 0:\n",
    "    print(f\"Number of reviews in list: {len(sample_reviews)}\")\n",
    "    print(f\"First review (truncated): {sample_reviews[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd00a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved basic report with required columns to 'output_csvs/theme_report.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c6973922e24598bff3911f2dfd46cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing summary batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 1.14 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2025-05-07 01:49:55,917 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:55,923 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:56,496 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:56,658 - distributed.worker.memory - WARNING - Worker is at 100% memory usage. Pausing worker.  Process memory: 3.74 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:56,882 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:56,882 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:56,957 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:49:56,959 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:02,955 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 3.29 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:02,956 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:10,078 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:10,378 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:10,422 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 3.31 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:10,615 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 3.33 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:11,183 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:13,257 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:14,078 - distributed.worker.memory - WARNING - Worker is at 22% memory usage. Resuming worker. Process memory: 862.86 MiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:16,377 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.24 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:20,809 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n",
      "2025-05-07 01:50:20,810 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('from_sequence-process_summary_item-42e21853e75f5bee5f1ce4c84bd89b70', 9) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:32827. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKilledWorker\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m     batch = summary_items[i:i+batch_size]\n\u001b[32m    107\u001b[39m     bag = db.from_sequence(batch)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     batch_results = bag.map(process_summary_item).compute()\n\u001b[32m    109\u001b[39m     all_summaries.extend(batch_results)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Create a mapping from (appid, theme) to summary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/dask/base.py:374\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    351\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     (result,) = compute(\u001b[38;5;28mself\u001b[39m, traverse=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs)\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/dask/base.py:662\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     postcomputes.append(x.__dask_postcompute__())\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     results = schedule(dsk, keys, **kwargs)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, *a) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:2426\u001b[39m, in \u001b[36mClient._gather\u001b[39m\u001b[34m(self, futures, errors, direct, local_worker)\u001b[39m\n\u001b[32m   2424\u001b[39m     exception = st.exception\n\u001b[32m   2425\u001b[39m     traceback = st.traceback\n\u001b[32m-> \u001b[39m\u001b[32m2426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(traceback)\n\u001b[32m   2427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2428\u001b[39m     bad_keys.add(key)\n",
      "\u001b[31mKilledWorker\u001b[39m: Attempted to run task ('from_sequence-process_summary_item-42e21853e75f5bee5f1ce4c84bd89b70', 9) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:32827. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "# Cell 9: Parallelize Hierarchical Summarization with Dask (OPTIONAL)\n",
    "# NOTE: This cell is optional. The basic report with the required columns is \n",
    "# already saved in Cell 7. Only run this if you want theme summarization.\n",
    "\n",
    "# First save the report with required columns\n",
    "final_report_basic = final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio', 'Reviews']]\n",
    "final_report_basic.to_csv('output_csvs/theme_report.csv', index=False)\n",
    "print(f\"Saved basic report with required columns to 'output_csvs/theme_report.csv'\")\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = pipeline(\n",
    "    task='summarization',\n",
    "    model='sshleifer/distilbart-cnn-12-6',\n",
    "    device=0,  # change to -1 if no GPU\n",
    "    framework='pt'\n",
    ")\n",
    "\n",
    "def hierarchical_summary(reviews, chunk_size=200,\n",
    "                         max_len=60, min_len=20):\n",
    "    \"\"\"\n",
    "    Summarize a long list of reviews into one short summary:\n",
    "      1) Chunk the reviews into batches of chunk_size\n",
    "      2) Summarize each batch\n",
    "      3) Summarize the concatenation of batch summaries\n",
    "    \n",
    "    Params:\n",
    "      reviews    : list of str, the reviews to summarize\n",
    "      chunk_size : int, number of reviews per intermediate chunk\n",
    "      max_len    : int, max summary tokens per call\n",
    "      min_len    : int, min summary tokens per call\n",
    "    \n",
    "    Returns:\n",
    "      str, final \"quick read\" summary\n",
    "    \"\"\"\n",
    "    # If there are fewer than chunk_size, just do one summary\n",
    "    if len(reviews) <= chunk_size:\n",
    "        doc = \"\\n\\n\".join(reviews[:chunk_size])  # Limit to chunk_size to avoid OOM\n",
    "        return summarizer(\n",
    "            doc,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # 2) Summarize each chunk\n",
    "    intermediate = []\n",
    "    for i in range(0, min(len(reviews), 1000), chunk_size):  # Limit to 1000 reviews max\n",
    "        batch = reviews[i:i+chunk_size]\n",
    "        text = \"\\n\\n\".join(batch)\n",
    "        summ = summarizer(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        intermediate.append(summ)\n",
    "    \n",
    "    # 3) Summarize the intermediate summaries\n",
    "    joined = \" \".join(intermediate)\n",
    "    return summarizer(\n",
    "        joined,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "# Prepare data for parallel processing\n",
    "summary_items = []\n",
    "for _, row in final_report.iterrows():\n",
    "    summary_items.append({\n",
    "        'appid': row['steam_appid'], \n",
    "        'theme': row['Theme'], \n",
    "        'reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "# Function for parallel processing\n",
    "def process_summary_item(item):\n",
    "    try:\n",
    "        summary = hierarchical_summary(\n",
    "            item['reviews'],\n",
    "            chunk_size=200,\n",
    "            max_len=60,\n",
    "            min_len=20\n",
    "        )\n",
    "        return {\n",
    "            'appid': item['appid'],\n",
    "            'theme': item['theme'],\n",
    "            'summary': summary\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'appid': item['appid'],\n",
    "            'theme': item['theme'],\n",
    "            'summary': f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Process in parallel with Dask\n",
    "# Split into smaller batches to avoid memory issues\n",
    "batch_size = 10  # Process 10 themes at a time\n",
    "all_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(summary_items), batch_size), desc=\"Processing summary batches\"):\n",
    "    batch = summary_items[i:i+batch_size]\n",
    "    bag = db.from_sequence(batch)\n",
    "    batch_results = bag.map(process_summary_item).compute()\n",
    "    all_summaries.extend(batch_results)\n",
    "\n",
    "# Create a mapping from (appid, theme) to summary\n",
    "summary_map = {\n",
    "    (item['appid'], item['theme']): item['summary'] \n",
    "    for item in all_summaries\n",
    "}\n",
    "\n",
    "# Create a copy of the final report and add summaries\n",
    "final_report_with_summary = final_report.copy()\n",
    "final_report_with_summary['QuickSummary'] = final_report_with_summary.apply(\n",
    "    lambda row: summary_map.get((row['steam_appid'], row['Theme']), \"No summary available\"),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save enhanced results\n",
    "final_report_with_summary.to_csv('output_csvs/theme_report_with_summary.csv', index=False)\n",
    "\n",
    "# Display sample of final results\n",
    "print(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Clean up\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48b690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b26041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
