{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f13561",
   "metadata": {},
   "source": [
    "# This code has dynamic allocation of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeff0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üß† Cell 1 ‚Äì The Brain that Adapts: Dynamic Dask Initialization\n",
    "Imagine you're about to load and analyze thousands of Steam game reviews ‚Äî each review is rich, unstructured text. Now, if we throw all this data at a single processor, it‚Äôll choke. But what if our code could intelligently adapt to the machine it's running on?\n",
    "\n",
    "Welcome to Cell 1 ‚Äî the brain of our pipeline. It starts with:\n",
    "\n",
    "üß∞ Toolbox Assembly\n",
    "We load essential Python libraries:\n",
    "\n",
    "psutil to peek into system resources.\n",
    "\n",
    "dask.distributed to launch parallel computation across multiple cores.\n",
    "\n",
    "sentence-transformers, transformers, tqdm, and more for embedding, similarity scoring, and progress bars.\n",
    "\n",
    "üßÆ Step 1: Measuring the Machine\n",
    "We define get_system_resources(), a smart function that inspects:\n",
    "\n",
    "üß† Total RAM: How much memory do we have to work with?\n",
    "\n",
    "üîß CPU cores: How many brains can we use in parallel?\n",
    "\n",
    "It makes two clever decisions:\n",
    "\n",
    "It uses only 70% of total memory ‚Äî to avoid crashes and leave room for the OS.\n",
    "\n",
    "It calculates memory per worker by splitting that 70% evenly across a sensible number of worker processes (leaving one CPU core free).\n",
    "\n",
    "üõ†Ô∏è Step 2: Setting Up the Workers\n",
    "Once we have the memory and CPU plan, we initialize a Dask LocalCluster:\n",
    "\n",
    "Each worker is like a mini-computer that handles a chunk of the workload.\n",
    "\n",
    "Each worker gets 2 threads and a specific memory limit.\n",
    "\n",
    "This setup ensures predictable performance, tailored to the hardware.\n",
    "\n",
    "üì° Step 3: Launching the Cluster\n",
    "With Client(cluster), Dask connects to the cluster, and ‚Äî boom ‚Äî a dashboard link appears! This is your control tower, showing how tasks are distributed, how memory is used, and how fast everything is running.\n",
    "\n",
    "You now have a flexible, dynamic, and scalable backbone for the rest of the pipeline.\n",
    "\n",
    "üîç TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell intelligently analyzes your system's RAM and CPU, dynamically spins up a custom-sized Dask cluster, and connects a live dashboard ‚Äî so every run is hardware-optimized and ready for heavy review analysis.‚Äù'''\n",
    "# Modified Cell 1: Dynamic resource allocation for Dask Client\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "\n",
    "# Dynamically determine system resources\n",
    "def get_system_resources():\n",
    "    # Get available memory (in GB)\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)\n",
    "    # Get CPU count\n",
    "    cpu_count = psutil.cpu_count(logical=False)  # Physical cores only\n",
    "    if not cpu_count:\n",
    "        cpu_count = psutil.cpu_count(logical=True)  # Logical if physical not available\n",
    "    \n",
    "    # Use 70% of available memory for Dask, split across workers\n",
    "    dask_memory = int(total_memory * 0.7)\n",
    "    # Determine optimal worker count (leave at least 1 core for system)\n",
    "    worker_count = max(1, cpu_count - 1)\n",
    "    # Memory per worker\n",
    "    memory_per_worker = int(dask_memory / worker_count)\n",
    "    \n",
    "    return {\n",
    "        'worker_count': worker_count,\n",
    "        'memory_per_worker': memory_per_worker,\n",
    "        'total_memory': total_memory\n",
    "    }\n",
    "\n",
    "# Get system resources\n",
    "resources = get_system_resources()\n",
    "print(f\"System has {resources['total_memory']:.1f}GB memory and {resources['worker_count']} CPU cores\")\n",
    "print(f\"Allocating {resources['worker_count']} workers with {resources['memory_per_worker']}GB each\")\n",
    "\n",
    "# Start a local Dask cluster with dynamically determined resources\n",
    "cluster = LocalCluster(\n",
    "    n_workers=resources['worker_count'],\n",
    "    threads_per_worker=2,\n",
    "    memory_limit=f\"{resources['memory_per_worker']}GB\"\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54250fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üéØ Cell 2 ‚Äì Building the Knowledge Base: Game Themes & Semantic Fingerprints\n",
    "Now that we‚Äôve built a scalable engine in Cell 1, it‚Äôs time to feed it something meaningful ‚Äî the intelligence that powers topic detection for each game.\n",
    "\n",
    "This cell is all about loading the brains of the operation: the themes per game and their semantic representations.\n",
    "\n",
    "üìö Step 1: Load the Theme Dictionary\n",
    "We start by opening a file called game_themes.json. Think of this as a manual provided by indie devs or curators ‚Äî it lists themes for each game, such as:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{\n",
    "  \"123456\": {\n",
    "    \"story\": [\"plot\", \"narrative\", \"characters\"],\n",
    "    \"visuals\": [\"art\", \"graphics\", \"color\"]\n",
    "  }\n",
    "}\n",
    "Each Steam appid is mapped to themes, and each theme has a set of seed keywords that represent it. This forms the semantic space of what each game cares about.\n",
    "\n",
    "We store this in a Python dictionary: GAME_THEMES.\n",
    "\n",
    "üß† Step 2: Initialize the SBERT Model\n",
    "Here comes the transformer ‚Äî SentenceTransformer('all-MiniLM-L6-v2').\n",
    "\n",
    "This model takes in keywords or phrases and converts them into dense numerical vectors ‚Äî basically, semantic fingerprints that machines can compare.\n",
    "\n",
    "This is crucial, because later we‚Äôll compare review texts to these fingerprints to figure out:\n",
    "üìå ‚ÄúIs this review talking about story, visuals, or gameplay?‚Äù\n",
    "\n",
    "‚ö° Step 3: Embedding Only What You Need\n",
    "Rather than encoding all themes for all games upfront (which would be wasteful and memory-heavy), we define a smart function:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def get_theme_embeddings(app_ids):\n",
    "This function:\n",
    "\n",
    "Takes a list of specific game IDs.\n",
    "\n",
    "For each game:\n",
    "\n",
    "Loops through its themes.\n",
    "\n",
    "For each theme, embeds all its seed keywords using SBERT.\n",
    "\n",
    "Averages these vectors to get a single theme vector.\n",
    "\n",
    "Stacks all the theme vectors together for that game.\n",
    "\n",
    "This way, we get a compact matrix of theme embeddings per game ‚Äî just in time and just for the data we‚Äôre processing. No memory bloat. ‚öôÔ∏è\n",
    "\n",
    "üí° Why This Matters:\n",
    "This step gives us a shared language between reviews and themes. Without this, we‚Äôd just have a pile of raw text. With it, we can ask:\n",
    "\n",
    "‚ÄúWhich predefined theme is this review most similar to?‚Äù\n",
    "\n",
    "üßæ TL;DR for Presentation Slide:\n",
    "‚ÄúWe load curated theme keywords for each game and convert them into semantic embeddings using SBERT. These vectors form the core reference that allows us to match incoming reviews to meaningful themes ‚Äî efficiently and on-demand.‚Äù'''\n",
    "# Cell 2: Load Theme Dictionary & Optimize Theme Embeddings\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get theme embeddings for specific app IDs\n",
    "# This avoids loading all embeddings at once\n",
    "def get_theme_embeddings(app_ids):\n",
    "    \"\"\"Get theme embeddings for a specific set of app IDs\"\"\"\n",
    "    embeddings = {}\n",
    "    for appid in app_ids:\n",
    "        if appid not in embeddings and appid in GAME_THEMES:\n",
    "            emb_list = []\n",
    "            for theme, seeds in GAME_THEMES[appid].items():\n",
    "                seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "                emb_list.append(seed_emb.mean(axis=0))\n",
    "            embeddings[appid] = np.vstack(emb_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8528a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üß± Cell 3 ‚Äì Smart Data Loading: The Right Bite for Every File\n",
    "Our pipeline so far has been resource-aware (Cell 1) and meaning-aware (Cell 2). Now it‚Äôs time to dive into the actual review data ‚Äî thousands of user thoughts spread across dozens of .parquet files.\n",
    "\n",
    "But here‚Äôs the twist:\n",
    "\n",
    "You don‚Äôt want to gulp all the data at once‚Ä¶ you want to bite just the right amount.\n",
    "\n",
    "This cell is where smart, adaptive data ingestion happens.\n",
    "\n",
    "üìè Step 1: Peek Before You Leap\n",
    "We define a function estimate_dataset_size() ‚Äî it walks through the review directory and adds up the file sizes of all .parquet files:\n",
    "\n",
    "This helps us measure the total dataset size in GB.\n",
    "\n",
    "Why? Because memory issues can sneak in if we blindly read everything with default settings.\n",
    "\n",
    "Example output:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "Estimated dataset size: 21.4GB\n",
    "üßÆ Step 2: Choose the Right Block Size\n",
    "Here‚Äôs where the code gets smart ‚Äî it adjusts how data is loaded based on size:\n",
    "\n",
    "üîπ Big dataset (>100GB) ‚Üí use tiny chunks (16MB)\n",
    "\n",
    "üî∏ Medium dataset (>10GB) ‚Üí moderate chunks (32MB)\n",
    "\n",
    "‚ö™ Small dataset (‚â§10GB) ‚Üí larger chunks (64MB)\n",
    "\n",
    "Why does this matter?\n",
    "\n",
    "Smaller chunks mean less memory strain and better parallelism.\n",
    "\n",
    "Larger chunks are fine when memory isn't tight.\n",
    "\n",
    "It's like loading a truck:\n",
    "\n",
    "For heavy cargo, you use more trips with smaller boxes. For lighter loads, you pack more in each trip.\n",
    "\n",
    "üõ†Ô∏è Step 3: Load with Dask using Dynamic Blocksize\n",
    "We now read the review data using:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "dd.read_parquet(..., blocksize=blocksize)\n",
    "The data is read as a Dask DataFrame, meaning it's still lazy and parallelized.\n",
    "\n",
    "We only pick essential columns for the analysis:\n",
    "\n",
    "steam_appid ‚Äì which game?\n",
    "\n",
    "review ‚Äì the actual text\n",
    "\n",
    "review_language ‚Äì filter for English later\n",
    "\n",
    "voted_up ‚Äì thumbs up or down?\n",
    "\n",
    "The result: a memory-efficient, scalable review loader that adapts to any machine and any dataset size.\n",
    "\n",
    "üßæ TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell intelligently estimates the dataset size and adjusts the read blocksize accordingly. It prevents memory overload and ensures efficient streaming of .parquet reviews into a Dask DataFrame ‚Äî tailored to the system‚Äôs capabilities.‚Äù'''\n",
    "# Modified Cell 3: Dynamic blocksize for reading Parquet Files\n",
    "# Estimate dataset size first\n",
    "def estimate_dataset_size(path):\n",
    "    import os\n",
    "    total_size = 0\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size / (1024**3)  # Convert to GB\n",
    "\n",
    "# Estimate dataset size\n",
    "dataset_path = 'parquet_output_theme_combo'\n",
    "estimated_size = estimate_dataset_size(dataset_path)\n",
    "print(f\"Estimated dataset size: {estimated_size:.2f}GB\")\n",
    "\n",
    "# Dynamically determine blocksize based on dataset and memory\n",
    "# Use smaller blocks for larger datasets to prevent memory issues\n",
    "if estimated_size > 100:  # Very large dataset\n",
    "    blocksize = '16MB'\n",
    "elif estimated_size > 10:  # Medium-large dataset\n",
    "    blocksize = '32MB'\n",
    "else:  # Smaller dataset\n",
    "    blocksize = '64MB'\n",
    "\n",
    "print(f\"Using dynamic blocksize: {blocksize}\")\n",
    "\n",
    "# Read with dynamic blocksize\n",
    "ddf = dd.read_parquet(\n",
    "    f'{dataset_path}/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up'],\n",
    "    blocksize=blocksize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üßπ Cell 4 ‚Äì Filtering the Noise: Clean Reviews, Clear Insights\n",
    "Now that we‚Äôve successfully loaded the raw reviews into memory ‚Äî chunked, distributed, and ready ‚Äî it‚Äôs time to clean the data pipeline.\n",
    "\n",
    "Because let‚Äôs face it:\n",
    "\n",
    "Not all reviews are useful. Some are in different languages. Some are empty. Some are just noise.\n",
    "\n",
    "This cell is the data janitor ‚Äî quietly ensuring we only work with reviews that matter.\n",
    "\n",
    "üåê Step 1: Keep It English\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "Steam users write reviews in dozens of languages. But our theme keywords and SBERT model are trained on English ‚Äî mixing in other languages would dilute and confuse our embeddings.\n",
    "\n",
    "So here, we filter out all non-English reviews, keeping only those with:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "'review_language' == 'english'\n",
    "üï≥Ô∏è Step 2: Drop Empty Reviews\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "ddf = ddf.dropna(subset=['review'])\n",
    "Sometimes, reviews might be technically present but actually missing or null. These offer no insight, no words ‚Äî just blank space.\n",
    "\n",
    "So we call .dropna() to clean out anything where the 'review' column is missing. This ensures:\n",
    "\n",
    "Every row we embed has actual text.\n",
    "\n",
    "No wasted compute cycles or model calls.\n",
    "\n",
    "üí° Why This Is Important:\n",
    "This cleaning step happens before any heavy embedding or summarization.\n",
    "\n",
    "It prevents wasted GPU/CPU usage on irrelevant rows.\n",
    "\n",
    "It boosts accuracy and efficiency ‚Äî we only analyze real, readable content.\n",
    "\n",
    "üßæ TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell filters the dataset to include only valid English reviews with non-empty text. It eliminates irrelevant or malformed rows, ensuring that our pipeline processes only meaningful content.‚Äù'''\n",
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üß† Cell 5 ‚Äì The Matchmaker: Assigning Reviews to Game-Specific Themes\n",
    "Until now, we‚Äôve:\n",
    "\n",
    "Loaded and cleaned data.\n",
    "\n",
    "Parsed themes and embedded them.\n",
    "\n",
    "Set up a Dask engine to scale it all.\n",
    "\n",
    "But here comes the magic moment ‚Äî the point where we ask:\n",
    "\n",
    "‚ÄúWhat is this review really talking about?‚Äù\n",
    "\n",
    "This cell is the theme assignment engine, assigning each review to the most relevant topic ‚Äî and it does it intelligently, efficiently, and in parallel.\n",
    "\n",
    "üß© Step 1: Partition-wise Processing\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def assign_topic(df_partition):\n",
    "Dask splits your huge dataset into smaller partitions, each processed separately. This function works on each chunk independently, which is:\n",
    "\n",
    "More scalable.\n",
    "\n",
    "Easier on memory.\n",
    "\n",
    "Inherently parallel.\n",
    "\n",
    "The first check:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "if df_partition.empty: return as-is\n",
    "This avoids wasting compute on empty chunks.\n",
    "\n",
    "üéÆ Step 2: Game-Specific Theme Lookup\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "app_ids = df_partition['steam_appid'].unique().tolist()\n",
    "Each review belongs to a specific Steam game. Instead of loading all game embeddings, we fetch only the theme embeddings relevant to this partition:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "local_theme_embeddings = get_theme_embeddings(app_ids)\n",
    "This keeps memory low and speeds up lookup.\n",
    "\n",
    "üß† Step 3: Semantic Comparison\n",
    "We embed all reviews in this chunk in a single batch:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "review_embeds = embedder.encode(reviews, batch_size=64, convert_to_numpy=True)\n",
    "Then, for each review:\n",
    "\n",
    "We find its appid.\n",
    "\n",
    "We fetch the game‚Äôs theme embeddings.\n",
    "\n",
    "We compute cosine similarity between the review vector and each theme vector.\n",
    "\n",
    "We assign the theme with the highest similarity as the topic_id.\n",
    "\n",
    "If for some reason the game‚Äôs themes are missing (edge case), we fallback to:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "topic_ids.append(0)\n",
    "üßæ Step 4: Return Result with Topic Labels\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df_partition['topic_id'] = topic_ids\n",
    "Each row now has a topic_id, telling us which theme this review is most aligned with.\n",
    "\n",
    "Then we apply it across all partitions:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "ddf.map_partitions(assign_topic, meta=meta)\n",
    "We also specify the output structure using meta, to make Dask‚Äôs lazy execution happy.\n",
    "\n",
    "‚öôÔ∏è Why This Is Special:\n",
    "‚úÖ It‚Äôs game-specific: different games have different theme vocabularies.\n",
    "\n",
    "‚úÖ It‚Äôs parallel: runs on all partitions at once.\n",
    "\n",
    "‚úÖ It‚Äôs batch-embedded: maximizes GPU/CPU efficiency.\n",
    "\n",
    "‚úÖ It adds a critical column: topic_id ‚Äî the foundation for aggregation, summarization, and insights.\n",
    "\n",
    "üßæ TL;DR for Presentation Slide:\n",
    "‚ÄúEach review is semantically compared to the themes of its game. This cell assigns a topic_id to every review by finding the closest matching theme vector using cosine similarity ‚Äî all done in parallel across Dask partitions.‚Äù'''\n",
    "# Cell 5: Optimized Partition-wise Topic Assignment\n",
    "def assign_topic(df_partition):\n",
    "    \"\"\"Assign topics using only theme embeddings for app IDs in this partition\"\"\"\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "    \n",
    "    # Get unique app IDs in this partition\n",
    "    app_ids = df_partition['steam_appid'].unique().tolist()\n",
    "    app_ids = [int(appid) for appid in app_ids]\n",
    "    \n",
    "    # Get embeddings only for app IDs in this partition\n",
    "    local_theme_embeddings = get_theme_embeddings(app_ids)\n",
    "    \n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go with batching\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        appid = int(appid)\n",
    "        if appid in local_theme_embeddings:\n",
    "            theme_embs = local_theme_embeddings[appid]\n",
    "            sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "            topic_ids.append(int(sims.argmax()))\n",
    "        else:\n",
    "            # Default topic if theme embeddings not available\n",
    "            topic_ids.append(0)\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üìä Cell 6 ‚Äì Smart Batching for Scalable Review Aggregation\n",
    "We‚Äôve now reached a powerful phase in our pipeline ‚Äî we‚Äôve cleaned the data, labeled each review with a topic_id, and embedded everything efficiently.\n",
    "\n",
    "Now it‚Äôs time to zoom out:\n",
    "\n",
    "\"How are reviews distributed across themes? What topics are loved? Which are controversial?\"\n",
    "\n",
    "But there‚Äôs a challenge:\n",
    "Each game (each steam_appid) has hundreds or thousands of reviews. If we try to aggregate all app IDs at once, we could overwhelm memory ‚Äî especially on larger datasets.\n",
    "\n",
    "So this cell introduces something clever:\n",
    "\n",
    "‚öñÔ∏è Step 1: Adaptive Batching\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "unique_app_ids = ddf['steam_appid'].unique().compute()\n",
    "We compute the number of distinct Steam games present.\n",
    "\n",
    "Then we dynamically decide a batch size:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "if total_app_ids > 1000:\n",
    "    batch_size = 3\n",
    "elif total_app_ids > 500:\n",
    "    batch_size = 5\n",
    "...\n",
    "üîÅ This means:\n",
    "\n",
    "For huge datasets ‚Üí small batches to avoid memory overflow.\n",
    "\n",
    "For smaller datasets ‚Üí larger batches for speed.\n",
    "\n",
    "üì¶ Step 2: Batch-by-Batch Processing\n",
    "We now process game IDs in manageable chunks:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "for i in range(0, len(unique_app_ids), batch_size):\n",
    "In each iteration:\n",
    "\n",
    "We isolate reviews for a few games using .isin(batch_app_ids).\n",
    "\n",
    "We group by both steam_appid and topic_id to calculate:\n",
    "\n",
    "üìà review_count ‚Äì how many reviews belong to this theme\n",
    "\n",
    "üëç likes_sum ‚Äì how many of them were upvoted\n",
    "\n",
    "These values help us measure engagement and sentiment toward each theme.\n",
    "\n",
    "üßæ Step 3: Collecting Raw Reviews for Summarization\n",
    "We also collect the raw review text, grouped by game and topic:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "reviews_series = batch_ddf.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "    .apply(lambda x: list(x), meta=('review', object))\n",
    "This allows future steps (like summarization) to read all reviews related to a specific theme for a game.\n",
    "\n",
    "‚öôÔ∏è Step 4: Parallel Execution & Final Assembly\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "Dask executes both operations in parallel ‚Äî leveraging our cluster ‚Äî and we:\n",
    "\n",
    "Flatten the results with .reset_index()\n",
    "\n",
    "Rename the review list column to Reviews\n",
    "\n",
    "Append everything to the final result lists\n",
    "\n",
    "This gives us:\n",
    "\n",
    "all_agg_dfs: count-based summaries\n",
    "\n",
    "all_review_dfs: theme-wise grouped raw review text\n",
    "\n",
    "Both ready for visualization or summarization.\n",
    "\n",
    "üìå TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell dynamically adjusts batch size based on dataset scale, then processes each game in chunks ‚Äî aggregating review counts and upvotes per theme while also collecting raw reviews for future summarization. It balances memory use and compute power for scalable insight extraction.‚Äù'''\n",
    "# Modified Cell 6: Dynamic batch sizing for aggregation\n",
    "# Get unique app IDs\n",
    "unique_app_ids = ddf['steam_appid'].unique().compute()\n",
    "total_app_ids = len(unique_app_ids)\n",
    "\n",
    "# Dynamically determine batch size based on number of app IDs and memory\n",
    "# For larger datasets, use smaller batches to avoid memory issues\n",
    "if total_app_ids > 1000:  # Very large number of app IDs\n",
    "    batch_size = 3\n",
    "elif total_app_ids > 500:  # Medium-large number\n",
    "    batch_size = 5\n",
    "elif total_app_ids > 100:  # Medium number\n",
    "    batch_size = 10\n",
    "else:  # Smaller number\n",
    "    batch_size = 20\n",
    "\n",
    "print(f\"Processing {total_app_ids} unique app IDs with batch size {batch_size}\")\n",
    "\n",
    "# Initialize empty dataframes for results\n",
    "all_agg_dfs = []\n",
    "all_review_dfs = []\n",
    "\n",
    "# Process in dynamically sized batches\n",
    "for i in tqdm(range(0, len(unique_app_ids), batch_size)):\n",
    "    batch_app_ids = unique_app_ids[i:i+batch_size]\n",
    "    \n",
    "    # Filter data for this batch of app IDs\n",
    "    batch_ddf = ddf_with_topic[ddf_with_topic['steam_appid'].isin(batch_app_ids)]\n",
    "    \n",
    "    # Aggregate for this batch\n",
    "    agg = batch_ddf.groupby(['steam_appid', 'topic_id']).agg(\n",
    "        review_count=('review', 'count'),\n",
    "        likes_sum=('voted_up', 'sum')\n",
    "    )\n",
    "    \n",
    "    # Collect reviews for this batch\n",
    "    reviews_series = batch_ddf.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "        .apply(lambda x: list(x), meta=('review', object))\n",
    "    \n",
    "    # Compute both in parallel\n",
    "    agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    agg_df = agg_df.reset_index()\n",
    "    reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "    \n",
    "    # Append to results\n",
    "    all_agg_dfs.append(agg_df)\n",
    "    all_review_dfs.append(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48de627",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üßæ Cell 7 ‚Äì The Final Report: Turning Numbers into Meaning\n",
    "After all the data crunching, embedding, batching, and aggregation, we arrive at the final step of our pipeline:\n",
    "\n",
    "‚ÄúLet‚Äôs turn this into something a developer can actually use.‚Äù\n",
    "\n",
    "This cell crafts a concise, readable summary that answers the big question:\n",
    "\n",
    "‚ÄúWhat themes are people talking about in my game, and how are they reacting to them?‚Äù\n",
    "\n",
    "üîó Step 1: Merge Everything\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "pd.merge(agg_df, reviews_df, on=['steam_appid', 'topic_id'], how='left')\n",
    "We combine:\n",
    "\n",
    "agg_df: the numerical summary ‚Äî review counts and like sums.\n",
    "\n",
    "reviews_df: the raw grouped review lists ‚Äî needed for qualitative insights or summarization.\n",
    "\n",
    "They‚Äôre merged on:\n",
    "\n",
    "steam_appid ‚Äî which game?\n",
    "\n",
    "topic_id ‚Äî which theme?\n",
    "\n",
    "Now we have both numbers and text in one place.\n",
    "\n",
    "üèóÔ∏è Step 2: Build Final Rows\n",
    "We loop over each row of the merged dataframe, and construct the final human-readable format.\n",
    "\n",
    "For each review group, we:\n",
    "\n",
    "üî¢ Extract game ID (appid) and topic index (tid)\n",
    "\n",
    "üß† Convert tid into a real theme name using the GAME_THEMES dictionary:\n",
    "\n",
    "If the theme index is valid, we extract its name.\n",
    "\n",
    "Otherwise, we assign it a fallback label like \"Unknown Theme 2\" (to avoid breaking things).\n",
    "\n",
    "üí° Step 3: Compute Like Ratio\n",
    "We calculate how positively that theme is being received:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "This gives game developers a quick feel:\n",
    "\n",
    "Are reviews around this theme mostly positive?\n",
    "\n",
    "Or is this a hot-button issue?\n",
    "\n",
    "üìã Step 4: Construct Final Rows\n",
    "Each row includes:\n",
    "\n",
    "steam_appid ‚Äì which game\n",
    "\n",
    "Theme ‚Äì human-readable theme name\n",
    "\n",
    "#Reviews ‚Äì number of reviews in this group\n",
    "\n",
    "LikeRatio ‚Äì % of upvotes\n",
    "\n",
    "Reviews ‚Äì actual review texts (for later summarization)\n",
    "\n",
    "We store each entry in a rows[] list, and finally convert it into:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_report = pd.DataFrame(rows)\n",
    "üíæ Step 5: Save as CSV\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_report.to_csv('output_csvs/SBERT_DD_new_report.csv', index=False)\n",
    "This creates a snapshot of everything we‚Äôve processed ‚Äî so if summarization or visualization fails later, we don‚Äôt lose progress. It‚Äôs the final checkpoint.\n",
    "\n",
    "üìå TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell merges aggregated metrics with review text and maps each topic ID back to a theme name. It calculates like ratios per theme, formats the result into a clean report, and saves it as a CSV for downstream analysis or visualization.‚Äù'''\n",
    "# Cell 7: Construct Final Report DataFrame\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    \n",
    "    # Check if appid exists in GAME_THEMES\n",
    "    if appid in GAME_THEMES:\n",
    "        theme_keys = list(GAME_THEMES[appid].keys())\n",
    "        # Check if tid is a valid index\n",
    "        if tid < len(theme_keys):\n",
    "            theme_name = theme_keys[tid]\n",
    "        else:\n",
    "            theme_name = f\"Unknown Theme {tid}\"\n",
    "    else:\n",
    "        theme_name = f\"Unknown Theme {tid}\"\n",
    "    \n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Save intermediate results to avoid recomputation if summarization fails\n",
    "final_report.to_csv('output_csvs/SBERT_DD_new_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ed1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''üëÅÔ∏è Cell 8 ‚Äì Final Preview: A Glimpse Into The Insight\n",
    "After all the work ‚Äî loading, embedding, filtering, batching, labeling, aggregating ‚Äî it‚Äôs time to answer:\n",
    "\n",
    "‚ÄúWhat did we actually get out of this pipeline?‚Äù\n",
    "\n",
    "This cell does just that: it opens the curtains on the final report and gives us a preview without overwhelming us.\n",
    "\n",
    "üñºÔ∏è Step 1: Print Summary View\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio']].head())\n",
    "We show only the essential columns:\n",
    "\n",
    "üïπÔ∏è steam_appid ‚Äì which game this row belongs to\n",
    "\n",
    "üè∑Ô∏è Theme ‚Äì the specific topic (e.g., ‚Äústory‚Äù, ‚Äúvisuals‚Äù)\n",
    "\n",
    "üî¢ #Reviews ‚Äì how many reviews were assigned to this theme\n",
    "\n",
    "üëç LikeRatio ‚Äì what percentage of those were positive\n",
    "\n",
    "This is the part you could easily show in a dashboard or export to a spreadsheet.\n",
    "\n",
    "üßê Step 2: Check Raw Review Content\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "sample_reviews = final_report['Reviews'].iloc[0]\n",
    "Each entry in the Reviews column is actually a list of raw review texts ‚Äî grouped by game and theme. But we don‚Äôt print the whole list ‚Äî that would flood the terminal.\n",
    "\n",
    "Instead, we:\n",
    "\n",
    "Confirm it's a list ‚úÖ\n",
    "\n",
    "Print how many reviews it contains\n",
    "\n",
    "Show just the first review, safely truncated at 100 characters\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(f\"First review (truncated): {sample_reviews[0][:100]}...\")\n",
    "This is useful for:\n",
    "\n",
    "üß™ Sanity checking: Are the reviews grouped correctly?\n",
    "\n",
    "üìù Summarization: This is what the summarizer will later process into a ‚Äúquick insights‚Äù paragraph.\n",
    "\n",
    "‚úÖ Step 3: Clean Exit\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "client.close()\n",
    "We politely shut down the Dask client ‚Äî freeing up RAM, CPU, and GPU. This is especially good hygiene for heavy pipelines running in notebooks or servers.\n",
    "\n",
    "üßæ TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell previews the final report, showing game-wise theme stats and verifying that raw reviews were correctly grouped. It offers a final check before visualization or summarization ‚Äî and closes the Dask client cleanly.‚Äù'''\n",
    "# Cell 8: View the Report\n",
    "# Print preview of the DataFrame (excluding the Reviews column as it contains lists)\n",
    "print(\"Final report preview (Reviews column contains lists of review texts):\")\n",
    "print(final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio']].head())\n",
    "\n",
    "# Verify that Reviews column contains lists\n",
    "sample_reviews = final_report['Reviews'].iloc[0]\n",
    "print(f\"\\nSample from first Reviews entry (showing first review only):\")\n",
    "if isinstance(sample_reviews, list) and len(sample_reviews) > 0:\n",
    "    print(f\"Number of reviews in list: {len(sample_reviews)}\")\n",
    "    print(f\"First review (truncated): {sample_reviews[0][:100]}...\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6242442",
   "metadata": {},
   "source": [
    "# Tuned for my hardware 1m 50 secs inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''‚ö° Cell 9 ‚Äì The Fast Lane: Hardware-Optimized Summarization for RTX 4080 Super + Ryzen 9700X\n",
    "You‚Äôve built the themes. Assigned the topics. Collected the data.\n",
    "Now it‚Äôs time to summarize thousands of game reviews, game by game, theme by theme ‚Äî into short, readable insights.\n",
    "\n",
    "But summarization is expensive ‚Äî it runs on transformer models, which means:\n",
    "\n",
    "High VRAM usage üíæ\n",
    "\n",
    "High CPU throughput üß†\n",
    "\n",
    "High memory churn ‚öôÔ∏è\n",
    "\n",
    "So we don‚Äôt just run it ‚Äî we engineer the entire process for your specific hardware: an RTX 4080 Super + Ryzen 9700X + 20GB RAM workstation.\n",
    "\n",
    "Let‚Äôs break it down.\n",
    "\n",
    "üõ†Ô∏è Step 1: Build an Optimized Runtime Environment\n",
    "We define a HARDWARE_CONFIG:\n",
    "\n",
    "6 workers, each using 3GB memory\n",
    "\n",
    "96 reviews per GPU batch ‚Äî a sweet spot for the RTX 4080\n",
    "\n",
    "‚Äòsshleifer/distilbart-cnn-12-6‚Äô ‚Äî a fast, high-quality summarization model\n",
    "\n",
    "Chunk sizes, checkpoint frequency, and cleanup parameters ‚Äî all hand-tuned\n",
    "\n",
    "Then we launch:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "cluster = LocalCluster(...)\n",
    "client = Client(cluster)\n",
    "This is the second Dask cluster, purpose-built for heavy summarization.\n",
    "\n",
    "üß± Step 2: Create Balanced Partitions\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "prepare_partition(start_idx, end_idx)\n",
    "We divide the final_report into N partitions, one per worker. Each partition contains a chunk of games to process. Bigger chunks mean:\n",
    "\n",
    "Fewer I/O calls\n",
    "\n",
    "Better GPU utilization\n",
    "\n",
    "Faster throughput\n",
    "\n",
    "Each partition is prepared using dask.delayed for lazy evaluation.\n",
    "\n",
    "ü§ñ Step 3: Custom Worker Function\n",
    "We define a highly tuned function:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def process_partition(partition_df, worker_id):\n",
    "Each worker:\n",
    "\n",
    "Initializes its own summarizer pipeline, using:\n",
    "\n",
    "Half-precision (float16) for speed\n",
    "\n",
    "device_map=\"auto\" for seamless GPU assignment\n",
    "\n",
    "Reports its own GPU memory usage\n",
    "\n",
    "Batches reviews into large chunks, summarized with:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "summarizer(..., num_beams=2)\n",
    "We use a hierarchical summarization strategy:\n",
    "\n",
    "For small review sets ‚Üí summarize directly\n",
    "\n",
    "For large ones ‚Üí break into chunks ‚Üí summarize each ‚Üí then summarize the summaries\n",
    "\n",
    "This gives fast + coherent outputs, while preventing model overload.\n",
    "\n",
    "‚è±Ô∏è Step 4: Schedule and Track\n",
    "We submit all partitioned tasks:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "futures = client.compute(delayed_results)\n",
    "And we monitor progress with a background thread that updates a tqdm progress bar every 5 seconds ‚Äî with zero UI lag and minimal CPU load.\n",
    "\n",
    "‚öôÔ∏è Step 5: Execution and Fallback\n",
    "If distributed futures fail, we fallback to:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "results = dask.compute(*delayed_results)\n",
    "This ensures robustness, even in environments where GPU config changes.\n",
    "\n",
    "üìã Step 6: Assemble the Results\n",
    "Once done, we:\n",
    "\n",
    "Collect and sort the results\n",
    "\n",
    "Extract the summaries\n",
    "\n",
    "Inject them into a new column in final_report:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_report['QuickSummary'] = summaries\n",
    "Each row now has a bite-sized summary of all reviews for a specific theme in a specific game.\n",
    "\n",
    "üíæ Step 7: Output & Clean-Up\n",
    "We report performance:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "Average time per item: 0.93 seconds\n",
    "Then:\n",
    "\n",
    "Preview the results\n",
    "\n",
    "Save the full CSV to optimized_hardware_report.csv\n",
    "\n",
    "Shut down the Dask cluster cleanly\n",
    "\n",
    "üìå TL;DR for Presentation Slide:\n",
    "‚ÄúThis cell uses hardware-tuned Dask workers and half-precision transformers to summarize thousands of grouped reviews in parallel. With batching, chunking, hierarchical compression, and aggressive memory reuse, it extracts fast, theme-specific insights from massive text volumes ‚Äî in under a second per summary.‚Äù'''\n",
    "# Cell 9: Hardware-optimized GPU summarization with Dask - Tuned for Ryzen 9700X & RTX 4080 Super\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist (minimal overhead)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Optimized configuration for your specific hardware\n",
    "# RTX 4080 Super (12GB usable VRAM) + Ryzen 9700X + 20GB usable RAM\n",
    "HARDWARE_CONFIG = {\n",
    "    'worker_count': 6,                # Optimal for Ryzen 9700X\n",
    "    'memory_per_worker': '3GB',       # 18GB total for workers, leaving headroom\n",
    "    'gpu_batch_size': 96,             # Aggressive batch size for RTX 4080 Super\n",
    "    'model_name': 'sshleifer/distilbart-cnn-12-6',  # Best model for your GPU\n",
    "    'chunk_size': 400,                # Larger chunks for faster processing\n",
    "    'checkpoint_frequency': 25,       # Less frequent checkpoints for speed\n",
    "    'cleanup_frequency': 10,          # Less frequent memory cleanup\n",
    "}\n",
    "\n",
    "print(f\"Starting optimized Dask cluster for Ryzen 9700X + RTX 4080 Super configuration\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=HARDWARE_CONFIG['worker_count'], \n",
    "    threads_per_worker=2,\n",
    "    memory_limit=HARDWARE_CONFIG['memory_per_worker']\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# Determine optimal partition sizes - larger for better throughput\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx):\n",
    "    \"\"\"Prepare a partition optimized for high-end hardware\"\"\"\n",
    "    return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Create larger partitions for better throughput\n",
    "n_workers = HARDWARE_CONFIG['worker_count']\n",
    "partition_size = len(final_report) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx))\n",
    "    print(f\"Prepared partition {i+1} with {end_idx-start_idx} items\")\n",
    "\n",
    "# Optimized worker function with aggressive resource usage\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Optimized worker for RTX 4080 Super\"\"\"\n",
    "    # Import needed packages\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    # Load model components with optimal settings for RTX 4080 Super\n",
    "    print(f\"Worker {worker_id} initializing with optimized settings for RTX 4080 Super\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HARDWARE_CONFIG['model_name'])\n",
    "    \n",
    "    # Load model with optimized settings for RTX 4080 Super\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        HARDWARE_CONFIG['model_name'],\n",
    "        torch_dtype=torch.float16,        # Half precision for speed\n",
    "        device_map=\"auto\",                # Automatic device placement\n",
    "        low_cpu_mem_usage=True            # Optimized memory usage\n",
    "    )\n",
    "    \n",
    "    # Create optimized pipeline\n",
    "    summarizer = pipeline(\n",
    "        task='summarization',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework='pt',\n",
    "        model_kwargs={\n",
    "            \"use_cache\": True,            # Enable caching for speed\n",
    "            \"return_dict_in_generate\": True  # More efficient generation\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Report GPU status\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "    # Highly optimized batch processing function\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in large batches for RTX 4080 Super\"\"\"\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Use large batches for the RTX 4080 Super\n",
    "        for i in range(0, len(chunks), HARDWARE_CONFIG['gpu_batch_size']):\n",
    "            batch = chunks[i:i+HARDWARE_CONFIG['gpu_batch_size']]\n",
    "            batch_summaries = summarizer(\n",
    "                batch,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False,\n",
    "                num_beams=2  # Use beam search for better quality with minimal speed impact\n",
    "            )\n",
    "            all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "            # Minimal cleanup - only when really needed\n",
    "            if i % (HARDWARE_CONFIG['gpu_batch_size'] * 3) == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                    \n",
    "        return all_summaries\n",
    "    \n",
    "    # Optimized hierarchical summary function\n",
    "    def hierarchical_summary(reviews):\n",
    "        \"\"\"Create hierarchical summary with optimized chunk sizes\"\"\"\n",
    "        # Handle edge cases efficiently\n",
    "        if not reviews or not isinstance(reviews, list):\n",
    "            return \"No reviews available for summarization.\"\n",
    "        \n",
    "        # Fast path for small review sets\n",
    "        if len(reviews) <= HARDWARE_CONFIG['chunk_size']:\n",
    "            doc = \"\\n\\n\".join(reviews)\n",
    "            return summarizer(\n",
    "                doc,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "        \n",
    "        # Process larger review sets with optimized chunking\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), HARDWARE_CONFIG['chunk_size']):\n",
    "            batch = reviews[i:i+HARDWARE_CONFIG['chunk_size']]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with optimized batching\n",
    "        intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "        # Create final summary\n",
    "        joined = \" \".join(intermediate_summaries)\n",
    "        return summarizer(\n",
    "            joined,\n",
    "            max_length=60,\n",
    "            min_length=20,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # Process the partition with minimal overhead\n",
    "    results = []\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            # Process the review\n",
    "            summary = hierarchical_summary(row['Reviews'])\n",
    "            results.append((idx, summary))\n",
    "            \n",
    "            # Minimal cleanup - only every N iterations\n",
    "            if len(results) % HARDWARE_CONFIG['cleanup_frequency'] == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Worker {worker_id} completed successfully\")\n",
    "    return results\n",
    "\n",
    "# Schedule tasks\n",
    "print(f\"Scheduling {n_workers} optimized partitions...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "\n",
    "# Streamlined progress tracking\n",
    "print(\"\\nStarting optimized computation...\")\n",
    "main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Minimal checkpoint system - only save occasionally\n",
    "def update_main_progress(futures):\n",
    "    while not stop_flag:\n",
    "        # Count completed futures\n",
    "        completed_count = sum(f.status == 'finished' for f in futures)\n",
    "        completed_percentage = completed_count / len(futures)\n",
    "        \n",
    "        # Update progress bar\n",
    "        main_progress.n = int(len(final_report) * completed_percentage)\n",
    "        main_progress.refresh()\n",
    "        \n",
    "        # Only check every 5 seconds to reduce overhead\n",
    "        time.sleep(5)\n",
    "\n",
    "# Submit tasks to cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start progress monitor with minimal overhead\n",
    "stop_flag = False\n",
    "monitor_thread = threading.Thread(target=update_main_progress, args=(futures,))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for computation\n",
    "try:\n",
    "    print(\"Computing with optimal settings for RTX 4080 Super...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join(timeout=3)\n",
    "\n",
    "# Update progress to completion\n",
    "main_progress.n = len(final_report)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# Process results efficiently\n",
    "all_results = []\n",
    "for worker_results in results:\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "# Sort results\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Report timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nOptimized processing completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Average time per item: {elapsed_time/len(final_report):.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults sample:\")\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Save results\n",
    "final_report.to_csv('output_csvs/optimized_hardware_report.csv')\n",
    "print(\"Results saved to output_csvs/optimized_hardware_report.csv\")\n",
    "\n",
    "# Clean up\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
