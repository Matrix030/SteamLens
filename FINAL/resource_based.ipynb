{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Cell 1: Dynamic resource allocation for Dask Client\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "\n",
    "# Dynamically determine system resources\n",
    "def get_system_resources():\n",
    "    # Get available memory (in GB)\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)\n",
    "    # Get CPU count\n",
    "    cpu_count = psutil.cpu_count(logical=False)  # Physical cores only\n",
    "    if not cpu_count:\n",
    "        cpu_count = psutil.cpu_count(logical=True)  # Logical if physical not available\n",
    "    \n",
    "    # Use 70% of available memory for Dask, split across workers\n",
    "    dask_memory = int(total_memory * 0.7)\n",
    "    # Determine optimal worker count (leave at least 1 core for system)\n",
    "    worker_count = max(1, cpu_count - 1)\n",
    "    # Memory per worker\n",
    "    memory_per_worker = int(dask_memory / worker_count)\n",
    "    \n",
    "    return {\n",
    "        'worker_count': worker_count,\n",
    "        'memory_per_worker': memory_per_worker,\n",
    "        'total_memory': total_memory\n",
    "    }\n",
    "\n",
    "# Get system resources\n",
    "resources = get_system_resources()\n",
    "print(f\"System has {resources['total_memory']:.1f}GB memory and {resources['worker_count']} CPU cores\")\n",
    "print(f\"Allocating {resources['worker_count']} workers with {resources['memory_per_worker']}GB each\")\n",
    "\n",
    "# Start a local Dask cluster with dynamically determined resources\n",
    "cluster = LocalCluster(\n",
    "    n_workers=resources['worker_count'],\n",
    "    threads_per_worker=2,\n",
    "    memory_limit=f\"{resources['memory_per_worker']}GB\"\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Theme Dictionary & Optimize Theme Embeddings\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get theme embeddings for specific app IDs\n",
    "# This avoids loading all embeddings at once\n",
    "def get_theme_embeddings(app_ids):\n",
    "    \"\"\"Get theme embeddings for a specific set of app IDs\"\"\"\n",
    "    embeddings = {}\n",
    "    for appid in app_ids:\n",
    "        if appid not in embeddings and appid in GAME_THEMES:\n",
    "            emb_list = []\n",
    "            for theme, seeds in GAME_THEMES[appid].items():\n",
    "                seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "                emb_list.append(seed_emb.mean(axis=0))\n",
    "            embeddings[appid] = np.vstack(emb_list)\n",
    "    return embeddings# Cell 2: Load Theme Dictionary & Optimize Theme Embeddings\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get theme embeddings for specific app IDs\n",
    "# This avoids loading all embeddings at once\n",
    "def get_theme_embeddings(app_ids):\n",
    "    \"\"\"Get theme embeddings for a specific set of app IDs\"\"\"\n",
    "    embeddings = {}\n",
    "    for appid in app_ids:\n",
    "        if appid not in embeddings and appid in GAME_THEMES:\n",
    "            emb_list = []\n",
    "            for theme, seeds in GAME_THEMES[appid].items():\n",
    "                seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "                emb_list.append(seed_emb.mean(axis=0))\n",
    "            embeddings[appid] = np.vstack(emb_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Cell 3: Dynamic blocksize for reading Parquet Files\n",
    "# Estimate dataset size first\n",
    "def estimate_dataset_size(path):\n",
    "    import os\n",
    "    total_size = 0\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size / (1024**3)  # Convert to GB\n",
    "\n",
    "# Estimate dataset size\n",
    "dataset_path = 'parquet_output_theme_combo'\n",
    "estimated_size = estimate_dataset_size(dataset_path)\n",
    "print(f\"Estimated dataset size: {estimated_size:.2f}GB\")\n",
    "\n",
    "# Dynamically determine blocksize based on dataset and memory\n",
    "# Use smaller blocks for larger datasets to prevent memory issues\n",
    "if estimated_size > 100:  # Very large dataset\n",
    "    blocksize = '16MB'\n",
    "elif estimated_size > 10:  # Medium-large dataset\n",
    "    blocksize = '32MB'\n",
    "else:  # Smaller dataset\n",
    "    blocksize = '64MB'\n",
    "\n",
    "print(f\"Using dynamic blocksize: {blocksize}\")\n",
    "\n",
    "# Read with dynamic blocksize\n",
    "ddf = dd.read_parquet(\n",
    "    f'{dataset_path}/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up'],\n",
    "    blocksize=blocksize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bedc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Optimized Partition-wise Topic Assignment\n",
    "def assign_topic(df_partition):\n",
    "    \"\"\"Assign topics using only theme embeddings for app IDs in this partition\"\"\"\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "    \n",
    "    # Get unique app IDs in this partition\n",
    "    app_ids = df_partition['steam_appid'].unique().tolist()\n",
    "    app_ids = [int(appid) for appid in app_ids]\n",
    "    \n",
    "    # Get embeddings only for app IDs in this partition\n",
    "    local_theme_embeddings = get_theme_embeddings(app_ids)\n",
    "    \n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go with batching\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        appid = int(appid)\n",
    "        if appid in local_theme_embeddings:\n",
    "            theme_embs = local_theme_embeddings[appid]\n",
    "            sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "            topic_ids.append(int(sims.argmax()))\n",
    "        else:\n",
    "            # Default topic if theme embeddings not available\n",
    "            topic_ids.append(0)\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Cell 6: Dynamic batch sizing for aggregation\n",
    "# Get unique app IDs\n",
    "unique_app_ids = ddf['steam_appid'].unique().compute()\n",
    "total_app_ids = len(unique_app_ids)\n",
    "\n",
    "# Dynamically determine batch size based on number of app IDs and memory\n",
    "# For larger datasets, use smaller batches to avoid memory issues\n",
    "if total_app_ids > 1000:  # Very large number of app IDs\n",
    "    batch_size = 3\n",
    "elif total_app_ids > 500:  # Medium-large number\n",
    "    batch_size = 5\n",
    "elif total_app_ids > 100:  # Medium number\n",
    "    batch_size = 10\n",
    "else:  # Smaller number\n",
    "    batch_size = 20\n",
    "\n",
    "print(f\"Processing {total_app_ids} unique app IDs with batch size {batch_size}\")\n",
    "\n",
    "# Initialize empty dataframes for results\n",
    "all_agg_dfs = []\n",
    "all_review_dfs = []\n",
    "\n",
    "# Process in dynamically sized batches\n",
    "for i in tqdm(range(0, len(unique_app_ids), batch_size)):\n",
    "    batch_app_ids = unique_app_ids[i:i+batch_size]\n",
    "    \n",
    "    # Filter data for this batch of app IDs\n",
    "    batch_ddf = ddf_with_topic[ddf_with_topic['steam_appid'].isin(batch_app_ids)]\n",
    "    \n",
    "    # Aggregate for this batch\n",
    "    agg = batch_ddf.groupby(['steam_appid', 'topic_id']).agg(\n",
    "        review_count=('review', 'count'),\n",
    "        likes_sum=('voted_up', 'sum')\n",
    "    )\n",
    "    \n",
    "    # Collect reviews for this batch\n",
    "    reviews_series = batch_ddf.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "        .apply(lambda x: list(x), meta=('review', object))\n",
    "    \n",
    "    # Compute both in parallel\n",
    "    agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    agg_df = agg_df.reset_index()\n",
    "    reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "    \n",
    "    # Append to results\n",
    "    all_agg_dfs.append(agg_df)\n",
    "    all_review_dfs.append(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58840673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Construct Final Report DataFrame\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    \n",
    "    # Check if appid exists in GAME_THEMES\n",
    "    if appid in GAME_THEMES:\n",
    "        theme_keys = list(GAME_THEMES[appid].keys())\n",
    "        # Check if tid is a valid index\n",
    "        if tid < len(theme_keys):\n",
    "            theme_name = theme_keys[tid]\n",
    "        else:\n",
    "            theme_name = f\"Unknown Theme {tid}\"\n",
    "    else:\n",
    "        theme_name = f\"Unknown Theme {tid}\"\n",
    "    \n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Save intermediate results to avoid recomputation if summarization fails\n",
    "final_report.to_csv('output_csvs/SBERT_DD_new_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: View the Report\n",
    "# Print preview of the DataFrame (excluding the Reviews column as it contains lists)\n",
    "print(\"Final report preview (Reviews column contains lists of review texts):\")\n",
    "print(final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio']].head())\n",
    "\n",
    "# Verify that Reviews column contains lists\n",
    "sample_reviews = final_report['Reviews'].iloc[0]\n",
    "print(f\"\\nSample from first Reviews entry (showing first review only):\")\n",
    "if isinstance(sample_reviews, list) and len(sample_reviews) > 0:\n",
    "    print(f\"Number of reviews in list: {len(sample_reviews)}\")\n",
    "    print(f\"First review (truncated): {sample_reviews[0][:100]}...\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea965f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Dynamically optimized GPU hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import json\n",
    "import threading\n",
    "\n",
    "# 1. Dynamic resource allocation based on system capabilities\n",
    "def get_system_resources():\n",
    "    \"\"\"Determine optimal system resource allocation\"\"\"\n",
    "    # Get available memory and CPU resources\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)  # GB\n",
    "    available_memory = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "    cpu_count = psutil.cpu_count(logical=False) or psutil.cpu_count(logical=True)\n",
    "    \n",
    "    # Check for GPU presence and memory\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_count = torch.cuda.device_count() if gpu_available else 0\n",
    "    gpu_memory = [torch.cuda.get_device_properties(i).total_memory / (1024**3) for i in range(gpu_count)] if gpu_available else []\n",
    "    \n",
    "    # Determine optimal worker count - leave cores for system and GPU processes\n",
    "    if gpu_available:\n",
    "        # For GPU workloads, fewer workers but more memory per worker\n",
    "        worker_count = min(max(1, cpu_count // 2), gpu_count + 1)\n",
    "    else:\n",
    "        # For CPU workloads, use more workers\n",
    "        worker_count = max(1, cpu_count - 1)\n",
    "    \n",
    "    # Memory per worker (70% of available to leave headroom)\n",
    "    safe_memory = available_memory * 0.7\n",
    "    memory_per_worker = safe_memory / worker_count\n",
    "    \n",
    "    # Dynamic chunk size based on available memory\n",
    "    if memory_per_worker > 8:  # High memory\n",
    "        chunk_size = 300\n",
    "    elif memory_per_worker > 4:  # Medium memory\n",
    "        chunk_size = 200\n",
    "    else:  # Low memory\n",
    "        chunk_size = 100\n",
    "    \n",
    "    print(f\"System resources: {total_memory:.1f}GB total RAM, {available_memory:.1f}GB available\")\n",
    "    print(f\"CPU cores: {cpu_count}, GPU count: {gpu_count}\")\n",
    "    if gpu_count > 0:\n",
    "        for i, mem in enumerate(gpu_memory):\n",
    "            print(f\"GPU {i}: {mem:.1f}GB memory\")\n",
    "    \n",
    "    return {\n",
    "        'worker_count': worker_count,\n",
    "        'memory_per_worker': memory_per_worker,\n",
    "        'chunk_size': chunk_size,\n",
    "        'gpu_available': gpu_available,\n",
    "        'gpu_count': gpu_count,\n",
    "        'gpu_memory': gpu_memory\n",
    "    }\n",
    "\n",
    "# Get system resources\n",
    "resources = get_system_resources()\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Start a local Dask cluster with dynamic resources\n",
    "n_workers = resources['worker_count']\n",
    "print(f\"Starting Dask cluster with {n_workers} workers, {resources['memory_per_worker']:.1f}GB per worker\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=n_workers, \n",
    "    threads_per_worker=2,\n",
    "    memory_limit=f\"{resources['memory_per_worker']:.1f}GB\"\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# 2. Determine model based on available resources\n",
    "def select_model():\n",
    "    \"\"\"Select appropriate model based on available resources\"\"\"\n",
    "    if resources['gpu_available'] and any(mem > 8 for mem in resources['gpu_memory']):\n",
    "        # For high-end GPUs, use more powerful model\n",
    "        return 'sshleifer/distilbart-cnn-12-6'\n",
    "    elif resources['gpu_available']:\n",
    "        # For lower-end GPUs, use smaller model\n",
    "        return 'facebook/bart-large-cnn'\n",
    "    else:\n",
    "        # For CPU-only, use smallest model\n",
    "        return 'facebook/bart-base'\n",
    "\n",
    "# Select model based on resources\n",
    "MODEL_NAME = select_model()\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "\n",
    "# 3. First, load the data and check for existing checkpoints\n",
    "def load_with_checkpoint():\n",
    "    \"\"\"Load data with checkpoint recovery\"\"\"\n",
    "    checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"Found checkpoint with {len(checkpoint)} completed summaries\")\n",
    "            \n",
    "        # Filter the dataframe to only process remaining rows\n",
    "        completed_indices = list(map(int, checkpoint.keys()))\n",
    "        remaining_df = final_report[~final_report.index.isin(completed_indices)].copy()\n",
    "        \n",
    "        print(f\"Resuming processing for {len(remaining_df)} remaining items\")\n",
    "        return remaining_df, checkpoint\n",
    "    else:\n",
    "        print(\"No checkpoint found, processing all items\")\n",
    "        return final_report, {}\n",
    "\n",
    "# Load data with checkpoint support\n",
    "df_to_process, existing_summaries = load_with_checkpoint()\n",
    "\n",
    "# 4. Prepare partitions with optimized distribution\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx, df):\n",
    "    \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "    # Get just this partition\n",
    "    return df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Distribute the remaining work\n",
    "partition_size = len(df_to_process) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(df_to_process)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx, df_to_process))\n",
    "\n",
    "# 5. Worker processing function with dynamic GPU batch sizing\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition with dynamic batch sizes and error recovery\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # Determine optimal GPU batch size based on available memory\n",
    "    def determine_gpu_batch_size():\n",
    "        if not torch.cuda.is_available():\n",
    "            return 8  # Conservative default for CPU\n",
    "            \n",
    "        try:\n",
    "            # Get GPU memory info for this worker\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "            # Reserve 10% for system processes and overhead\n",
    "            usable_mem = total_mem * 0.9\n",
    "            \n",
    "            # Scale batch size based on available GPU memory\n",
    "            if usable_mem > 16:  # High-end GPU with >16GB\n",
    "                return 64\n",
    "            elif usable_mem > 8:  # Mid-range GPU with >8GB\n",
    "                return 32\n",
    "            elif usable_mem > 4:  # Lower-end GPU with >4GB\n",
    "                return 16\n",
    "            else:  # Minimal GPU\n",
    "                return 8\n",
    "        except Exception as e:\n",
    "            print(f\"Error determining GPU batch size: {e}\")\n",
    "            return 8  # Conservative fallback\n",
    "    \n",
    "    # Worker initialization with error handling\n",
    "    try:\n",
    "        # Load tokenizer first\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # Configure device placement based on available resources\n",
    "        if torch.cuda.is_available():\n",
    "            device_map = \"auto\"\n",
    "            dtype = torch.float16  # Use half precision with GPU\n",
    "        else:\n",
    "            device_map = None\n",
    "            dtype = torch.float32  # Use full precision with CPU\n",
    "        \n",
    "        # Load model with appropriate configuration\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device_map,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Create pipeline with model AND tokenizer\n",
    "        summarizer = pipeline(\n",
    "            task='summarization',\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            framework='pt',\n",
    "            model_kwargs={\"use_cache\": True}\n",
    "        )\n",
    "        \n",
    "        # Report worker status\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "            MAX_GPU_BATCH_SIZE = determine_gpu_batch_size()\n",
    "            print(f\"Worker {worker_id}: Using GPU batch size: {MAX_GPU_BATCH_SIZE}\")\n",
    "        else:\n",
    "            MAX_GPU_BATCH_SIZE = 8\n",
    "            print(f\"Worker {worker_id}: Using CPU with batch size: {MAX_GPU_BATCH_SIZE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Worker {worker_id} initialization error: {e}\")\n",
    "        # Fall back to a simpler configuration\n",
    "        try:\n",
    "            print(f\"Falling back to CPU-only mode for worker {worker_id}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "            summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "            MAX_GPU_BATCH_SIZE = 4  # Conservative batch size for fallback mode\n",
    "        except Exception as e2:\n",
    "            print(f\"Critical failure in worker {worker_id}: {e2}\")\n",
    "            return []  # Return empty results to avoid deadlock\n",
    "    \n",
    "    # Efficient batch processing function with memory management\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in batches with dynamic memory management\"\"\"\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Process in dynamically sized batches\n",
    "        for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "            try:\n",
    "                batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "                batch_summaries = summarizer(\n",
    "                    batch,\n",
    "                    max_length=60,\n",
    "                    min_length=20,\n",
    "                    truncation=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "                \n",
    "                # Proactively manage memory\n",
    "                if i % (MAX_GPU_BATCH_SIZE * 2) == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//MAX_GPU_BATCH_SIZE} of worker {worker_id}: {e}\")\n",
    "                # Try smaller batch on failure\n",
    "                if len(batch) > 1:\n",
    "                    print(\"Retrying with smaller batches...\")\n",
    "                    for single_item in batch:\n",
    "                        try:\n",
    "                            summary = summarizer(\n",
    "                                [single_item],\n",
    "                                max_length=60,\n",
    "                                min_length=20,\n",
    "                                truncation=True,\n",
    "                                do_sample=False\n",
    "                            )\n",
    "                            all_summaries.append(summary[0][\"summary_text\"])\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Failed to process single item: {e2}\")\n",
    "                            all_summaries.append(\"Error generating summary.\")\n",
    "                else:\n",
    "                    all_summaries.append(\"Error generating summary.\")\n",
    "                \n",
    "                # Clean up after errors\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        return all_summaries\n",
    "    \n",
    "    # Hierarchical summary function with adaptive chunking\n",
    "    def hierarchical_summary(reviews, base_chunk_size=200):\n",
    "        \"\"\"Create hierarchical summary with adaptive chunk sizing\"\"\"\n",
    "        # Defense against empty or invalid reviews\n",
    "        if not reviews or not isinstance(reviews, list):\n",
    "            return \"No reviews available for summarization.\"\n",
    "        \n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= base_chunk_size:\n",
    "            try:\n",
    "                # Join reviews with clear separation\n",
    "                doc = \"\\n\\n\".join(reviews[:base_chunk_size])\n",
    "                return summarizer(\n",
    "                    doc,\n",
    "                    max_length=60,\n",
    "                    min_length=20,\n",
    "                    truncation=True,\n",
    "                    do_sample=False\n",
    "                )[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                print(f\"Error summarizing small batch: {e}\")\n",
    "                # Try with even smaller batch if original fails\n",
    "                try:\n",
    "                    half_size = len(reviews) // 2\n",
    "                    doc = \"\\n\\n\".join(reviews[:half_size])\n",
    "                    return summarizer(\n",
    "                        doc,\n",
    "                        max_length=60,\n",
    "                        min_length=20, \n",
    "                        truncation=True,\n",
    "                        do_sample=False\n",
    "                    )[0]['summary_text']\n",
    "                except:\n",
    "                    return \"Error generating summary for this batch.\"\n",
    "        \n",
    "        # Adaptively determine chunk size based on review length\n",
    "        # If reviews are very short, use larger chunks\n",
    "        avg_review_len = sum(len(r) for r in reviews[:100]) / min(100, len(reviews))\n",
    "        if avg_review_len < 100:  # Very short reviews\n",
    "            chunk_size = min(base_chunk_size * 2, 500)\n",
    "        elif avg_review_len > 500:  # Very long reviews\n",
    "            chunk_size = max(base_chunk_size // 2, 50)\n",
    "        else:\n",
    "            chunk_size = base_chunk_size\n",
    "            \n",
    "        print(f\"Worker {worker_id}: Using chunk size {chunk_size} for avg review length {avg_review_len:.1f}\")\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with batched processing\n",
    "        try:\n",
    "            intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "            \n",
    "            # Summarize the intermediate summaries\n",
    "            joined = \" \".join(intermediate_summaries)\n",
    "            final_summary = summarizer(\n",
    "                joined,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "            \n",
    "            return final_summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error in hierarchical summarization: {e}\")\n",
    "            # Try to salvage what we can\n",
    "            if intermediate_summaries:\n",
    "                try:\n",
    "                    return f\"Partial summary: {' '.join(intermediate_summaries[:3])}\"\n",
    "                except:\n",
    "                    pass\n",
    "            return \"Error generating hierarchical summary.\"\n",
    "    \n",
    "    # Process the partition with checkpointing\n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            try:\n",
    "                # Skip processing if we already have too many errors in a row\n",
    "                if processed_count > 0 and len(results) == 0:\n",
    "                    # If first N items all failed, skip this worker\n",
    "                    if processed_count >= 5:\n",
    "                        print(f\"Worker {worker_id} failing consistently, aborting\")\n",
    "                        break\n",
    "                \n",
    "                # Process the review with the adaptive chunk size\n",
    "                summary = hierarchical_summary(row['Reviews'], base_chunk_size=resources['chunk_size'])\n",
    "                results.append((idx, summary))\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Clean up every few iterations\n",
    "                if processed_count % 5 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                # Checkpoint every 10 items\n",
    "                if processed_count % 10 == 0:\n",
    "                    print(f\"Worker {worker_id}: Processed {processed_count}/{len(partition_df)} items\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx} in worker {worker_id}: {e}\")\n",
    "                # Still record the error so we know this row was attempted\n",
    "                results.append((idx, f\"Error: Failed to generate summary.\"))\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Final cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Worker {worker_id} completed: {len(results)}/{len(partition_df)} successful\")\n",
    "    return results\n",
    "\n",
    "# 6. Schedule the tasks with the delayed partitions\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# 7. Progress tracking and checkpointing\n",
    "# Create main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(df_to_process), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater with checkpointing\n",
    "def update_main_progress(futures):\n",
    "    \"\"\"Update progress bar and save checkpoints\"\"\"\n",
    "    checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "    summaries_so_far = existing_summaries.copy()\n",
    "    \n",
    "    while not stop_flag:\n",
    "        # Count completed futures\n",
    "        completed_count = sum(f.status == 'finished' for f in futures)\n",
    "        completed_percentage = completed_count / len(futures)\n",
    "        \n",
    "        # Update progress bar\n",
    "        main_progress.n = int(len(df_to_process) * completed_percentage)\n",
    "        main_progress.refresh()\n",
    "        \n",
    "        # Check for newly completed results and update checkpoint\n",
    "        for future in [f for f in futures if f.status == 'finished']:\n",
    "            try:\n",
    "                result = future.result()\n",
    "                for idx, summary in result:\n",
    "                    summaries_so_far[str(idx)] = summary\n",
    "            except:\n",
    "                pass  # Skip failed futures\n",
    "        \n",
    "        # Save checkpoint every 30 seconds\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(summaries_so_far, f)\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "stop_flag = False\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=update_main_progress, args=(futures,))\n",
    "monitor_thread.daemon = True  # Allow program to exit if thread is still running\n",
    "monitor_thread.start()\n",
    "\n",
    "# 8. Wait for computation to complete with robust error handling\n",
    "try:\n",
    "    print(\"Computing all partitions...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    # Fallback to direct computation if future gathering fails\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join(timeout=5)  # Wait for thread to terminate, but with timeout\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(df_to_process)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# 9. Process results with checkpoint recovery\n",
    "all_results = []\n",
    "\n",
    "# Gather results from all workers\n",
    "for worker_results in results:\n",
    "    if worker_results:  # Check if worker returned any results\n",
    "        all_results.extend(worker_results)\n",
    "\n",
    "# Load checkpoint file for any results we already had\n",
    "checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "        \n",
    "    # Add checkpoint data for any missing indices\n",
    "    result_indices = [idx for idx, _ in all_results]\n",
    "    for idx_str, summary in checkpoint_data.items():\n",
    "        idx = int(idx_str)\n",
    "        if idx not in result_indices:\n",
    "            all_results.append((idx, summary))\n",
    "\n",
    "# Sort by index to maintain order\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create a dictionary mapping of indices to summaries\n",
    "result_dict = {idx: summary for idx, summary in all_results}\n",
    "\n",
    "# Apply to final report\n",
    "final_report['QuickSummary'] = final_report.index.map(\n",
    "    lambda idx: result_dict.get(idx, \"Summary not generated\")\n",
    ")\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Successfully summarized {len(result_dict)}/{len(final_report)} items\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSample results:\")\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# 10. Save the results\n",
    "final_report.to_csv('output_csvs/dynamic_summarized_report.csv')\n",
    "print(\"Results saved to output_csvs/dynamic_summarized_report.csv\")\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
