{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (DASK DISTRIBUTED - FINAL WITH PROGRESS): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from tqdm.auto import tqdm\n",
    "# import time\n",
    "\n",
    "# # Start a local Dask cluster\n",
    "# n_workers = 4  # Adjust based on your CPU core count\n",
    "# cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "# client = Client(cluster)\n",
    "# print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# # Define model parameters \n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "# @dask.delayed\n",
    "# def process_partition(partition_df, worker_id):\n",
    "#     \"\"\"Process a partition of the data on a worker\"\"\"\n",
    "#     # Import packages needed in the worker\n",
    "#     from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "#     import torch\n",
    "#     from tqdm.auto import tqdm\n",
    "    \n",
    "#     # Load tokenizer first\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "#     # Load model with device_map=\"auto\" only\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#         MODEL_NAME,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         device_map=\"auto\"  # This will handle device placement automatically\n",
    "#     )\n",
    "    \n",
    "#     # Create pipeline with both model AND tokenizer\n",
    "#     summarizer = pipeline(\n",
    "#         task='summarization',\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         framework='pt',\n",
    "#         model_kwargs={\"use_cache\": True}\n",
    "#     )\n",
    "    \n",
    "#     # Report worker GPU status\n",
    "#     gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "#     print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "#     # Define the hierarchical summary function within the worker\n",
    "#     def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#         # If there are fewer than chunk_size, just do one summary\n",
    "#         if len(reviews) <= chunk_size:\n",
    "#             doc = \"\\n\\n\".join(reviews)\n",
    "#             return summarizer(\n",
    "#                 doc,\n",
    "#                 max_length=max_len,\n",
    "#                 min_length=min_len,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )[0]['summary_text']\n",
    "        \n",
    "#         # Prepare all chunks for processing\n",
    "#         all_chunks = []\n",
    "#         for i in range(0, len(reviews), chunk_size):\n",
    "#             batch = reviews[i:i+chunk_size]\n",
    "#             text = \"\\n\\n\".join(batch)\n",
    "#             all_chunks.append(text)\n",
    "        \n",
    "#         # Process in large batches to utilize GPU\n",
    "#         summaries = []\n",
    "#         for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "#             batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "#             batch_summaries = summarizer(\n",
    "#                 batch,\n",
    "#                 max_length=max_len,\n",
    "#                 min_length=min_len,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )\n",
    "#             summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "        \n",
    "#         # Summarize the intermediate summaries\n",
    "#         joined = \" \".join(summaries)\n",
    "#         return summarizer(\n",
    "#             joined,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # Process the partition with a progress bar\n",
    "#     results = []\n",
    "#     # Create a progress bar for this worker\n",
    "#     with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "#         for idx, row in partition_df.iterrows():\n",
    "#             summary = hierarchical_summary(row['Reviews'], chunk_size=200, max_len=60, min_len=20)\n",
    "#             results.append((idx, summary))\n",
    "#             pbar.update(1)\n",
    "            \n",
    "#             # Clean up every few iterations\n",
    "#             if len(results) % 5 == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Clean up at the end\n",
    "#     torch.cuda.empty_cache()\n",
    "#     del model\n",
    "#     del summarizer\n",
    "    \n",
    "#     # Return the results for this partition\n",
    "#     return results\n",
    "\n",
    "# # Convert pandas DataFrame to Dask DataFrame\n",
    "# dask_df = dd.from_pandas(final_report, npartitions=n_workers)\n",
    "\n",
    "# # Set up manual progress tracking\n",
    "# print(f\"Processing {len(final_report)} rows across {n_workers} partitions...\")\n",
    "\n",
    "# # Simple approach to split the dataframe\n",
    "# partition_size = len(final_report) // n_workers\n",
    "# delayed_results = []\n",
    "\n",
    "# # Process each partition separately\n",
    "# print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "# for i in range(n_workers):\n",
    "#     # Get start and end index for this partition\n",
    "#     start_idx = i * partition_size\n",
    "#     end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    \n",
    "#     # Get this partition as a pandas DataFrame\n",
    "#     partition_df = final_report.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "#     # Create a delayed task to process this partition\n",
    "#     delayed_result = process_partition(partition_df, i)\n",
    "#     delayed_results.append(delayed_result)\n",
    "#     print(f\"Scheduled partition {i+1}/{n_workers} with {len(partition_df)} rows\")\n",
    "\n",
    "# # Create a main progress bar for overall progress\n",
    "# print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "# main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Create a global progress updater\n",
    "# def update_main_progress(future):\n",
    "#     # Update main progress bar based on worker progress\n",
    "#     # This function will be called repeatedly to update the main progress bar\n",
    "#     completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "#     main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "#     main_progress.refresh()\n",
    "\n",
    "# # Submit the tasks to the cluster\n",
    "# futures = client.compute(delayed_results)\n",
    "\n",
    "# # Start a loop to update the main progress bar\n",
    "# import threading\n",
    "# stop_flag = False\n",
    "\n",
    "# def progress_monitor():\n",
    "#     while not stop_flag:\n",
    "#         update_main_progress(futures)\n",
    "#         time.sleep(0.5)\n",
    "\n",
    "# # Start the progress monitor in a separate thread\n",
    "# monitor_thread = threading.Thread(target=progress_monitor)\n",
    "# monitor_thread.start()\n",
    "\n",
    "# # Wait for computation to complete\n",
    "# results = dask.compute(*delayed_results)\n",
    "\n",
    "# # Stop the progress monitor\n",
    "# stop_flag = True\n",
    "# monitor_thread.join()\n",
    "\n",
    "# # Update progress bar to completion\n",
    "# main_progress.n = len(final_report)\n",
    "# main_progress.refresh()\n",
    "# main_progress.close()\n",
    "\n",
    "# # Flatten the nested list of results\n",
    "# all_results = []\n",
    "# for worker_results in results:\n",
    "#     all_results.extend(worker_results)\n",
    "\n",
    "# # Sort by index\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # Report final timing\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# # Display results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # Shut down the client and cluster\n",
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (ULTRA OPTIMIZED - FIXED): Maximum GPU utilization for RTX 4080 Super\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from tqdm.auto import tqdm\n",
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # Force CUDA initialization and check memory\n",
    "# torch.cuda.init()\n",
    "# total_gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "# print(f\"Total GPU memory: {total_gpu_mem:.2f} GB\")\n",
    "\n",
    "# # Ultra-aggressive GPU optimization parameters for RTX 4080 Super\n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 64  # Much larger batch size to fully utilize VRAM\n",
    "# MAX_SEQUENCE_LENGTH = 1024  # Set maximum context length to optimize memory usage\n",
    "\n",
    "# # Load model and tokenizer directly for maximum control\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     torch_dtype=torch.float16,  # Half-precision for maximum throughput\n",
    "#     device_map=\"auto\"           # Automatically map to available GPU\n",
    "# )\n",
    "\n",
    "# # Move model to GPU and optimize for inference\n",
    "# model.to(\"cuda\")\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "# # Create a custom pipeline with maximum batch efficiency\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     framework='pt',\n",
    "#     # Force maximum GPU memory usage\n",
    "#     model_kwargs={\"use_cache\": True}\n",
    "# )\n",
    "\n",
    "# # Monitor GPU memory usage\n",
    "# def gpu_memory_usage():\n",
    "#     \"\"\"Return GPU memory usage in GB\"\"\"\n",
    "#     reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "#     allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "#     print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "#     return allocated, reserved\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     Ultra-optimized hierarchical summarization for maximum GPU utilization\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for processing with ultra-large batches\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Process in maximally large batches to saturate GPU\n",
    "#     # This is the key optimization - use much larger batches to fill VRAM\n",
    "#     summaries = []\n",
    "#     for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "#         batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "        \n",
    "#         # Log memory usage before batch\n",
    "#         print(f\"Processing batch of size {len(batch)} ({i}/{len(all_chunks)})\")\n",
    "#         gpu_memory_usage()\n",
    "        \n",
    "#         # Process maximum-sized batch\n",
    "#         batch_summaries = summarizer(\n",
    "#             batch,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )\n",
    "#         summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "        \n",
    "#         # Log memory after batch\n",
    "#         gpu_memory_usage()\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries in a single batch\n",
    "#     joined = \" \".join(summaries)\n",
    "    \n",
    "#     # Final summary\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # Pre-process all reviews to maximize throughput - FIXED THIS LINE\n",
    "# print(\"Preparing all reviews for processing...\")\n",
    "# all_rows = [(i, row['Reviews']) for i, (_, row) in enumerate(final_report.iterrows())]\n",
    "\n",
    "# # Process the entire dataset in sequential maximum-sized batches\n",
    "# # This approach ensures GPU is fully saturated\n",
    "# all_results = []\n",
    "# with tqdm(total=len(final_report), desc=\"Ultra GPU Optimization\") as pbar:\n",
    "#     # Process each row with maximum batch efficiency\n",
    "#     for i in range(0, len(all_rows), 10):  # Process in batches of 10 rows\n",
    "#         batch_rows = all_rows[i:i+10]\n",
    "#         batch_results = []\n",
    "        \n",
    "#         for batch_idx, (row_idx, reviews) in enumerate(batch_rows):\n",
    "#             # Force garbage collection before large operations\n",
    "#             if batch_idx % 5 == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 gc.collect()\n",
    "            \n",
    "#             # Process with maximum GPU utilization\n",
    "#             summary = hierarchical_summary(\n",
    "#                 reviews, \n",
    "#                 chunk_size=200, \n",
    "#                 max_len=60, \n",
    "#                 min_len=20\n",
    "#             )\n",
    "#             batch_results.append((row_idx, summary))\n",
    "#             pbar.update(1)\n",
    "        \n",
    "#         all_results.extend(batch_results)\n",
    "#         # Force GPU memory cleanup between large batches\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "\n",
    "# # Sort and store results\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # Inspect results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # Final cleanup\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b60ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (OPTIMIZED - FASTEST SO FAR): GPU-optimized hierarchical summarization for RTX 4080 Super\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import pipeline, AutoTokenizer\n",
    "# from tqdm.auto import tqdm\n",
    "# import torch\n",
    "# import concurrent.futures\n",
    "# import multiprocessing\n",
    "\n",
    "# # Check GPU memory and set optimal batch sizes\n",
    "# gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "# print(f\"Available GPU memory: {gpu_mem:.2f} GB\")\n",
    "\n",
    "# # RTX 4080 Super optimization parameters\n",
    "# # With 16GB VRAM, we can use larger batch sizes and optimize throughput\n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 32  # Larger batch size for 16GB VRAM\n",
    "# PARALLEL_PROCESSES = 4   # Optimal number for balancing CPU and GPU workloads\n",
    "\n",
    "# # 1) Initialize tokenizer to estimate token counts for optimal batching\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# # 2) Initialize the summarizer pipeline with optimized settings for RTX 4080 Super\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model=MODEL_NAME,\n",
    "#     device=0,\n",
    "#     framework='pt',\n",
    "#     # Optimized settings for higher throughput\n",
    "#     model_kwargs={\n",
    "#         \"use_cache\": True,  # Enable KV caching for faster inference\n",
    "#     },\n",
    "#     # Enable half-precision for faster processing and lower memory usage\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# def estimate_tokens(text):\n",
    "#     \"\"\"Estimate token count to optimize batching\"\"\"\n",
    "#     return len(tokenizer.encode(text))\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     GPU-optimized hierarchical summarization with dynamic batching\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for processing\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Dynamically determine optimal batch size based on token counts\n",
    "#     # For RTX 4080 Super with 16GB, we can process larger batches\n",
    "#     summaries = []\n",
    "#     for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "#         batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "#         batch_summaries = summarizer(\n",
    "#             batch,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )\n",
    "#         summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries\n",
    "#     # RTX 4080 Super can handle the full set of intermediate summaries\n",
    "#     joined = \" \".join(summaries)\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # 4) Function to process each batch of rows with GPU optimization\n",
    "# def process_gpu_batch(batch_df):\n",
    "#     results = []\n",
    "#     # Pre-collect all reviews to optimize memory transfers to GPU\n",
    "#     all_rows = [(row.name, row['Reviews']) for _, row in batch_df.iterrows()]\n",
    "    \n",
    "#     for idx, reviews in all_rows:\n",
    "#         # Use optimized hierarchical summary function\n",
    "#         summary = hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20)\n",
    "#         results.append((idx, summary))\n",
    "        \n",
    "#         # Optional: Force CUDA cache clearing every few iterations to prevent memory fragmentation\n",
    "#         if idx % 10 == 0:\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#     return results\n",
    "\n",
    "# # Calculate optimal processing strategy based on dataset size\n",
    "# total_rows = len(final_report)\n",
    "# # Determine batch size for parallel processing\n",
    "# optimal_batch_size = max(1, total_rows // PARALLEL_PROCESSES)\n",
    "\n",
    "# # Split dataframe into optimized batches\n",
    "# batches = [final_report.iloc[i:i+optimal_batch_size] for i in range(0, total_rows, optimal_batch_size)]\n",
    "\n",
    "# # Process with concurrent.futures and progress tracking\n",
    "# all_results = []\n",
    "# with tqdm(total=total_rows, desc=\"GPU Summarizing (RTX 4080 Super)\") as pbar:\n",
    "#     # Use ThreadPoolExecutor to manage parallel GPU tasks\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=PARALLEL_PROCESSES) as executor:\n",
    "#         future_to_batch = {executor.submit(process_gpu_batch, batch): batch for batch in batches}\n",
    "        \n",
    "#         for future in concurrent.futures.as_completed(future_to_batch):\n",
    "#             try:\n",
    "#                 batch_results = future.result()\n",
    "#                 all_results.extend(batch_results)\n",
    "#                 batch_size = len(future_to_batch[future])\n",
    "#                 pbar.update(batch_size)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing batch: {e}\")\n",
    "#                 # Continue with remaining batches\n",
    "\n",
    "# # Sort results by the original index\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # 5) Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # 6) Inspect results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # 7) Clean up GPU memory\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (BIG DATA - BUT WITH PYTHON-FUTURES): Hierarchical summarization of all reviews per theme using parallel processing\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import pipeline\n",
    "# from tqdm.auto import tqdm\n",
    "# import concurrent.futures\n",
    "# import multiprocessing\n",
    "\n",
    "# # Set up the number of workers based on CPU cores (with one less to avoid overloading)\n",
    "# num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "# # 1) Initialize the summarizer pipeline globally\n",
    "# # This avoids serialization issues\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model='sshleifer/distilbart-cnn-12-6',\n",
    "#     framework='pt'\n",
    "# )\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     Summarize a long list of reviews into one short summary using parallel processing for chunks\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for processing\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Process chunks in a batch to maximize GPU usage\n",
    "#     intermediate_summaries = summarizer(\n",
    "#         all_chunks,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )\n",
    "    \n",
    "#     # Extract summary texts\n",
    "#     intermediate = [summary['summary_text'] for summary in intermediate_summaries]\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries\n",
    "#     joined = \" \".join(intermediate)\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # 4) Function to process each batch of rows in parallel\n",
    "# def process_batch(batch_df):\n",
    "#     results = []\n",
    "#     for _, row in batch_df.iterrows():\n",
    "#         summary = hierarchical_summary(row['Reviews'], chunk_size=200, max_len=60, min_len=20)\n",
    "#         results.append((row.name, summary))\n",
    "#     return results\n",
    "\n",
    "# # Split the dataframe into batches for parallel processing\n",
    "# def split_dataframe(df, batch_size):\n",
    "#     batches = []\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         batches.append(df.iloc[i:i+batch_size])\n",
    "#     return batches\n",
    "\n",
    "# # Calculate optimal batch size based on dataset size and worker count\n",
    "# batch_size = max(1, len(final_report) // num_workers)\n",
    "# batches = split_dataframe(final_report, batch_size)\n",
    "\n",
    "# # Use ThreadPoolExecutor for parallel processing with progress bar\n",
    "# all_results = []\n",
    "# with tqdm(total=len(final_report), desc=\"Summarizing themes\") as pbar:\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#         # Submit all batches to the executor\n",
    "#         future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        \n",
    "#         # Process completed batches and update progress\n",
    "#         for future in concurrent.futures.as_completed(future_to_batch):\n",
    "#             batch_results = future.result()\n",
    "#             all_results.extend(batch_results)\n",
    "#             # Update progress bar by the number of rows processed in this batch\n",
    "#             pbar.update(len(future_to_batch[future]))\n",
    "\n",
    "# # Sort results by the original index and extract summaries\n",
    "# all_results.sort(key=lambda x: x[0])  # Sort by index\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # 5) Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # 6) Inspect\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (FIXED - FIRST): Hierarchical summarization of all reviews per theme\n",
    "\n",
    "# from transformers import pipeline\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # 1) Initialize a single summarizer pipeline\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model='sshleifer/distilbart-cnn-12-6',\n",
    "#     device=0,              # change to -1 if no GPU\n",
    "#     framework='pt'\n",
    "# )\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200,\n",
    "#                          max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     Summarize a long list of reviews into one short summary:\n",
    "#       1) Chunk the reviews into batches of chunk_size\n",
    "#       2) Summarize each batch\n",
    "#       3) Summarize the concatenation of batch summaries\n",
    "    \n",
    "#     Params:\n",
    "#       reviews    : list of str, the reviews to summarize\n",
    "#       chunk_size : int, number of reviews per intermediate chunk\n",
    "#       max_len    : int, max summary tokens per call\n",
    "#       min_len    : int, min summary tokens per call\n",
    "    \n",
    "#     Returns:\n",
    "#       str, final \"quick read\" summary\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for batch processing\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Process all chunks in one batch\n",
    "#     intermediate_summaries = summarizer(\n",
    "#         all_chunks,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )\n",
    "    \n",
    "#     # Extract summary texts\n",
    "#     intermediate = [summary['summary_text'] for summary in intermediate_summaries]\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries\n",
    "#     joined = \" \".join(intermediate)\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # 4) Apply to each row of final_report with progress bar\n",
    "# quick_summaries = []\n",
    "# for _, row in tqdm(final_report.iterrows(),\n",
    "#                   total=len(final_report),\n",
    "#                   desc=\"Summarizing themes\"):\n",
    "#     revs = row['Reviews']\n",
    "#     quick = hierarchical_summary(revs,\n",
    "#                                  chunk_size=200,\n",
    "#                                  max_len=60,\n",
    "#                                  min_len=20)\n",
    "#     quick_summaries.append(quick)\n",
    "\n",
    "# # 5) Store results in a new column\n",
    "# final_report['QuickSummary'] = quick_summaries\n",
    "\n",
    "# # 6) Inspect\n",
    "# display(final_report[['steam_appid','Theme','QuickSummary']].head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
