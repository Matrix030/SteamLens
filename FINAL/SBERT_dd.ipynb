{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0651ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 03:14:03,540 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b363889d5e19b1699df6cb63c567481a initialized by task ('shuffle-transfer-b363889d5e19b1699df6cb63c567481a', 0) executed on worker tcp://127.0.0.1:46729\n",
      "2025-05-07 03:14:40,943 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b363889d5e19b1699df6cb63c567481a deactivated due to stimulus 'task-finished-1746602080.9425142'\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Dask Client\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Start a local Dask client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Theme Dictionary & Precompute Embeddings\n",
    "import json\n",
    "\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Precompute theme embeddings for each game\n",
    "THEME_EMBEDDINGS = {}\n",
    "for appid, themes in GAME_THEMES.items():\n",
    "    emb_list = []\n",
    "    for theme, seeds in themes.items():\n",
    "        seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "        emb_list.append(seed_emb.mean(axis=0))\n",
    "    THEME_EMBEDDINGS[appid] = np.vstack(emb_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e133960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Read All Parquet Files into a Dask DataFrame\n",
    "# Assumes all game parquet files are in the same folder\n",
    "ddf = dd.read_parquet(\n",
    "    'parquet_output_theme_combo/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ff75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5aea5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define Partition-wise Topic Assignment\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # reused on each worker\n",
    "\n",
    "def assign_topic(df_partition):\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "\n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        theme_embs = THEME_EMBEDDINGS[int(appid)]\n",
    "        sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "        topic_ids.append(int(sims.argmax()))\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1450bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 87.26 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Aggregate Counts, Likes, and Collect Reviews per Theme\n",
    "# Count reviews and sum votes per (game, theme)\n",
    "agg = ddf_with_topic.groupby(['steam_appid', 'topic_id']).agg(\n",
    "    review_count=('review', 'count'),\n",
    "    likes_sum=('voted_up', 'sum')\n",
    ")\n",
    "\n",
    "# Also collect reviews into lists per group\n",
    "reviews_series = ddf_with_topic.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "    .apply(lambda x: list(x), meta=('review', object))\n",
    "\n",
    "# Compute both in parallel\n",
    "agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "\n",
    "# Convert reviews series to DataFrame\n",
    "reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "\n",
    "# Convert aggregation to DataFrame\n",
    "agg_df = agg_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a2c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Construct Final Report DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    theme_name = list(GAME_THEMES[appid].keys())[tid]\n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "final_report.to_csv('output_csvs/SBERT_DD_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd04e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   steam_appid        Theme  #Reviews LikeRatio  \\\n",
      "0           10    community      2511     96.2%   \n",
      "1           10   anti_cheat      3654     93.7%   \n",
      "2           10  performance      2527     91.7%   \n",
      "3           10  competitive      9644     98.1%   \n",
      "4           10     gameplay      2416     96.9%   \n",
      "\n",
      "                                             Reviews  \n",
      "0  [Actually the best game in this world. It stil...  \n",
      "1  [So here's a little story.\\nBefore my dad and ...  \n",
      "2  [How to correctly play this game:\\n-Noisiest f...  \n",
      "3  [[h1] Once a fire lit in my heart and now it w...  \n",
      "4  [Counter-Strike won't ever be canceled.\\nCount...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 8: View the Report\n",
    "print(final_report.head())\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3176a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 (FIXED - FIRST): Hierarchical summarization of all reviews per theme\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Initialize a single summarizer pipeline\n",
    "summarizer = pipeline(\n",
    "    task='summarization',\n",
    "    model='sshleifer/distilbart-cnn-12-6',\n",
    "    device=0,              # change to -1 if no GPU\n",
    "    framework='pt'\n",
    ")\n",
    "\n",
    "def hierarchical_summary(reviews, chunk_size=200,\n",
    "                         max_len=60, min_len=20):\n",
    "    \"\"\"\n",
    "    Summarize a long list of reviews into one short summary:\n",
    "      1) Chunk the reviews into batches of chunk_size\n",
    "      2) Summarize each batch\n",
    "      3) Summarize the concatenation of batch summaries\n",
    "    \n",
    "    Params:\n",
    "      reviews    : list of str, the reviews to summarize\n",
    "      chunk_size : int, number of reviews per intermediate chunk\n",
    "      max_len    : int, max summary tokens per call\n",
    "      min_len    : int, min summary tokens per call\n",
    "    \n",
    "    Returns:\n",
    "      str, final \"quick read\" summary\n",
    "    \"\"\"\n",
    "    # If there are fewer than chunk_size, just do one summary\n",
    "    if len(reviews) <= chunk_size:\n",
    "        doc = \"\\n\\n\".join(reviews)\n",
    "        return summarizer(\n",
    "            doc,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # 2) Prepare all chunks for batch processing\n",
    "    all_chunks = []\n",
    "    for i in range(0, len(reviews), chunk_size):\n",
    "        batch = reviews[i:i+chunk_size]\n",
    "        text = \"\\n\\n\".join(batch)\n",
    "        all_chunks.append(text)\n",
    "    \n",
    "    # Process all chunks in one batch\n",
    "    intermediate_summaries = summarizer(\n",
    "        all_chunks,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    # Extract summary texts\n",
    "    intermediate = [summary['summary_text'] for summary in intermediate_summaries]\n",
    "    \n",
    "    # 3) Summarize the intermediate summaries\n",
    "    joined = \" \".join(intermediate)\n",
    "    return summarizer(\n",
    "        joined,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "# 4) Apply to each row of final_report with progress bar\n",
    "quick_summaries = []\n",
    "for _, row in tqdm(final_report.iterrows(),\n",
    "                  total=len(final_report),\n",
    "                  desc=\"Summarizing themes\"):\n",
    "    revs = row['Reviews']\n",
    "    quick = hierarchical_summary(revs,\n",
    "                                 chunk_size=200,\n",
    "                                 max_len=60,\n",
    "                                 min_len=20)\n",
    "    quick_summaries.append(quick)\n",
    "\n",
    "# 5) Store results in a new column\n",
    "final_report['QuickSummary'] = quick_summaries\n",
    "\n",
    "# 6) Inspect\n",
    "display(final_report[['steam_appid','Theme','QuickSummary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 (BIG DATA - BUT WITH PYTHON FUTURES): Hierarchical summarization of all reviews per theme using parallel processing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "# Set up the number of workers based on CPU cores (with one less to avoid overloading)\n",
    "num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "# 1) Initialize the summarizer pipeline globally\n",
    "# This avoids serialization issues\n",
    "summarizer = pipeline(\n",
    "    task='summarization',\n",
    "    model='sshleifer/distilbart-cnn-12-6',\n",
    "    framework='pt'\n",
    ")\n",
    "\n",
    "def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "    \"\"\"\n",
    "    Summarize a long list of reviews into one short summary using parallel processing for chunks\n",
    "    \"\"\"\n",
    "    # If there are fewer than chunk_size, just do one summary\n",
    "    if len(reviews) <= chunk_size:\n",
    "        doc = \"\\n\\n\".join(reviews)\n",
    "        return summarizer(\n",
    "            doc,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # 2) Prepare all chunks for processing\n",
    "    all_chunks = []\n",
    "    for i in range(0, len(reviews), chunk_size):\n",
    "        batch = reviews[i:i+chunk_size]\n",
    "        text = \"\\n\\n\".join(batch)\n",
    "        all_chunks.append(text)\n",
    "    \n",
    "    # Process chunks in a batch to maximize GPU usage\n",
    "    intermediate_summaries = summarizer(\n",
    "        all_chunks,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    # Extract summary texts\n",
    "    intermediate = [summary['summary_text'] for summary in intermediate_summaries]\n",
    "    \n",
    "    # 3) Summarize the intermediate summaries\n",
    "    joined = \" \".join(intermediate)\n",
    "    return summarizer(\n",
    "        joined,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "# 4) Function to process each batch of rows in parallel\n",
    "def process_batch(batch_df):\n",
    "    results = []\n",
    "    for _, row in batch_df.iterrows():\n",
    "        summary = hierarchical_summary(row['Reviews'], chunk_size=200, max_len=60, min_len=20)\n",
    "        results.append((row.name, summary))\n",
    "    return results\n",
    "\n",
    "# Split the dataframe into batches for parallel processing\n",
    "def split_dataframe(df, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batches.append(df.iloc[i:i+batch_size])\n",
    "    return batches\n",
    "\n",
    "# Calculate optimal batch size based on dataset size and worker count\n",
    "batch_size = max(1, len(final_report) // num_workers)\n",
    "batches = split_dataframe(final_report, batch_size)\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing with progress bar\n",
    "all_results = []\n",
    "with tqdm(total=len(final_report), desc=\"Summarizing themes\") as pbar:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Submit all batches to the executor\n",
    "        future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        \n",
    "        # Process completed batches and update progress\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_results = future.result()\n",
    "            all_results.extend(batch_results)\n",
    "            # Update progress bar by the number of rows processed in this batch\n",
    "            pbar.update(len(future_to_batch[future]))\n",
    "\n",
    "# Sort results by the original index and extract summaries\n",
    "all_results.sort(key=lambda x: x[0])  # Sort by index\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# 5) Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# 6) Inspect\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3214efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 15.56 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9cda310c4d4d66873632e26ef6c7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GPU Summarizing (RTX 4080 Super):   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Your max_length is set to 60, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 60, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steam_appid</th>\n",
       "      <th>Theme</th>\n",
       "      <th>QuickSummary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>community</td>\n",
       "      <td>Counter-Strike 1.6 was a significant part of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>anti_cheat</td>\n",
       "      <td>Counter-Strike is a first-person shooter vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>performance</td>\n",
       "      <td>The game crashes constantly, there is no impl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>competitive</td>\n",
       "      <td>CounterStrike 1.6 is like a time travel, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>gameplay</td>\n",
       "      <td>Counter Strike is a first-person shooter game...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steam_appid        Theme                                       QuickSummary\n",
       "0           10    community   Counter-Strike 1.6 was a significant part of ...\n",
       "1           10   anti_cheat   Counter-Strike is a first-person shooter vide...\n",
       "2           10  performance   The game crashes constantly, there is no impl...\n",
       "3           10  competitive   CounterStrike 1.6 is like a time travel, just...\n",
       "4           10     gameplay   Counter Strike is a first-person shooter game..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9 (OPTIMIZED - FASTEST SO FAR): GPU-optimized hierarchical summarization for RTX 4080 Super\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "# Check GPU memory and set optimal batch sizes\n",
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "print(f\"Available GPU memory: {gpu_mem:.2f} GB\")\n",
    "\n",
    "# RTX 4080 Super optimization parameters\n",
    "# With 16GB VRAM, we can use larger batch sizes and optimize throughput\n",
    "MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "MAX_GPU_BATCH_SIZE = 32  # Larger batch size for 16GB VRAM\n",
    "PARALLEL_PROCESSES = 4   # Optimal number for balancing CPU and GPU workloads\n",
    "\n",
    "# 1) Initialize tokenizer to estimate token counts for optimal batching\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2) Initialize the summarizer pipeline with optimized settings for RTX 4080 Super\n",
    "summarizer = pipeline(\n",
    "    task='summarization',\n",
    "    model=MODEL_NAME,\n",
    "    device=0,\n",
    "    framework='pt',\n",
    "    # Optimized settings for higher throughput\n",
    "    model_kwargs={\n",
    "        \"use_cache\": True,  # Enable KV caching for faster inference\n",
    "    },\n",
    "    # Enable half-precision for faster processing and lower memory usage\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimate token count to optimize batching\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "    \"\"\"\n",
    "    GPU-optimized hierarchical summarization with dynamic batching\n",
    "    \"\"\"\n",
    "    # If there are fewer than chunk_size, just do one summary\n",
    "    if len(reviews) <= chunk_size:\n",
    "        doc = \"\\n\\n\".join(reviews)\n",
    "        return summarizer(\n",
    "            doc,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # 2) Prepare all chunks for processing\n",
    "    all_chunks = []\n",
    "    for i in range(0, len(reviews), chunk_size):\n",
    "        batch = reviews[i:i+chunk_size]\n",
    "        text = \"\\n\\n\".join(batch)\n",
    "        all_chunks.append(text)\n",
    "    \n",
    "    # Dynamically determine optimal batch size based on token counts\n",
    "    # For RTX 4080 Super with 16GB, we can process larger batches\n",
    "    summaries = []\n",
    "    for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "        batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "        batch_summaries = summarizer(\n",
    "            batch,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "        summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "    \n",
    "    # 3) Summarize the intermediate summaries\n",
    "    # RTX 4080 Super can handle the full set of intermediate summaries\n",
    "    joined = \" \".join(summaries)\n",
    "    return summarizer(\n",
    "        joined,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "# 4) Function to process each batch of rows with GPU optimization\n",
    "def process_gpu_batch(batch_df):\n",
    "    results = []\n",
    "    # Pre-collect all reviews to optimize memory transfers to GPU\n",
    "    all_rows = [(row.name, row['Reviews']) for _, row in batch_df.iterrows()]\n",
    "    \n",
    "    for idx, reviews in all_rows:\n",
    "        # Use optimized hierarchical summary function\n",
    "        summary = hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20)\n",
    "        results.append((idx, summary))\n",
    "        \n",
    "        # Optional: Force CUDA cache clearing every few iterations to prevent memory fragmentation\n",
    "        if idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Calculate optimal processing strategy based on dataset size\n",
    "total_rows = len(final_report)\n",
    "# Determine batch size for parallel processing\n",
    "optimal_batch_size = max(1, total_rows // PARALLEL_PROCESSES)\n",
    "\n",
    "# Split dataframe into optimized batches\n",
    "batches = [final_report.iloc[i:i+optimal_batch_size] for i in range(0, total_rows, optimal_batch_size)]\n",
    "\n",
    "# Process with concurrent.futures and progress tracking\n",
    "all_results = []\n",
    "with tqdm(total=total_rows, desc=\"GPU Summarizing (RTX 4080 Super)\") as pbar:\n",
    "    # Use ThreadPoolExecutor to manage parallel GPU tasks\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=PARALLEL_PROCESSES) as executor:\n",
    "        future_to_batch = {executor.submit(process_gpu_batch, batch): batch for batch in batches}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "                batch_size = len(future_to_batch[future])\n",
    "                pbar.update(batch_size)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Continue with remaining batches\n",
    "\n",
    "# Sort results by the original index\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# 5) Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# 6) Inspect results\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# 7) Clean up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a95557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 15.56 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing all reviews for processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257b1c985374497a92d5b57bb20d5d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ultra GPU Optimization:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of size 13 (0/13)\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "Processing batch of size 19 (0/19)\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "Processing batch of size 13 (0/13)\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "Processing batch of size 49 (0/49)\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "Processing batch of size 13 (0/13)\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.34GB reserved\n",
      "Processing batch of size 11 (0/11)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 8 (0/8)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 60, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 2 (0/2)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 13 (0/13)\n",
      "GPU Memory: 0.69GB allocated, 1.09GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.09GB reserved\n",
      "Processing batch of size 4 (0/4)\n",
      "GPU Memory: 0.69GB allocated, 1.09GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.09GB reserved\n",
      "Processing batch of size 64 (0/210)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (64/210)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (128/210)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 18 (192/210)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 56 (0/56)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/72)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 8 (64/72)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 25 (0/25)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 10 (0/10)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 15 (0/15)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 44 (0/44)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 6 (0/6)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 9 (0/9)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 10 (0/10)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 27 (0/27)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/242)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (64/242)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (128/242)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 50 (192/242)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 17 (0/17)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 52 (0/52)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 24 (0/24)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 16 (0/16)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 42 (0/42)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 60, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 19 (0/19)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 13 (0/13)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 28 (0/28)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 15 (0/15)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 16 (0/16)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 9 (0/9)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/64)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/124)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 60 (64/124)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 50 (0/50)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/225)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (64/225)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (128/225)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 33 (192/225)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 12 (0/12)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/188)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (64/188)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 60 (128/188)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/118)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 54 (64/118)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 23 (0/23)\n",
      "GPU Memory: 0.69GB allocated, 1.06GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 64 (0/71)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 7 (64/71)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 42 (0/42)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 39 (0/39)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "Processing batch of size 14 (0/14)\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n",
      "GPU Memory: 0.69GB allocated, 1.08GB reserved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steam_appid</th>\n",
       "      <th>Theme</th>\n",
       "      <th>QuickSummary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>community</td>\n",
       "      <td>Counter-Strike 1.6 was a significant part of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>anti_cheat</td>\n",
       "      <td>Counter-Strike is a first-person shooter vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>performance</td>\n",
       "      <td>The game crashes constantly, there is no impl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>competitive</td>\n",
       "      <td>CounterStrike 1.6 is like a time travel, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>gameplay</td>\n",
       "      <td>Counter Strike is a first-person shooter game...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steam_appid        Theme                                       QuickSummary\n",
       "0           10    community   Counter-Strike 1.6 was a significant part of ...\n",
       "1           10   anti_cheat   Counter-Strike is a first-person shooter vide...\n",
       "2           10  performance   The game crashes constantly, there is no impl...\n",
       "3           10  competitive   CounterStrike 1.6 is like a time travel, just...\n",
       "4           10     gameplay   Counter Strike is a first-person shooter game..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9 (ULTRA OPTIMIZED - FIXED): Maximum GPU utilization for RTX 4080 Super\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Force CUDA initialization and check memory\n",
    "torch.cuda.init()\n",
    "total_gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "print(f\"Total GPU memory: {total_gpu_mem:.2f} GB\")\n",
    "\n",
    "# Ultra-aggressive GPU optimization parameters for RTX 4080 Super\n",
    "MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "MAX_GPU_BATCH_SIZE = 64  # Much larger batch size to fully utilize VRAM\n",
    "MAX_SEQUENCE_LENGTH = 1024  # Set maximum context length to optimize memory usage\n",
    "\n",
    "# Load model and tokenizer directly for maximum control\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.float16,  # Half-precision for maximum throughput\n",
    "    device_map=\"auto\"           # Automatically map to available GPU\n",
    ")\n",
    "\n",
    "# Move model to GPU and optimize for inference\n",
    "model.to(\"cuda\")\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create a custom pipeline with maximum batch efficiency\n",
    "summarizer = pipeline(\n",
    "    task='summarization',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework='pt',\n",
    "    # Force maximum GPU memory usage\n",
    "    model_kwargs={\"use_cache\": True}\n",
    ")\n",
    "\n",
    "# Monitor GPU memory usage\n",
    "def gpu_memory_usage():\n",
    "    \"\"\"Return GPU memory usage in GB\"\"\"\n",
    "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    return allocated, reserved\n",
    "\n",
    "def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "    \"\"\"\n",
    "    Ultra-optimized hierarchical summarization for maximum GPU utilization\n",
    "    \"\"\"\n",
    "    # If there are fewer than chunk_size, just do one summary\n",
    "    if len(reviews) <= chunk_size:\n",
    "        doc = \"\\n\\n\".join(reviews)\n",
    "        return summarizer(\n",
    "            doc,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # 2) Prepare all chunks for processing with ultra-large batches\n",
    "    all_chunks = []\n",
    "    for i in range(0, len(reviews), chunk_size):\n",
    "        batch = reviews[i:i+chunk_size]\n",
    "        text = \"\\n\\n\".join(batch)\n",
    "        all_chunks.append(text)\n",
    "    \n",
    "    # Process in maximally large batches to saturate GPU\n",
    "    # This is the key optimization - use much larger batches to fill VRAM\n",
    "    summaries = []\n",
    "    for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "        batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "        \n",
    "        # Log memory usage before batch\n",
    "        print(f\"Processing batch of size {len(batch)} ({i}/{len(all_chunks)})\")\n",
    "        gpu_memory_usage()\n",
    "        \n",
    "        # Process maximum-sized batch\n",
    "        batch_summaries = summarizer(\n",
    "            batch,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "        summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "        \n",
    "        # Log memory after batch\n",
    "        gpu_memory_usage()\n",
    "    \n",
    "    # 3) Summarize the intermediate summaries in a single batch\n",
    "    joined = \" \".join(summaries)\n",
    "    \n",
    "    # Final summary\n",
    "    return summarizer(\n",
    "        joined,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        truncation=True,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "# Pre-process all reviews to maximize throughput - FIXED THIS LINE\n",
    "print(\"Preparing all reviews for processing...\")\n",
    "all_rows = [(i, row['Reviews']) for i, (_, row) in enumerate(final_report.iterrows())]\n",
    "\n",
    "# Process the entire dataset in sequential maximum-sized batches\n",
    "# This approach ensures GPU is fully saturated\n",
    "all_results = []\n",
    "with tqdm(total=len(final_report), desc=\"Ultra GPU Optimization\") as pbar:\n",
    "    # Process each row with maximum batch efficiency\n",
    "    for i in range(0, len(all_rows), 10):  # Process in batches of 10 rows\n",
    "        batch_rows = all_rows[i:i+10]\n",
    "        batch_results = []\n",
    "        \n",
    "        for batch_idx, (row_idx, reviews) in enumerate(batch_rows):\n",
    "            # Force garbage collection before large operations\n",
    "            if batch_idx % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Process with maximum GPU utilization\n",
    "            summary = hierarchical_summary(\n",
    "                reviews, \n",
    "                chunk_size=200, \n",
    "                max_len=60, \n",
    "                min_len=20\n",
    "            )\n",
    "            batch_results.append((row_idx, summary))\n",
    "            pbar.update(1)\n",
    "        \n",
    "        all_results.extend(batch_results)\n",
    "        # Force GPU memory cleanup between large batches\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Sort and store results\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Inspect results\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97831f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 (DASK DISTRIBUTED - FINAL WITH PROGRESS): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Start a local Dask cluster\n",
    "n_workers = 4  # Adjust based on your CPU core count\n",
    "cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# Define model parameters \n",
    "MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition of the data on a worker\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Load tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model with device_map=\"auto\" only\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"  # This will handle device placement automatically\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with both model AND tokenizer\n",
    "    summarizer = pipeline(\n",
    "        task='summarization',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework='pt',\n",
    "        model_kwargs={\"use_cache\": True}\n",
    "    )\n",
    "    \n",
    "    # Report worker GPU status\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "    # Define the hierarchical summary function within the worker\n",
    "    def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= chunk_size:\n",
    "            doc = \"\\n\\n\".join(reviews)\n",
    "            return summarizer(\n",
    "                doc,\n",
    "                max_length=max_len,\n",
    "                min_length=min_len,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process in large batches to utilize GPU\n",
    "        summaries = []\n",
    "        for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "            batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "            batch_summaries = summarizer(\n",
    "                batch,\n",
    "                max_length=max_len,\n",
    "                min_length=min_len,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "            summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "        \n",
    "        # Summarize the intermediate summaries\n",
    "        joined = \" \".join(summaries)\n",
    "        return summarizer(\n",
    "            joined,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # Process the partition with a progress bar\n",
    "    results = []\n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            summary = hierarchical_summary(row['Reviews'], chunk_size=200, max_len=60, min_len=20)\n",
    "            results.append((idx, summary))\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Clean up every few iterations\n",
    "            if len(results) % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Clean up at the end\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    del summarizer\n",
    "    \n",
    "    # Return the results for this partition\n",
    "    return results\n",
    "\n",
    "# Convert pandas DataFrame to Dask DataFrame\n",
    "dask_df = dd.from_pandas(final_report, npartitions=n_workers)\n",
    "\n",
    "# Set up manual progress tracking\n",
    "print(f\"Processing {len(final_report)} rows across {n_workers} partitions...\")\n",
    "\n",
    "# Simple approach to split the dataframe\n",
    "partition_size = len(final_report) // n_workers\n",
    "delayed_results = []\n",
    "\n",
    "# Process each partition separately\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "for i in range(n_workers):\n",
    "    # Get start and end index for this partition\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    \n",
    "    # Get this partition as a pandas DataFrame\n",
    "    partition_df = final_report.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    # Create a delayed task to process this partition\n",
    "    delayed_result = process_partition(partition_df, i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers} with {len(partition_df)} rows\")\n",
    "\n",
    "# Create a main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater\n",
    "def update_main_progress(future):\n",
    "    # Update main progress bar based on worker progress\n",
    "    # This function will be called repeatedly to update the main progress bar\n",
    "    completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "    main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "    main_progress.refresh()\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "import threading\n",
    "stop_flag = False\n",
    "\n",
    "def progress_monitor():\n",
    "    while not stop_flag:\n",
    "        update_main_progress(futures)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=progress_monitor)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for computation to complete\n",
    "results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join()\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(final_report)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# Flatten the nested list of results\n",
    "all_results = []\n",
    "for worker_results in results:\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "# Sort by index\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e07422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard available at: http://127.0.0.1:8787/status\n",
      "Scheduling 4 partitions for processing...\n",
      "Scheduled partition 1/4\n",
      "Scheduled partition 2/4\n",
      "Scheduled partition 3/4\n",
      "Scheduled partition 4/4\n",
      "\n",
      "Starting distributed computation with progress tracking:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7169a1bcda9244bea042ab6bd18582bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing all partitions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 68.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "\n",
      "\n",
      "Worker 2:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[ADevice set to use cuda:0\n",
      "\n",
      "\n",
      "\n",
      "Worker 3:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ADevice set to use cuda:0\n",
      "\n",
      "Worker 1:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[ADevice set to use cuda:0\n",
      "Worker 0:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 2: GPU Memory: 0.57GB allocated\n",
      "Worker 3: GPU Memory: 0.57GB allocated\n",
      "Worker 1: GPU Memory: 0.57GB allocated\n",
      "Worker 0: GPU Memory: 0.57GB allocated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Worker 0:   9%|▉         | 1/11 [00:05<00:56,  5.65s/it]\n",
      "\n",
      "Worker 0:  27%|██▋       | 3/11 [00:19<00:50,  6.34s/it]\u001b[A\u001b[A\n",
      "Worker 1:   9%|▉         | 1/11 [00:26<04:27, 26.75s/it]\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:   8%|▊         | 1/12 [00:27<05:01, 27.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Worker 2:  18%|█▊        | 2/11 [00:28<02:18, 15.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Worker 0:  45%|████▌     | 5/11 [00:43<00:55,  9.31s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "Worker 0:  55%|█████▍    | 6/11 [00:49<00:39,  7.97s/it]\u001b[A\u001b[AYour max_length is set to 60, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Worker 0:  82%|████████▏ | 9/11 [00:58<00:09,  4.78s/it]\n",
      "Worker 1:  18%|█▊        | 2/11 [00:59<04:32, 30.26s/it]\u001b[AYour max_length is set to 60, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Worker 0:  91%|█████████ | 10/11 [01:00<00:03,  3.81s/it]\n",
      "\n",
      "Worker 2:  45%|████▌     | 5/11 [01:00<01:14, 12.40s/it]\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "Worker 2:  55%|█████▍    | 6/11 [01:08<00:54, 10.97s/it]\u001b[A\u001b[A\n",
      "Worker 1:  27%|██▋       | 3/11 [01:10<02:52, 21.52s/it]\u001b[A\n",
      "\n",
      "Worker 2:  64%|██████▎   | 7/11 [01:13<00:35,  8.96s/it]\u001b[A\u001b[A\n",
      "Worker 1:  36%|███▋      | 4/11 [01:15<01:43, 14.78s/it]\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  17%|█▋        | 2/12 [01:17<06:46, 40.65s/it]\u001b[A\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "Worker 1:  45%|████▌     | 5/11 [01:22<01:12, 12.00s/it]\u001b[A\n",
      "\n",
      "Worker 2:  73%|███████▎  | 8/11 [01:25<00:29,  9.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "Worker 2:  82%|████████▏ | 9/11 [01:31<00:17,  8.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Worker 2:  91%|█████████ | 10/11 [01:37<00:07,  7.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  25%|██▌       | 3/12 [01:38<04:44, 31.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "Worker 1:  55%|█████▍    | 6/11 [01:41<01:11, 14.40s/it]\u001b[A\n",
      "\n",
      "Worker 2: 100%|██████████| 11/11 [01:41<00:00,  9.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Worker 1:  64%|██████▎   | 7/11 [01:43<00:41, 10.48s/it]\u001b[A\n",
      "Worker 1:  73%|███████▎  | 8/11 [01:46<00:24,  8.20s/it]\u001b[A\n",
      "Worker 1:  82%|████████▏ | 9/11 [01:50<00:13,  6.75s/it]\u001b[A\n",
      "Worker 0: 100%|██████████| 11/11 [02:20<00:00, 12.75s/it]\u001b[A\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "\n",
      "Worker 3:  33%|███▎      | 4/12 [02:40<05:48, 43.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  42%|████▏     | 5/12 [02:43<03:22, 28.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "Worker 1: 100%|██████████| 11/11 [02:55<00:00, 15.98s/it]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 3:  50%|█████     | 6/12 [03:09<02:48, 28.14s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  58%|█████▊    | 7/12 [03:22<01:55, 23.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  67%|██████▋   | 8/12 [03:24<01:06, 16.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  75%|███████▌  | 9/12 [03:32<00:41, 13.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  83%|████████▎ | 10/12 [03:37<00:22, 11.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  92%|█████████▏| 11/12 [03:41<00:09,  9.01s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed in 227.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Worker 3: 100%|██████████| 12/12 [03:43<00:00, 18.61s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steam_appid</th>\n",
       "      <th>Theme</th>\n",
       "      <th>QuickSummary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>community</td>\n",
       "      <td>Counter-Strike 1.6 was a significant part of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>anti_cheat</td>\n",
       "      <td>Counter-Strike is a first-person shooter vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>performance</td>\n",
       "      <td>The game crashes constantly, there is no impl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>competitive</td>\n",
       "      <td>CounterStrike 1.6 is like a time travel, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>gameplay</td>\n",
       "      <td>Counter Strike is a first-person shooter game...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steam_appid        Theme                                       QuickSummary\n",
       "0           10    community   Counter-Strike 1.6 was a significant part of ...\n",
       "1           10   anti_cheat   Counter-Strike is a first-person shooter vide...\n",
       "2           10  performance   The game crashes constantly, there is no impl...\n",
       "3           10  competitive   CounterStrike 1.6 is like a time travel, just...\n",
       "4           10     gameplay   Counter Strike is a first-person shooter game..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9 (FULLY OPTIMIZED - FIXED): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Start a local Dask cluster\n",
    "n_workers = 4  # Adjust based on your CPU core count\n",
    "cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# Define model parameters \n",
    "MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "# First, load the data once and distribute it to avoid repetition\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx):\n",
    "    \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "    # Get just this partition\n",
    "    return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Prepare partitions with delayed\n",
    "partition_size = len(final_report) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx))\n",
    "\n",
    "# The main processing function - FIXED: Removed dependency on datasets library\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition of the data on a worker with batch processing\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Load tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model with device_map=\"auto\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with model AND tokenizer\n",
    "    summarizer = pipeline(\n",
    "        task='summarization',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework='pt',\n",
    "        model_kwargs={\"use_cache\": True}\n",
    "    )\n",
    "    \n",
    "    # Report worker GPU status\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "    # FIXED: Process chunks in batches without requiring the datasets library\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in batches for efficient GPU utilization\"\"\"\n",
    "        # Process in large batches to utilize GPU effectively\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Process in batches of MAX_GPU_BATCH_SIZE\n",
    "        for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "            batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "            batch_summaries = summarizer(\n",
    "                batch,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "            all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "        return all_summaries\n",
    "    \n",
    "    # Define the hierarchical summary function with batch processing\n",
    "    def hierarchical_summary(reviews, chunk_size=200):\n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= chunk_size:\n",
    "            doc = \"\\n\\n\".join(reviews)\n",
    "            return summarizer(\n",
    "                doc,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with batched processing\n",
    "        intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "        # Summarize the intermediate summaries\n",
    "        joined = \" \".join(intermediate_summaries)\n",
    "        return summarizer(\n",
    "            joined,\n",
    "            max_length=60,\n",
    "            min_length=20,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # Process the partition with a progress bar\n",
    "    results = []\n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            summary = hierarchical_summary(row['Reviews'], chunk_size=200)\n",
    "            results.append((idx, summary))\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Clean up every few iterations\n",
    "            if len(results) % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Clean up at the end\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    del summarizer\n",
    "    \n",
    "    # Return the results for this partition\n",
    "    return results\n",
    "\n",
    "# Schedule the tasks with the delayed partitions\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# Create a main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater\n",
    "def update_main_progress(future):\n",
    "    # Update main progress bar based on worker progress\n",
    "    completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "    main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "    main_progress.refresh()\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "import threading\n",
    "stop_flag = False\n",
    "\n",
    "def progress_monitor():\n",
    "    while not stop_flag:\n",
    "        update_main_progress(futures)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=progress_monitor)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for computation to complete - FIXED: Added more reliable computation approach\n",
    "try:\n",
    "    print(\"Computing all partitions...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    # Fallback to direct computation if future gathering fails\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join()\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(final_report)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# Flatten the nested list of results\n",
    "all_results = []\n",
    "for worker_results in results:\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "# Sort by index\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796c9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report.to_csv('output_csvs/summarized_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400fa28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
