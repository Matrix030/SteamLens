{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f13561",
   "metadata": {},
   "source": [
    "# This code has dynamic allocation of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeff0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System has 30.5GB memory and 7 CPU cores\n",
      "Allocating 7 workers with 3GB each\n",
      "Dashboard link: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-9d49b1c7-2b65-11f0-af57-58cdc9a29b5b</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">9c81597f</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 7\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 14\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 19.56 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-6766284e-c66e-4227-85f1-261e9d8797b1</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:44053\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 7\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 14\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 19.56 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:44649\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:35469/status\" target=\"_blank\">http://127.0.0.1:35469/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44245\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-6iwe6wzc\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:37081\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:44175/status\" target=\"_blank\">http://127.0.0.1:44175/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:32921\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-_gq3l8kq\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 2</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:35589\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:41929/status\" target=\"_blank\">http://127.0.0.1:41929/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42983\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-r_b3uwci\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 3</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43151\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:39441/status\" target=\"_blank\">http://127.0.0.1:39441/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42241\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-z12vto5c\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 4</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:46677\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:40199/status\" target=\"_blank\">http://127.0.0.1:40199/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:46513\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-wqyztcs8\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 5</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:45129\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:39363/status\" target=\"_blank\">http://127.0.0.1:39363/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38717\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-fp3rokra\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 6</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:37659\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 2\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:35943/status\" target=\"_blank\">http://127.0.0.1:35943/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 2.79 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:45729\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-u4mfzayr\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44053' processes=7 threads=14, memory=19.56 GiB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 13:06:30,231 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle f28148cf3f2a7d848d69d3b064f94f5e initialized by task ('shuffle-transfer-f28148cf3f2a7d848d69d3b064f94f5e', 0) executed on worker tcp://127.0.0.1:37081\n",
      "2025-05-07 13:06:30,647 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle f28148cf3f2a7d848d69d3b064f94f5e deactivated due to stimulus 'task-finished-1746637590.6467838'\n",
      "2025-05-07 13:06:52,998 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e509950cc22f20a5e39efc40f3827677 initialized by task ('shuffle-transfer-e509950cc22f20a5e39efc40f3827677', 0) executed on worker tcp://127.0.0.1:45129\n",
      "2025-05-07 13:07:30,756 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e509950cc22f20a5e39efc40f3827677 deactivated due to stimulus 'task-finished-1746637650.7553768'\n"
     ]
    }
   ],
   "source": [
    "# Modified Cell 1: Dynamic resource allocation for Dask Client\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "\n",
    "# Dynamically determine system resources\n",
    "def get_system_resources():\n",
    "    # Get available memory (in GB)\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)\n",
    "    # Get CPU count\n",
    "    cpu_count = psutil.cpu_count(logical=False)  # Physical cores only\n",
    "    if not cpu_count:\n",
    "        cpu_count = psutil.cpu_count(logical=True)  # Logical if physical not available\n",
    "    \n",
    "    # Use 70% of available memory for Dask, split across workers\n",
    "    dask_memory = int(total_memory * 0.7)\n",
    "    # Determine optimal worker count (leave at least 1 core for system)\n",
    "    worker_count = max(1, cpu_count - 1)\n",
    "    # Memory per worker\n",
    "    memory_per_worker = int(dask_memory / worker_count)\n",
    "    \n",
    "    return {\n",
    "        'worker_count': worker_count,\n",
    "        'memory_per_worker': memory_per_worker,\n",
    "        'total_memory': total_memory\n",
    "    }\n",
    "\n",
    "# Get system resources\n",
    "resources = get_system_resources()\n",
    "print(f\"System has {resources['total_memory']:.1f}GB memory and {resources['worker_count']} CPU cores\")\n",
    "print(f\"Allocating {resources['worker_count']} workers with {resources['memory_per_worker']}GB each\")\n",
    "\n",
    "# Start a local Dask cluster with dynamically determined resources\n",
    "cluster = LocalCluster(\n",
    "    n_workers=resources['worker_count'],\n",
    "    threads_per_worker=2,\n",
    "    memory_limit=f\"{resources['memory_per_worker']}GB\"\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54250fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Theme Dictionary & Optimize Theme Embeddings\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get theme embeddings for specific app IDs\n",
    "# This avoids loading all embeddings at once\n",
    "def get_theme_embeddings(app_ids):\n",
    "    \"\"\"Get theme embeddings for a specific set of app IDs\"\"\"\n",
    "    embeddings = {}\n",
    "    for appid in app_ids:\n",
    "        if appid not in embeddings and appid in GAME_THEMES:\n",
    "            emb_list = []\n",
    "            for theme, seeds in GAME_THEMES[appid].items():\n",
    "                seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "                emb_list.append(seed_emb.mean(axis=0))\n",
    "            embeddings[appid] = np.vstack(emb_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8528a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated dataset size: 0.10GB\n",
      "Using dynamic blocksize: 64MB\n"
     ]
    }
   ],
   "source": [
    "# Modified Cell 3: Dynamic blocksize for reading Parquet Files\n",
    "# Estimate dataset size first\n",
    "def estimate_dataset_size(path):\n",
    "    import os\n",
    "    total_size = 0\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size / (1024**3)  # Convert to GB\n",
    "\n",
    "# Estimate dataset size\n",
    "dataset_path = 'parquet_output_theme_combo'\n",
    "estimated_size = estimate_dataset_size(dataset_path)\n",
    "print(f\"Estimated dataset size: {estimated_size:.2f}GB\")\n",
    "\n",
    "# Dynamically determine blocksize based on dataset and memory\n",
    "# Use smaller blocks for larger datasets to prevent memory issues\n",
    "if estimated_size > 100:  # Very large dataset\n",
    "    blocksize = '16MB'\n",
    "elif estimated_size > 10:  # Medium-large dataset\n",
    "    blocksize = '32MB'\n",
    "else:  # Smaller dataset\n",
    "    blocksize = '64MB'\n",
    "\n",
    "print(f\"Using dynamic blocksize: {blocksize}\")\n",
    "\n",
    "# Read with dynamic blocksize\n",
    "ddf = dd.read_parquet(\n",
    "    f'{dataset_path}/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up'],\n",
    "    blocksize=blocksize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f95402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a4b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Optimized Partition-wise Topic Assignment\n",
    "def assign_topic(df_partition):\n",
    "    \"\"\"Assign topics using only theme embeddings for app IDs in this partition\"\"\"\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "    \n",
    "    # Get unique app IDs in this partition\n",
    "    app_ids = df_partition['steam_appid'].unique().tolist()\n",
    "    app_ids = [int(appid) for appid in app_ids]\n",
    "    \n",
    "    # Get embeddings only for app IDs in this partition\n",
    "    local_theme_embeddings = get_theme_embeddings(app_ids)\n",
    "    \n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go with batching\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        appid = int(appid)\n",
    "        if appid in local_theme_embeddings:\n",
    "            theme_embs = local_theme_embeddings[appid]\n",
    "            sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "            topic_ids.append(int(sims.argmax()))\n",
    "        else:\n",
    "            # Default topic if theme embeddings not available\n",
    "            topic_ids.append(0)\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c9617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4 unique app IDs with batch size 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11005a3a349e48fb86ad4f7ba64c1450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 87.21 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2025-05-07 13:07:14,137 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.24 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:07:14,197 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.24 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:07:29,876 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 1.94 GiB -- Worker memory limit: 2.79 GiB\n"
     ]
    }
   ],
   "source": [
    "# Modified Cell 6: Dynamic batch sizing for aggregation\n",
    "# Get unique app IDs\n",
    "unique_app_ids = ddf['steam_appid'].unique().compute()\n",
    "total_app_ids = len(unique_app_ids)\n",
    "\n",
    "# Dynamically determine batch size based on number of app IDs and memory\n",
    "# For larger datasets, use smaller batches to avoid memory issues\n",
    "if total_app_ids > 1000:  # Very large number of app IDs\n",
    "    batch_size = 3\n",
    "elif total_app_ids > 500:  # Medium-large number\n",
    "    batch_size = 5\n",
    "elif total_app_ids > 100:  # Medium number\n",
    "    batch_size = 10\n",
    "else:  # Smaller number\n",
    "    batch_size = 20\n",
    "\n",
    "print(f\"Processing {total_app_ids} unique app IDs with batch size {batch_size}\")\n",
    "\n",
    "# Initialize empty dataframes for results\n",
    "all_agg_dfs = []\n",
    "all_review_dfs = []\n",
    "\n",
    "# Process in dynamically sized batches\n",
    "for i in tqdm(range(0, len(unique_app_ids), batch_size)):\n",
    "    batch_app_ids = unique_app_ids[i:i+batch_size]\n",
    "    \n",
    "    # Filter data for this batch of app IDs\n",
    "    batch_ddf = ddf_with_topic[ddf_with_topic['steam_appid'].isin(batch_app_ids)]\n",
    "    \n",
    "    # Aggregate for this batch\n",
    "    agg = batch_ddf.groupby(['steam_appid', 'topic_id']).agg(\n",
    "        review_count=('review', 'count'),\n",
    "        likes_sum=('voted_up', 'sum')\n",
    "    )\n",
    "    \n",
    "    # Collect reviews for this batch\n",
    "    reviews_series = batch_ddf.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "        .apply(lambda x: list(x), meta=('review', object))\n",
    "    \n",
    "    # Compute both in parallel\n",
    "    agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    agg_df = agg_df.reset_index()\n",
    "    reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "    \n",
    "    # Append to results\n",
    "    all_agg_dfs.append(agg_df)\n",
    "    all_review_dfs.append(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48de627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Construct Final Report DataFrame\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    \n",
    "    # Check if appid exists in GAME_THEMES\n",
    "    if appid in GAME_THEMES:\n",
    "        theme_keys = list(GAME_THEMES[appid].keys())\n",
    "        # Check if tid is a valid index\n",
    "        if tid < len(theme_keys):\n",
    "            theme_name = theme_keys[tid]\n",
    "        else:\n",
    "            theme_name = f\"Unknown Theme {tid}\"\n",
    "    else:\n",
    "        theme_name = f\"Unknown Theme {tid}\"\n",
    "    \n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Save intermediate results to avoid recomputation if summarization fails\n",
    "final_report.to_csv('output_csvs/SBERT_DD_new_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976ed1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final report preview (Reviews column contains lists of review texts):\n",
      "   steam_appid        Theme  #Reviews LikeRatio\n",
      "0           10    community      2511     96.2%\n",
      "1           10   anti_cheat      3654     93.7%\n",
      "2           10  performance      2527     91.7%\n",
      "3           10  competitive      9644     98.1%\n",
      "4           10     gameplay      2416     96.9%\n",
      "\n",
      "Sample from first Reviews entry (showing first review only):\n",
      "Number of reviews in list: 2511\n",
      "First review (truncated): Actually the best game in this world. It still doesnt matter if u have NASA PC or you are playing on...\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: View the Report\n",
    "# Print preview of the DataFrame (excluding the Reviews column as it contains lists)\n",
    "print(\"Final report preview (Reviews column contains lists of review texts):\")\n",
    "print(final_report[['steam_appid', 'Theme', '#Reviews', 'LikeRatio']].head())\n",
    "\n",
    "# Verify that Reviews column contains lists\n",
    "sample_reviews = final_report['Reviews'].iloc[0]\n",
    "print(f\"\\nSample from first Reviews entry (showing first review only):\")\n",
    "if isinstance(sample_reviews, list) and len(sample_reviews) > 0:\n",
    "    print(f\"Number of reviews in list: {len(sample_reviews)}\")\n",
    "    print(f\"First review (truncated): {sample_reviews[0][:100]}...\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdbbbc",
   "metadata": {},
   "source": [
    "# Dynamic Allocation and takes twice as much time but can run on most systems by reading the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Dynamically optimized GPU hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import json\n",
    "import threading\n",
    "\n",
    "# 1. Dynamic resource allocation based on system capabilities\n",
    "def get_system_resources():\n",
    "    \"\"\"Determine optimal system resource allocation\"\"\"\n",
    "    # Get available memory and CPU resources\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)  # GB\n",
    "    available_memory = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "    cpu_count = psutil.cpu_count(logical=False) or psutil.cpu_count(logical=True)\n",
    "    \n",
    "    # Check for GPU presence and memory\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_count = torch.cuda.device_count() if gpu_available else 0\n",
    "    gpu_memory = [torch.cuda.get_device_properties(i).total_memory / (1024**3) for i in range(gpu_count)] if gpu_available else []\n",
    "    \n",
    "    # Determine optimal worker count - leave cores for system and GPU processes\n",
    "    if gpu_available:\n",
    "        # For GPU workloads, fewer workers but more memory per worker\n",
    "        worker_count = min(max(1, cpu_count // 2), gpu_count + 1)\n",
    "    else:\n",
    "        # For CPU workloads, use more workers\n",
    "        worker_count = max(1, cpu_count - 1)\n",
    "    \n",
    "    # Memory per worker (70% of available to leave headroom)\n",
    "    safe_memory = available_memory * 0.7\n",
    "    memory_per_worker = safe_memory / worker_count\n",
    "    \n",
    "    # Dynamic chunk size based on available memory\n",
    "    if memory_per_worker > 8:  # High memory\n",
    "        chunk_size = 300\n",
    "    elif memory_per_worker > 4:  # Medium memory\n",
    "        chunk_size = 200\n",
    "    else:  # Low memory\n",
    "        chunk_size = 100\n",
    "    \n",
    "    print(f\"System resources: {total_memory:.1f}GB total RAM, {available_memory:.1f}GB available\")\n",
    "    print(f\"CPU cores: {cpu_count}, GPU count: {gpu_count}\")\n",
    "    if gpu_count > 0:\n",
    "        for i, mem in enumerate(gpu_memory):\n",
    "            print(f\"GPU {i}: {mem:.1f}GB memory\")\n",
    "    \n",
    "    return {\n",
    "        'worker_count': worker_count,\n",
    "        'memory_per_worker': memory_per_worker,\n",
    "        'chunk_size': chunk_size,\n",
    "        'gpu_available': gpu_available,\n",
    "        'gpu_count': gpu_count,\n",
    "        'gpu_memory': gpu_memory\n",
    "    }\n",
    "\n",
    "# Get system resources\n",
    "resources = get_system_resources()\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Start a local Dask cluster with dynamic resources\n",
    "n_workers = resources['worker_count']\n",
    "print(f\"Starting Dask cluster with {n_workers} workers, {resources['memory_per_worker']:.1f}GB per worker\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=n_workers, \n",
    "    threads_per_worker=2,\n",
    "    memory_limit=f\"{resources['memory_per_worker']:.1f}GB\"\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# 2. Determine model based on available resources\n",
    "def select_model():\n",
    "    \"\"\"Select appropriate model based on available resources\"\"\"\n",
    "    if resources['gpu_available'] and any(mem > 8 for mem in resources['gpu_memory']):\n",
    "        # For high-end GPUs, use more powerful model\n",
    "        return 'sshleifer/distilbart-cnn-12-6'\n",
    "    elif resources['gpu_available']:\n",
    "        # For lower-end GPUs, use smaller model\n",
    "        return 'facebook/bart-large-cnn'\n",
    "    else:\n",
    "        # For CPU-only, use smallest model\n",
    "        return 'facebook/bart-base'\n",
    "\n",
    "# Select model based on resources\n",
    "MODEL_NAME = select_model()\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "\n",
    "# 3. First, load the data and check for existing checkpoints\n",
    "def load_with_checkpoint():\n",
    "    \"\"\"Load data with checkpoint recovery\"\"\"\n",
    "    checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"Found checkpoint with {len(checkpoint)} completed summaries\")\n",
    "            \n",
    "        # Filter the dataframe to only process remaining rows\n",
    "        completed_indices = list(map(int, checkpoint.keys()))\n",
    "        remaining_df = final_report[~final_report.index.isin(completed_indices)].copy()\n",
    "        \n",
    "        print(f\"Resuming processing for {len(remaining_df)} remaining items\")\n",
    "        return remaining_df, checkpoint\n",
    "    else:\n",
    "        print(\"No checkpoint found, processing all items\")\n",
    "        return final_report, {}\n",
    "\n",
    "# Load data with checkpoint support\n",
    "df_to_process, existing_summaries = load_with_checkpoint()\n",
    "\n",
    "# 4. Prepare partitions with optimized distribution\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx, df):\n",
    "    \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "    # Get just this partition\n",
    "    return df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Distribute the remaining work\n",
    "partition_size = len(df_to_process) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(df_to_process)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx, df_to_process))\n",
    "\n",
    "# 5. Worker processing function with dynamic GPU batch sizing\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition with dynamic batch sizes and error recovery\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # Determine optimal GPU batch size based on available memory\n",
    "    def determine_gpu_batch_size():\n",
    "        if not torch.cuda.is_available():\n",
    "            return 8  # Conservative default for CPU\n",
    "            \n",
    "        try:\n",
    "            # Get GPU memory info for this worker\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "            # Reserve 10% for system processes and overhead\n",
    "            usable_mem = total_mem * 0.9\n",
    "            \n",
    "            # Scale batch size based on available GPU memory\n",
    "            if usable_mem > 16:  # High-end GPU with >16GB\n",
    "                return 64\n",
    "            elif usable_mem > 8:  # Mid-range GPU with >8GB\n",
    "                return 32\n",
    "            elif usable_mem > 4:  # Lower-end GPU with >4GB\n",
    "                return 16\n",
    "            else:  # Minimal GPU\n",
    "                return 8\n",
    "        except Exception as e:\n",
    "            print(f\"Error determining GPU batch size: {e}\")\n",
    "            return 8  # Conservative fallback\n",
    "    \n",
    "    # Worker initialization with error handling\n",
    "    try:\n",
    "        # Load tokenizer first\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # Configure device placement based on available resources\n",
    "        if torch.cuda.is_available():\n",
    "            device_map = \"auto\"\n",
    "            dtype = torch.float16  # Use half precision with GPU\n",
    "        else:\n",
    "            device_map = None\n",
    "            dtype = torch.float32  # Use full precision with CPU\n",
    "        \n",
    "        # Load model with appropriate configuration\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device_map,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Create pipeline with model AND tokenizer\n",
    "        summarizer = pipeline(\n",
    "            task='summarization',\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            framework='pt',\n",
    "            model_kwargs={\"use_cache\": True}\n",
    "        )\n",
    "        \n",
    "        # Report worker status\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "            MAX_GPU_BATCH_SIZE = determine_gpu_batch_size()\n",
    "            print(f\"Worker {worker_id}: Using GPU batch size: {MAX_GPU_BATCH_SIZE}\")\n",
    "        else:\n",
    "            MAX_GPU_BATCH_SIZE = 8\n",
    "            print(f\"Worker {worker_id}: Using CPU with batch size: {MAX_GPU_BATCH_SIZE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Worker {worker_id} initialization error: {e}\")\n",
    "        # Fall back to a simpler configuration\n",
    "        try:\n",
    "            print(f\"Falling back to CPU-only mode for worker {worker_id}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "            summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "            MAX_GPU_BATCH_SIZE = 4  # Conservative batch size for fallback mode\n",
    "        except Exception as e2:\n",
    "            print(f\"Critical failure in worker {worker_id}: {e2}\")\n",
    "            return []  # Return empty results to avoid deadlock\n",
    "    \n",
    "    # Efficient batch processing function with memory management\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in batches with dynamic memory management\"\"\"\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Process in dynamically sized batches\n",
    "        for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "            try:\n",
    "                batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "                batch_summaries = summarizer(\n",
    "                    batch,\n",
    "                    max_length=60,\n",
    "                    min_length=20,\n",
    "                    truncation=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "                \n",
    "                # Proactively manage memory\n",
    "                if i % (MAX_GPU_BATCH_SIZE * 2) == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//MAX_GPU_BATCH_SIZE} of worker {worker_id}: {e}\")\n",
    "                # Try smaller batch on failure\n",
    "                if len(batch) > 1:\n",
    "                    print(\"Retrying with smaller batches...\")\n",
    "                    for single_item in batch:\n",
    "                        try:\n",
    "                            summary = summarizer(\n",
    "                                [single_item],\n",
    "                                max_length=60,\n",
    "                                min_length=20,\n",
    "                                truncation=True,\n",
    "                                do_sample=False\n",
    "                            )\n",
    "                            all_summaries.append(summary[0][\"summary_text\"])\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Failed to process single item: {e2}\")\n",
    "                            all_summaries.append(\"Error generating summary.\")\n",
    "                else:\n",
    "                    all_summaries.append(\"Error generating summary.\")\n",
    "                \n",
    "                # Clean up after errors\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        return all_summaries\n",
    "    \n",
    "    # Hierarchical summary function with adaptive chunking\n",
    "    def hierarchical_summary(reviews, base_chunk_size=200):\n",
    "        \"\"\"Create hierarchical summary with adaptive chunk sizing\"\"\"\n",
    "        # Defense against empty or invalid reviews\n",
    "        if not reviews or not isinstance(reviews, list):\n",
    "            return \"No reviews available for summarization.\"\n",
    "        \n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= base_chunk_size:\n",
    "            try:\n",
    "                # Join reviews with clear separation\n",
    "                doc = \"\\n\\n\".join(reviews[:base_chunk_size])\n",
    "                return summarizer(\n",
    "                    doc,\n",
    "                    max_length=60,\n",
    "                    min_length=20,\n",
    "                    truncation=True,\n",
    "                    do_sample=False\n",
    "                )[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                print(f\"Error summarizing small batch: {e}\")\n",
    "                # Try with even smaller batch if original fails\n",
    "                try:\n",
    "                    half_size = len(reviews) // 2\n",
    "                    doc = \"\\n\\n\".join(reviews[:half_size])\n",
    "                    return summarizer(\n",
    "                        doc,\n",
    "                        max_length=60,\n",
    "                        min_length=20, \n",
    "                        truncation=True,\n",
    "                        do_sample=False\n",
    "                    )[0]['summary_text']\n",
    "                except:\n",
    "                    return \"Error generating summary for this batch.\"\n",
    "        \n",
    "        # Adaptively determine chunk size based on review length\n",
    "        # If reviews are very short, use larger chunks\n",
    "        avg_review_len = sum(len(r) for r in reviews[:100]) / min(100, len(reviews))\n",
    "        if avg_review_len < 100:  # Very short reviews\n",
    "            chunk_size = min(base_chunk_size * 2, 500)\n",
    "        elif avg_review_len > 500:  # Very long reviews\n",
    "            chunk_size = max(base_chunk_size // 2, 50)\n",
    "        else:\n",
    "            chunk_size = base_chunk_size\n",
    "            \n",
    "        print(f\"Worker {worker_id}: Using chunk size {chunk_size} for avg review length {avg_review_len:.1f}\")\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with batched processing\n",
    "        try:\n",
    "            intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "            \n",
    "            # Summarize the intermediate summaries\n",
    "            joined = \" \".join(intermediate_summaries)\n",
    "            final_summary = summarizer(\n",
    "                joined,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "            \n",
    "            return final_summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error in hierarchical summarization: {e}\")\n",
    "            # Try to salvage what we can\n",
    "            if intermediate_summaries:\n",
    "                try:\n",
    "                    return f\"Partial summary: {' '.join(intermediate_summaries[:3])}\"\n",
    "                except:\n",
    "                    pass\n",
    "            return \"Error generating hierarchical summary.\"\n",
    "    \n",
    "    # Process the partition with checkpointing\n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            try:\n",
    "                # Skip processing if we already have too many errors in a row\n",
    "                if processed_count > 0 and len(results) == 0:\n",
    "                    # If first N items all failed, skip this worker\n",
    "                    if processed_count >= 5:\n",
    "                        print(f\"Worker {worker_id} failing consistently, aborting\")\n",
    "                        break\n",
    "                \n",
    "                # Process the review with the adaptive chunk size\n",
    "                summary = hierarchical_summary(row['Reviews'], base_chunk_size=resources['chunk_size'])\n",
    "                results.append((idx, summary))\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Clean up every few iterations\n",
    "                if processed_count % 5 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                # Checkpoint every 10 items\n",
    "                if processed_count % 10 == 0:\n",
    "                    print(f\"Worker {worker_id}: Processed {processed_count}/{len(partition_df)} items\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx} in worker {worker_id}: {e}\")\n",
    "                # Still record the error so we know this row was attempted\n",
    "                results.append((idx, f\"Error: Failed to generate summary.\"))\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Final cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Worker {worker_id} completed: {len(results)}/{len(partition_df)} successful\")\n",
    "    return results\n",
    "\n",
    "# 6. Schedule the tasks with the delayed partitions\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# 7. Progress tracking and checkpointing\n",
    "# Create main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(df_to_process), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater with checkpointing\n",
    "def update_main_progress(futures):\n",
    "    \"\"\"Update progress bar and save checkpoints\"\"\"\n",
    "    checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "    summaries_so_far = existing_summaries.copy()\n",
    "    \n",
    "    while not stop_flag:\n",
    "        # Count completed futures\n",
    "        completed_count = sum(f.status == 'finished' for f in futures)\n",
    "        completed_percentage = completed_count / len(futures)\n",
    "        \n",
    "        # Update progress bar\n",
    "        main_progress.n = int(len(df_to_process) * completed_percentage)\n",
    "        main_progress.refresh()\n",
    "        \n",
    "        # Check for newly completed results and update checkpoint\n",
    "        for future in [f for f in futures if f.status == 'finished']:\n",
    "            try:\n",
    "                result = future.result()\n",
    "                for idx, summary in result:\n",
    "                    summaries_so_far[str(idx)] = summary\n",
    "            except:\n",
    "                pass  # Skip failed futures\n",
    "        \n",
    "        # Save checkpoint every 30 seconds\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(summaries_so_far, f)\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "stop_flag = False\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=update_main_progress, args=(futures,))\n",
    "monitor_thread.daemon = True  # Allow program to exit if thread is still running\n",
    "monitor_thread.start()\n",
    "\n",
    "# 8. Wait for computation to complete with robust error handling\n",
    "try:\n",
    "    print(\"Computing all partitions...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    # Fallback to direct computation if future gathering fails\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join(timeout=5)  # Wait for thread to terminate, but with timeout\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(df_to_process)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# 9. Process results with checkpoint recovery\n",
    "all_results = []\n",
    "\n",
    "# Gather results from all workers\n",
    "for worker_results in results:\n",
    "    if worker_results:  # Check if worker returned any results\n",
    "        all_results.extend(worker_results)\n",
    "\n",
    "# Load checkpoint file for any results we already had\n",
    "checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "        \n",
    "    # Add checkpoint data for any missing indices\n",
    "    result_indices = [idx for idx, _ in all_results]\n",
    "    for idx_str, summary in checkpoint_data.items():\n",
    "        idx = int(idx_str)\n",
    "        if idx not in result_indices:\n",
    "            all_results.append((idx, summary))\n",
    "\n",
    "# Sort by index to maintain order\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create a dictionary mapping of indices to summaries\n",
    "result_dict = {idx: summary for idx, summary in all_results}\n",
    "\n",
    "# Apply to final report\n",
    "final_report['QuickSummary'] = final_report.index.map(\n",
    "    lambda idx: result_dict.get(idx, \"Summary not generated\")\n",
    ")\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Successfully summarized {len(result_dict)}/{len(final_report)} items\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSample results:\")\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# 10. Save the results\n",
    "final_report.to_csv('output_csvs/dynamic_summarized_report.csv')\n",
    "print(\"Results saved to output_csvs/dynamic_summarized_report.csv\")\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6242442",
   "metadata": {},
   "source": [
    "# Tuned for my hardware 1m 50 secs inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ec5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized Dask cluster for Ryzen 9700X + RTX 4080 Super configuration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45813 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard available at: http://127.0.0.1:45813/status\n",
      "Prepared partition 1 with 7 items\n",
      "Prepared partition 2 with 7 items\n",
      "Prepared partition 3 with 7 items\n",
      "Prepared partition 4 with 7 items\n",
      "Prepared partition 5 with 7 items\n",
      "Prepared partition 6 with 10 items\n",
      "Scheduling 6 optimized partitions...\n",
      "\n",
      "Starting optimized computation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fab67ae829d421e9d50aa9e6b045345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing with optimal settings for RTX 4080 Super...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgmatr1x/anaconda3/envs/rapids-25.04/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 68.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 5 initializing with optimized settings for RTX 4080 Super\n",
      "Worker 4 initializing with optimized settings for RTX 4080 Super\n",
      "Worker 1 initializing with optimized settings for RTX 4080 Super\n",
      "Worker 0 initializing with optimized settings for RTX 4080 Super\n",
      "Worker 3 initializing with optimized settings for RTX 4080 Super\n",
      "Worker 2 initializing with optimized settings for RTX 4080 Super\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 13:08:30,916 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.11 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:08:30,920 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.10 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:08:30,946 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.10 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:08:30,948 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.10 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:08:30,955 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.10 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-05-07 13:08:30,957 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.10 GiB -- Worker memory limit: 2.79 GiB\n",
      "Device set to use cuda:0\n",
      "\n",
      "Worker 1:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ADevice set to use cuda:0\n",
      "Worker 0:   0%|          | 0/7 [00:00<?, ?it/s]Device set to use cuda:0\n",
      "\n",
      "\n",
      "Worker 2:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[ADevice set to use cuda:0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 4:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ADevice set to use cuda:0\n",
      "\n",
      "\n",
      "\n",
      "Worker 3:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1: GPU Memory: 0.57GB allocated\n",
      "Worker 0: GPU Memory: 0.57GB allocated\n",
      "Worker 2: GPU Memory: 0.57GB allocated\n",
      "Worker 4: GPU Memory: 0.57GB allocated\n",
      "Worker 3: GPU Memory: 0.57GB allocated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 5: GPU Memory: 0.57GB allocated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 1:  14%|        | 1/7 [00:00<00:04,  1.27it/s]\u001b[A\n",
      "\n",
      "Worker 2:  14%|        | 1/7 [00:02<00:16,  2.77s/it]\u001b[A\u001b[A\n",
      "Worker 1:  29%|       | 2/7 [00:04<00:11,  2.26s/it]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 0:  14%|        | 1/7 [00:04<00:25,  4.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Worker 1:  43%|     | 3/7 [00:05<00:07,  2.00s/it]\u001b[A\n",
      "\n",
      "Worker 0:  29%|       | 2/7 [00:10<00:26,  5.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 4:  29%|       | 2/7 [00:11<00:29,  5.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 0:  43%|     | 3/7 [00:14<00:18,  4.68s/it]]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 4:  43%|     | 3/7 [00:16<00:21,  5.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Worker 2:  43%|     | 3/7 [00:20<00:31,  7.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 4:  57%|    | 4/7 [00:20<00:14,  4.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Worker 2:  57%|    | 4/7 [00:22<00:17,  5.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 4:  71%|  | 5/7 [00:23<00:08,  4.21s/it]\u001b[A\u001b[A\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "Worker 2:  71%|  | 5/7 [00:26<00:09,  4.97s/it]\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Worker 0:  57%|    | 4/7 [00:26<00:23,  7.89s/it]\n",
      "\n",
      "Worker 0:  71%|  | 5/7 [00:31<00:13,  6.55s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Worker 0: 100%|| 7/7 [00:38<00:00,  5.48s/it]\n",
      "\n",
      "\n",
      "Worker 2: 100%|| 7/7 [00:38<00:00,  5.48s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 completed successfully\n",
      "Worker 2 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 4:  86%| | 6/7 [00:40<00:08,  8.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  14%|        | 1/7 [00:55<05:33, 55.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "Worker 1:  57%|    | 4/7 [00:58<01:06, 22.02s/it]\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  29%|       | 2/7 [00:59<02:05, 25.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Worker 4: 100%|| 7/7 [01:03<00:00,  9.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  20%|        | 2/10 [01:03<04:37, 34.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 4 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  30%|       | 3/10 [01:05<02:18, 19.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  43%|     | 3/7 [01:07<01:09, 17.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "Worker 1:  71%|  | 5/7 [01:09<00:35, 17.93s/it]\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "\n",
      "Worker 3:  57%|    | 4/7 [01:10<00:35, 11.80s/it]\u001b[A\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "\n",
      "Worker 3:  71%|  | 5/7 [01:13<00:16,  8.49s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Worker 3:  86%| | 6/7 [01:18<00:07,  7.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "Worker 1:  86%| | 6/7 [01:20<00:15, 15.62s/it]\u001b[A\n",
      "\n",
      "\n",
      "Worker 3: 100%|| 7/7 [01:21<00:00, 11.64s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 3 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 1: 100%|| 7/7 [01:23<00:00, 11.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  40%|      | 4/10 [01:25<02:00, 20.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  50%|     | 5/10 [01:31<01:14, 14.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  60%|    | 6/10 [01:32<00:41, 10.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  70%|   | 7/10 [01:36<00:24,  8.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  80%|  | 8/10 [01:39<00:12,  6.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5:  90%| | 9/10 [01:41<00:05,  5.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Worker 5: 100%|| 10/10 [01:42<00:00, 10.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 5 completed successfully\n",
      "\n",
      "Optimized processing completed in 110.17 seconds\n",
      "Average time per item: 2.45 seconds\n",
      "\n",
      "Results sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steam_appid</th>\n",
       "      <th>Theme</th>\n",
       "      <th>QuickSummary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>community</td>\n",
       "      <td>Counter-Strike 1.6 was the first FPS game I e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>anti_cheat</td>\n",
       "      <td>Counter-Strike is a first-person shooter vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>performance</td>\n",
       "      <td>Counter-Strike: Global Offensive is the best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>competitive</td>\n",
       "      <td>CounterStrike: Global Offensive is the best C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>gameplay</td>\n",
       "      <td>Counter-Strike is one of the most critically ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steam_appid        Theme                                       QuickSummary\n",
       "0           10    community   Counter-Strike 1.6 was the first FPS game I e...\n",
       "1           10   anti_cheat   Counter-Strike is a first-person shooter vide...\n",
       "2           10  performance   Counter-Strike: Global Offensive is the best ...\n",
       "3           10  competitive   CounterStrike: Global Offensive is the best C...\n",
       "4           10     gameplay   Counter-Strike is one of the most critically ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_csvs/optimized_hardware_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Hardware-optimized GPU summarization with Dask - Tuned for Ryzen 9700X & RTX 4080 Super\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist (minimal overhead)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Optimized configuration for your specific hardware\n",
    "# RTX 4080 Super (12GB usable VRAM) + Ryzen 9700X + 20GB usable RAM\n",
    "HARDWARE_CONFIG = {\n",
    "    'worker_count': 6,                # Optimal for Ryzen 9700X\n",
    "    'memory_per_worker': '3GB',       # 18GB total for workers, leaving headroom\n",
    "    'gpu_batch_size': 96,             # Aggressive batch size for RTX 4080 Super\n",
    "    'model_name': 'sshleifer/distilbart-cnn-12-6',  # Best model for your GPU\n",
    "    'chunk_size': 400,                # Larger chunks for faster processing\n",
    "    'checkpoint_frequency': 25,       # Less frequent checkpoints for speed\n",
    "    'cleanup_frequency': 10,          # Less frequent memory cleanup\n",
    "}\n",
    "\n",
    "print(f\"Starting optimized Dask cluster for Ryzen 9700X + RTX 4080 Super configuration\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=HARDWARE_CONFIG['worker_count'], \n",
    "    threads_per_worker=2,\n",
    "    memory_limit=HARDWARE_CONFIG['memory_per_worker']\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# Determine optimal partition sizes - larger for better throughput\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx):\n",
    "    \"\"\"Prepare a partition optimized for high-end hardware\"\"\"\n",
    "    return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Create larger partitions for better throughput\n",
    "n_workers = HARDWARE_CONFIG['worker_count']\n",
    "partition_size = len(final_report) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx))\n",
    "    print(f\"Prepared partition {i+1} with {end_idx-start_idx} items\")\n",
    "\n",
    "# Optimized worker function with aggressive resource usage\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Optimized worker for RTX 4080 Super\"\"\"\n",
    "    # Import needed packages\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    # Load model components with optimal settings for RTX 4080 Super\n",
    "    print(f\"Worker {worker_id} initializing with optimized settings for RTX 4080 Super\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HARDWARE_CONFIG['model_name'])\n",
    "    \n",
    "    # Load model with optimized settings for RTX 4080 Super\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        HARDWARE_CONFIG['model_name'],\n",
    "        torch_dtype=torch.float16,        # Half precision for speed\n",
    "        device_map=\"auto\",                # Automatic device placement\n",
    "        low_cpu_mem_usage=True            # Optimized memory usage\n",
    "    )\n",
    "    \n",
    "    # Create optimized pipeline\n",
    "    summarizer = pipeline(\n",
    "        task='summarization',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework='pt',\n",
    "        model_kwargs={\n",
    "            \"use_cache\": True,            # Enable caching for speed\n",
    "            \"return_dict_in_generate\": True  # More efficient generation\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Report GPU status\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "    # Highly optimized batch processing function\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in large batches for RTX 4080 Super\"\"\"\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Use large batches for the RTX 4080 Super\n",
    "        for i in range(0, len(chunks), HARDWARE_CONFIG['gpu_batch_size']):\n",
    "            batch = chunks[i:i+HARDWARE_CONFIG['gpu_batch_size']]\n",
    "            batch_summaries = summarizer(\n",
    "                batch,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False,\n",
    "                num_beams=2  # Use beam search for better quality with minimal speed impact\n",
    "            )\n",
    "            all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "            # Minimal cleanup - only when really needed\n",
    "            if i % (HARDWARE_CONFIG['gpu_batch_size'] * 3) == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                    \n",
    "        return all_summaries\n",
    "    \n",
    "    # Optimized hierarchical summary function\n",
    "    def hierarchical_summary(reviews):\n",
    "        \"\"\"Create hierarchical summary with optimized chunk sizes\"\"\"\n",
    "        # Handle edge cases efficiently\n",
    "        if not reviews or not isinstance(reviews, list):\n",
    "            return \"No reviews available for summarization.\"\n",
    "        \n",
    "        # Fast path for small review sets\n",
    "        if len(reviews) <= HARDWARE_CONFIG['chunk_size']:\n",
    "            doc = \"\\n\\n\".join(reviews)\n",
    "            return summarizer(\n",
    "                doc,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "        \n",
    "        # Process larger review sets with optimized chunking\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), HARDWARE_CONFIG['chunk_size']):\n",
    "            batch = reviews[i:i+HARDWARE_CONFIG['chunk_size']]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with optimized batching\n",
    "        intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "        # Create final summary\n",
    "        joined = \" \".join(intermediate_summaries)\n",
    "        return summarizer(\n",
    "            joined,\n",
    "            max_length=60,\n",
    "            min_length=20,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # Process the partition with minimal overhead\n",
    "    results = []\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            # Process the review\n",
    "            summary = hierarchical_summary(row['Reviews'])\n",
    "            results.append((idx, summary))\n",
    "            \n",
    "            # Minimal cleanup - only every N iterations\n",
    "            if len(results) % HARDWARE_CONFIG['cleanup_frequency'] == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Worker {worker_id} completed successfully\")\n",
    "    return results\n",
    "\n",
    "# Schedule tasks\n",
    "print(f\"Scheduling {n_workers} optimized partitions...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "\n",
    "# Streamlined progress tracking\n",
    "print(\"\\nStarting optimized computation...\")\n",
    "main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Minimal checkpoint system - only save occasionally\n",
    "def update_main_progress(futures):\n",
    "    while not stop_flag:\n",
    "        # Count completed futures\n",
    "        completed_count = sum(f.status == 'finished' for f in futures)\n",
    "        completed_percentage = completed_count / len(futures)\n",
    "        \n",
    "        # Update progress bar\n",
    "        main_progress.n = int(len(final_report) * completed_percentage)\n",
    "        main_progress.refresh()\n",
    "        \n",
    "        # Only check every 5 seconds to reduce overhead\n",
    "        time.sleep(5)\n",
    "\n",
    "# Submit tasks to cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start progress monitor with minimal overhead\n",
    "stop_flag = False\n",
    "monitor_thread = threading.Thread(target=update_main_progress, args=(futures,))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for computation\n",
    "try:\n",
    "    print(\"Computing with optimal settings for RTX 4080 Super...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join(timeout=3)\n",
    "\n",
    "# Update progress to completion\n",
    "main_progress.n = len(final_report)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# Process results efficiently\n",
    "all_results = []\n",
    "for worker_results in results:\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "# Sort results\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Report timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nOptimized processing completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Average time per item: {elapsed_time/len(final_report):.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults sample:\")\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Save results\n",
    "final_report.to_csv('output_csvs/optimized_hardware_report.csv')\n",
    "print(\"Results saved to output_csvs/optimized_hardware_report.csv\")\n",
    "\n",
    "# Clean up\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e0360",
   "metadata": {},
   "source": [
    "# CLAI FIX [OLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (FULLY OPTIMIZED - FIXED): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from tqdm.auto import tqdm\n",
    "# import time\n",
    "\n",
    "# # Start a local Dask cluster\n",
    "# n_workers = 4  # Adjust based on your CPU core count\n",
    "# cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "# client = Client(cluster)\n",
    "# print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# # Define model parameters \n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "# # First, load the data once and distribute it to avoid repetition\n",
    "# @dask.delayed\n",
    "# def prepare_partition(start_idx, end_idx):\n",
    "#     \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "#     # Get just this partition\n",
    "#     return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# # Prepare partitions with delayed\n",
    "# partition_size = len(final_report) // n_workers\n",
    "# partitions = []\n",
    "# for i in range(n_workers):\n",
    "#     start_idx = i * partition_size\n",
    "#     end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "#     partitions.append(prepare_partition(start_idx, end_idx))\n",
    "\n",
    "# # The main processing function - FIXED: Removed dependency on datasets library\n",
    "# @dask.delayed\n",
    "# def process_partition(partition_df, worker_id):\n",
    "#     \"\"\"Process a partition of the data on a worker with batch processing\"\"\"\n",
    "#     # Import packages needed in the worker\n",
    "#     from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "#     import torch\n",
    "#     from tqdm.auto import tqdm\n",
    "    \n",
    "#     # Load tokenizer first\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "#     # Load model with device_map=\"auto\"\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#         MODEL_NAME,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "    \n",
    "#     # Create pipeline with model AND tokenizer\n",
    "#     summarizer = pipeline(\n",
    "#         task='summarization',\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         framework='pt',\n",
    "#         model_kwargs={\"use_cache\": True}\n",
    "#     )\n",
    "    \n",
    "#     # Report worker GPU status\n",
    "#     gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "#     print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "#     # FIXED: Process chunks in batches without requiring the datasets library\n",
    "#     def process_chunks_batched(chunks):\n",
    "#         \"\"\"Process chunks in batches for efficient GPU utilization\"\"\"\n",
    "#         # Process in large batches to utilize GPU effectively\n",
    "#         all_summaries = []\n",
    "        \n",
    "#         # Process in batches of MAX_GPU_BATCH_SIZE\n",
    "#         for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "#             batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "#             batch_summaries = summarizer(\n",
    "#                 batch,\n",
    "#                 max_length=60,\n",
    "#                 min_length=20,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )\n",
    "#             all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "#         return all_summaries\n",
    "    \n",
    "#     # Define the hierarchical summary function with batch processing\n",
    "#     def hierarchical_summary(reviews, chunk_size=200):\n",
    "#         # If there are fewer than chunk_size, just do one summary\n",
    "#         if len(reviews) <= chunk_size:\n",
    "#             doc = \"\\n\\n\".join(reviews)\n",
    "#             return summarizer(\n",
    "#                 doc,\n",
    "#                 max_length=60,\n",
    "#                 min_length=20,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )[0]['summary_text']\n",
    "        \n",
    "#         # Prepare all chunks for processing\n",
    "#         all_chunks = []\n",
    "#         for i in range(0, len(reviews), chunk_size):\n",
    "#             batch = reviews[i:i+chunk_size]\n",
    "#             text = \"\\n\\n\".join(batch)\n",
    "#             all_chunks.append(text)\n",
    "        \n",
    "#         # Process chunks with batched processing\n",
    "#         intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "#         # Summarize the intermediate summaries\n",
    "#         joined = \" \".join(intermediate_summaries)\n",
    "#         return summarizer(\n",
    "#             joined,\n",
    "#             max_length=60,\n",
    "#             min_length=20,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # Process the partition with a progress bar\n",
    "#     results = []\n",
    "#     # Create a progress bar for this worker\n",
    "#     with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "#         for idx, row in partition_df.iterrows():\n",
    "#             summary = hierarchical_summary(row['Reviews'], chunk_size=200)\n",
    "#             results.append((idx, summary))\n",
    "#             pbar.update(1)\n",
    "            \n",
    "#             # Clean up every few iterations\n",
    "#             if len(results) % 5 == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Clean up at the end\n",
    "#     torch.cuda.empty_cache()\n",
    "#     del model\n",
    "#     del summarizer\n",
    "    \n",
    "#     # Return the results for this partition\n",
    "#     return results\n",
    "\n",
    "# # Schedule the tasks with the delayed partitions\n",
    "# print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "# delayed_results = []\n",
    "# for i in range(n_workers):\n",
    "#     delayed_result = process_partition(partitions[i], i)\n",
    "#     delayed_results.append(delayed_result)\n",
    "#     print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# # Create a main progress bar for overall progress\n",
    "# print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "# main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Create a global progress updater\n",
    "# def update_main_progress(future):\n",
    "#     # Update main progress bar based on worker progress\n",
    "#     completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "#     main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "#     main_progress.refresh()\n",
    "\n",
    "# # Submit the tasks to the cluster\n",
    "# futures = client.compute(delayed_results)\n",
    "\n",
    "# # Start a loop to update the main progress bar\n",
    "# import threading\n",
    "# stop_flag = False\n",
    "\n",
    "# def progress_monitor():\n",
    "#     while not stop_flag:\n",
    "#         update_main_progress(futures)\n",
    "#         time.sleep(0.5)\n",
    "\n",
    "# # Start the progress monitor in a separate thread\n",
    "# monitor_thread = threading.Thread(target=progress_monitor)\n",
    "# monitor_thread.start()\n",
    "\n",
    "# # Wait for computation to complete - FIXED: Added more reliable computation approach\n",
    "# try:\n",
    "#     print(\"Computing all partitions...\")\n",
    "#     results = client.gather(futures)\n",
    "# except Exception as e:\n",
    "#     # Fallback to direct computation if future gathering fails\n",
    "#     print(f\"Error with futures: {e}\")\n",
    "#     print(\"Falling back to direct computation...\")\n",
    "#     results = dask.compute(*delayed_results)\n",
    "\n",
    "# # Stop the progress monitor\n",
    "# stop_flag = True\n",
    "# monitor_thread.join()\n",
    "\n",
    "# # Update progress bar to completion\n",
    "# main_progress.n = len(final_report)\n",
    "# main_progress.refresh()\n",
    "# main_progress.close()\n",
    "\n",
    "# # Flatten the nested list of results\n",
    "# all_results = []\n",
    "# for worker_results in results:\n",
    "#     all_results.extend(worker_results)\n",
    "\n",
    "# # Sort by index\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # Report final timing\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# # Display results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # Shut down the client and cluster\n",
    "# client.close()\n",
    "# cluster.close()\n",
    "# final_report.to_csv('output_csvs/SBERT_dd_clai.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
