{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (DASK DISTRIBUTED - FINAL WITH PROGRESS): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from tqdm.auto import tqdm\n",
    "# import time\n",
    "\n",
    "# # Start a local Dask cluster\n",
    "# n_workers = 4  # Adjust based on your CPU core count\n",
    "# cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "# client = Client(cluster)\n",
    "# print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# # Define model parameters \n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "# @dask.delayed\n",
    "# def process_partition(partition_df, worker_id):\n",
    "#     \"\"\"Process a partition of the data on a worker\"\"\"\n",
    "#     # Import packages needed in the worker\n",
    "#     from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "#     import torch\n",
    "#     from tqdm.auto import tqdm\n",
    "    \n",
    "#     # Load tokenizer first\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "#     # Load model with device_map=\"auto\" only\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#         MODEL_NAME,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         device_map=\"auto\"  # This will handle device placement automatically\n",
    "#     )\n",
    "    \n",
    "#     # Create pipeline with both model AND tokenizer\n",
    "#     summarizer = pipeline(\n",
    "#         task='summarization',\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         framework='pt',\n",
    "#         model_kwargs={\"use_cache\": True}\n",
    "#     )\n",
    "    \n",
    "#     # Report worker GPU status\n",
    "#     gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "#     print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "#     # Define the hierarchical summary function within the worker\n",
    "#     def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#         # If there are fewer than chunk_size, just do one summary\n",
    "#         if len(reviews) <= chunk_size:\n",
    "#             doc = \"\\n\\n\".join(reviews)\n",
    "#             return summarizer(\n",
    "#                 doc,\n",
    "#                 max_length=max_len,\n",
    "#                 min_length=min_len,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )[0]['summary_text']\n",
    "        \n",
    "#         # Prepare all chunks for processing\n",
    "#         all_chunks = []\n",
    "#         for i in range(0, len(reviews), chunk_size):\n",
    "#             batch = reviews[i:i+chunk_size]\n",
    "#             text = \"\\n\\n\".join(batch)\n",
    "#             all_chunks.append(text)\n",
    "        \n",
    "#         # Process in large batches to utilize GPU\n",
    "#         summaries = []\n",
    "#         for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "#             batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "#             batch_summaries = summarizer(\n",
    "#                 batch,\n",
    "#                 max_length=max_len,\n",
    "#                 min_length=min_len,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )\n",
    "#             summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "        \n",
    "#         # Summarize the intermediate summaries\n",
    "#         joined = \" \".join(summaries)\n",
    "#         return summarizer(\n",
    "#             joined,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # Process the partition with a progress bar\n",
    "#     results = []\n",
    "#     # Create a progress bar for this worker\n",
    "#     with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "#         for idx, row in partition_df.iterrows():\n",
    "#             summary = hierarchical_summary(row['Reviews'], chunk_size=200, max_len=60, min_len=20)\n",
    "#             results.append((idx, summary))\n",
    "#             pbar.update(1)\n",
    "            \n",
    "#             # Clean up every few iterations\n",
    "#             if len(results) % 5 == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Clean up at the end\n",
    "#     torch.cuda.empty_cache()\n",
    "#     del model\n",
    "#     del summarizer\n",
    "    \n",
    "#     # Return the results for this partition\n",
    "#     return results\n",
    "\n",
    "# # Convert pandas DataFrame to Dask DataFrame\n",
    "# dask_df = dd.from_pandas(final_report, npartitions=n_workers)\n",
    "\n",
    "# # Set up manual progress tracking\n",
    "# print(f\"Processing {len(final_report)} rows across {n_workers} partitions...\")\n",
    "\n",
    "# # Simple approach to split the dataframe\n",
    "# partition_size = len(final_report) // n_workers\n",
    "# delayed_results = []\n",
    "\n",
    "# # Process each partition separately\n",
    "# print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "# for i in range(n_workers):\n",
    "#     # Get start and end index for this partition\n",
    "#     start_idx = i * partition_size\n",
    "#     end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    \n",
    "#     # Get this partition as a pandas DataFrame\n",
    "#     partition_df = final_report.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "#     # Create a delayed task to process this partition\n",
    "#     delayed_result = process_partition(partition_df, i)\n",
    "#     delayed_results.append(delayed_result)\n",
    "#     print(f\"Scheduled partition {i+1}/{n_workers} with {len(partition_df)} rows\")\n",
    "\n",
    "# # Create a main progress bar for overall progress\n",
    "# print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "# main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Create a global progress updater\n",
    "# def update_main_progress(future):\n",
    "#     # Update main progress bar based on worker progress\n",
    "#     # This function will be called repeatedly to update the main progress bar\n",
    "#     completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "#     main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "#     main_progress.refresh()\n",
    "\n",
    "# # Submit the tasks to the cluster\n",
    "# futures = client.compute(delayed_results)\n",
    "\n",
    "# # Start a loop to update the main progress bar\n",
    "# import threading\n",
    "# stop_flag = False\n",
    "\n",
    "# def progress_monitor():\n",
    "#     while not stop_flag:\n",
    "#         update_main_progress(futures)\n",
    "#         time.sleep(0.5)\n",
    "\n",
    "# # Start the progress monitor in a separate thread\n",
    "# monitor_thread = threading.Thread(target=progress_monitor)\n",
    "# monitor_thread.start()\n",
    "\n",
    "# # Wait for computation to complete\n",
    "# results = dask.compute(*delayed_results)\n",
    "\n",
    "# # Stop the progress monitor\n",
    "# stop_flag = True\n",
    "# monitor_thread.join()\n",
    "\n",
    "# # Update progress bar to completion\n",
    "# main_progress.n = len(final_report)\n",
    "# main_progress.refresh()\n",
    "# main_progress.close()\n",
    "\n",
    "# # Flatten the nested list of results\n",
    "# all_results = []\n",
    "# for worker_results in results:\n",
    "#     all_results.extend(worker_results)\n",
    "\n",
    "# # Sort by index\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # Report final timing\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# # Display results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # Shut down the client and cluster\n",
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (ULTRA OPTIMIZED - FIXED): Maximum GPU utilization for RTX 4080 Super\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from tqdm.auto import tqdm\n",
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # Force CUDA initialization and check memory\n",
    "# torch.cuda.init()\n",
    "# total_gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "# print(f\"Total GPU memory: {total_gpu_mem:.2f} GB\")\n",
    "\n",
    "# # Ultra-aggressive GPU optimization parameters for RTX 4080 Super\n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 64  # Much larger batch size to fully utilize VRAM\n",
    "# MAX_SEQUENCE_LENGTH = 1024  # Set maximum context length to optimize memory usage\n",
    "\n",
    "# # Load model and tokenizer directly for maximum control\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     torch_dtype=torch.float16,  # Half-precision for maximum throughput\n",
    "#     device_map=\"auto\"           # Automatically map to available GPU\n",
    "# )\n",
    "\n",
    "# # Move model to GPU and optimize for inference\n",
    "# model.to(\"cuda\")\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "# # Create a custom pipeline with maximum batch efficiency\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     framework='pt',\n",
    "#     # Force maximum GPU memory usage\n",
    "#     model_kwargs={\"use_cache\": True}\n",
    "# )\n",
    "\n",
    "# # Monitor GPU memory usage\n",
    "# def gpu_memory_usage():\n",
    "#     \"\"\"Return GPU memory usage in GB\"\"\"\n",
    "#     reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "#     allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "#     print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "#     return allocated, reserved\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     Ultra-optimized hierarchical summarization for maximum GPU utilization\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for processing with ultra-large batches\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Process in maximally large batches to saturate GPU\n",
    "#     # This is the key optimization - use much larger batches to fill VRAM\n",
    "#     summaries = []\n",
    "#     for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "#         batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "        \n",
    "#         # Log memory usage before batch\n",
    "#         print(f\"Processing batch of size {len(batch)} ({i}/{len(all_chunks)})\")\n",
    "#         gpu_memory_usage()\n",
    "        \n",
    "#         # Process maximum-sized batch\n",
    "#         batch_summaries = summarizer(\n",
    "#             batch,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )\n",
    "#         summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "        \n",
    "#         # Log memory after batch\n",
    "#         gpu_memory_usage()\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries in a single batch\n",
    "#     joined = \" \".join(summaries)\n",
    "    \n",
    "#     # Final summary\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # Pre-process all reviews to maximize throughput - FIXED THIS LINE\n",
    "# print(\"Preparing all reviews for processing...\")\n",
    "# all_rows = [(i, row['Reviews']) for i, (_, row) in enumerate(final_report.iterrows())]\n",
    "\n",
    "# # Process the entire dataset in sequential maximum-sized batches\n",
    "# # This approach ensures GPU is fully saturated\n",
    "# all_results = []\n",
    "# with tqdm(total=len(final_report), desc=\"Ultra GPU Optimization\") as pbar:\n",
    "#     # Process each row with maximum batch efficiency\n",
    "#     for i in range(0, len(all_rows), 10):  # Process in batches of 10 rows\n",
    "#         batch_rows = all_rows[i:i+10]\n",
    "#         batch_results = []\n",
    "        \n",
    "#         for batch_idx, (row_idx, reviews) in enumerate(batch_rows):\n",
    "#             # Force garbage collection before large operations\n",
    "#             if batch_idx % 5 == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 gc.collect()\n",
    "            \n",
    "#             # Process with maximum GPU utilization\n",
    "#             summary = hierarchical_summary(\n",
    "#                 reviews, \n",
    "#                 chunk_size=200, \n",
    "#                 max_len=60, \n",
    "#                 min_len=20\n",
    "#             )\n",
    "#             batch_results.append((row_idx, summary))\n",
    "#             pbar.update(1)\n",
    "        \n",
    "#         all_results.extend(batch_results)\n",
    "#         # Force GPU memory cleanup between large batches\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "\n",
    "# # Sort and store results\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # Inspect results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # Final cleanup\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b60ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (OPTIMIZED - FASTEST SO FAR): GPU-optimized hierarchical summarization for RTX 4080 Super\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import pipeline, AutoTokenizer\n",
    "# from tqdm.auto import tqdm\n",
    "# import torch\n",
    "# import concurrent.futures\n",
    "# import multiprocessing\n",
    "\n",
    "# # Check GPU memory and set optimal batch sizes\n",
    "# gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "# print(f\"Available GPU memory: {gpu_mem:.2f} GB\")\n",
    "\n",
    "# # RTX 4080 Super optimization parameters\n",
    "# # With 16GB VRAM, we can use larger batch sizes and optimize throughput\n",
    "# MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "# MAX_GPU_BATCH_SIZE = 32  # Larger batch size for 16GB VRAM\n",
    "# PARALLEL_PROCESSES = 4   # Optimal number for balancing CPU and GPU workloads\n",
    "\n",
    "# # 1) Initialize tokenizer to estimate token counts for optimal batching\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# # 2) Initialize the summarizer pipeline with optimized settings for RTX 4080 Super\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model=MODEL_NAME,\n",
    "#     device=0,\n",
    "#     framework='pt',\n",
    "#     # Optimized settings for higher throughput\n",
    "#     model_kwargs={\n",
    "#         \"use_cache\": True,  # Enable KV caching for faster inference\n",
    "#     },\n",
    "#     # Enable half-precision for faster processing and lower memory usage\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# def estimate_tokens(text):\n",
    "#     \"\"\"Estimate token count to optimize batching\"\"\"\n",
    "#     return len(tokenizer.encode(text))\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     GPU-optimized hierarchical summarization with dynamic batching\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for processing\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Dynamically determine optimal batch size based on token counts\n",
    "#     # For RTX 4080 Super with 16GB, we can process larger batches\n",
    "#     summaries = []\n",
    "#     for i in range(0, len(all_chunks), MAX_GPU_BATCH_SIZE):\n",
    "#         batch = all_chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "#         batch_summaries = summarizer(\n",
    "#             batch,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )\n",
    "#         summaries.extend([s['summary_text'] for s in batch_summaries])\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries\n",
    "#     # RTX 4080 Super can handle the full set of intermediate summaries\n",
    "#     joined = \" \".join(summaries)\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # 4) Function to process each batch of rows with GPU optimization\n",
    "# def process_gpu_batch(batch_df):\n",
    "#     results = []\n",
    "#     # Pre-collect all reviews to optimize memory transfers to GPU\n",
    "#     all_rows = [(row.name, row['Reviews']) for _, row in batch_df.iterrows()]\n",
    "    \n",
    "#     for idx, reviews in all_rows:\n",
    "#         # Use optimized hierarchical summary function\n",
    "#         summary = hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20)\n",
    "#         results.append((idx, summary))\n",
    "        \n",
    "#         # Optional: Force CUDA cache clearing every few iterations to prevent memory fragmentation\n",
    "#         if idx % 10 == 0:\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#     return results\n",
    "\n",
    "# # Calculate optimal processing strategy based on dataset size\n",
    "# total_rows = len(final_report)\n",
    "# # Determine batch size for parallel processing\n",
    "# optimal_batch_size = max(1, total_rows // PARALLEL_PROCESSES)\n",
    "\n",
    "# # Split dataframe into optimized batches\n",
    "# batches = [final_report.iloc[i:i+optimal_batch_size] for i in range(0, total_rows, optimal_batch_size)]\n",
    "\n",
    "# # Process with concurrent.futures and progress tracking\n",
    "# all_results = []\n",
    "# with tqdm(total=total_rows, desc=\"GPU Summarizing (RTX 4080 Super)\") as pbar:\n",
    "#     # Use ThreadPoolExecutor to manage parallel GPU tasks\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=PARALLEL_PROCESSES) as executor:\n",
    "#         future_to_batch = {executor.submit(process_gpu_batch, batch): batch for batch in batches}\n",
    "        \n",
    "#         for future in concurrent.futures.as_completed(future_to_batch):\n",
    "#             try:\n",
    "#                 batch_results = future.result()\n",
    "#                 all_results.extend(batch_results)\n",
    "#                 batch_size = len(future_to_batch[future])\n",
    "#                 pbar.update(batch_size)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing batch: {e}\")\n",
    "#                 # Continue with remaining batches\n",
    "\n",
    "# # Sort results by the original index\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # 5) Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # 6) Inspect results\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # 7) Clean up GPU memory\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (BIG DATA - BUT WITH PYTHON-FUTURES): Hierarchical summarization of all reviews per theme using parallel processing\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import pipeline\n",
    "# from tqdm.auto import tqdm\n",
    "# import concurrent.futures\n",
    "# import multiprocessing\n",
    "\n",
    "# # Set up the number of workers based on CPU cores (with one less to avoid overloading)\n",
    "# num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "# # 1) Initialize the summarizer pipeline globally\n",
    "# # This avoids serialization issues\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model='sshleifer/distilbart-cnn-12-6',\n",
    "#     framework='pt'\n",
    "# )\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200, max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     Summarize a long list of reviews into one short summary using parallel processing for chunks\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for processing\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Process chunks in a batch to maximize GPU usage\n",
    "#     intermediate_summaries = summarizer(\n",
    "#         all_chunks,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )\n",
    "    \n",
    "#     # Extract summary texts\n",
    "#     intermediate = [summary['summary_text'] for summary in intermediate_summaries]\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries\n",
    "#     joined = \" \".join(intermediate)\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # 4) Function to process each batch of rows in parallel\n",
    "# def process_batch(batch_df):\n",
    "#     results = []\n",
    "#     for _, row in batch_df.iterrows():\n",
    "#         summary = hierarchical_summary(row['Reviews'], chunk_size=200, max_len=60, min_len=20)\n",
    "#         results.append((row.name, summary))\n",
    "#     return results\n",
    "\n",
    "# # Split the dataframe into batches for parallel processing\n",
    "# def split_dataframe(df, batch_size):\n",
    "#     batches = []\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         batches.append(df.iloc[i:i+batch_size])\n",
    "#     return batches\n",
    "\n",
    "# # Calculate optimal batch size based on dataset size and worker count\n",
    "# batch_size = max(1, len(final_report) // num_workers)\n",
    "# batches = split_dataframe(final_report, batch_size)\n",
    "\n",
    "# # Use ThreadPoolExecutor for parallel processing with progress bar\n",
    "# all_results = []\n",
    "# with tqdm(total=len(final_report), desc=\"Summarizing themes\") as pbar:\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#         # Submit all batches to the executor\n",
    "#         future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        \n",
    "#         # Process completed batches and update progress\n",
    "#         for future in concurrent.futures.as_completed(future_to_batch):\n",
    "#             batch_results = future.result()\n",
    "#             all_results.extend(batch_results)\n",
    "#             # Update progress bar by the number of rows processed in this batch\n",
    "#             pbar.update(len(future_to_batch[future]))\n",
    "\n",
    "# # Sort results by the original index and extract summaries\n",
    "# all_results.sort(key=lambda x: x[0])  # Sort by index\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # 5) Store results in a new column\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # 6) Inspect\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9 (FIXED - FIRST): Hierarchical summarization of all reviews per theme\n",
    "\n",
    "# from transformers import pipeline\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # 1) Initialize a single summarizer pipeline\n",
    "# summarizer = pipeline(\n",
    "#     task='summarization',\n",
    "#     model='sshleifer/distilbart-cnn-12-6',\n",
    "#     device=0,              # change to -1 if no GPU\n",
    "#     framework='pt'\n",
    "# )\n",
    "\n",
    "# def hierarchical_summary(reviews, chunk_size=200,\n",
    "#                          max_len=60, min_len=20):\n",
    "#     \"\"\"\n",
    "#     Summarize a long list of reviews into one short summary:\n",
    "#       1) Chunk the reviews into batches of chunk_size\n",
    "#       2) Summarize each batch\n",
    "#       3) Summarize the concatenation of batch summaries\n",
    "    \n",
    "#     Params:\n",
    "#       reviews    : list of str, the reviews to summarize\n",
    "#       chunk_size : int, number of reviews per intermediate chunk\n",
    "#       max_len    : int, max summary tokens per call\n",
    "#       min_len    : int, min summary tokens per call\n",
    "    \n",
    "#     Returns:\n",
    "#       str, final \"quick read\" summary\n",
    "#     \"\"\"\n",
    "#     # If there are fewer than chunk_size, just do one summary\n",
    "#     if len(reviews) <= chunk_size:\n",
    "#         doc = \"\\n\\n\".join(reviews)\n",
    "#         return summarizer(\n",
    "#             doc,\n",
    "#             max_length=max_len,\n",
    "#             min_length=min_len,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # 2) Prepare all chunks for batch processing\n",
    "#     all_chunks = []\n",
    "#     for i in range(0, len(reviews), chunk_size):\n",
    "#         batch = reviews[i:i+chunk_size]\n",
    "#         text = \"\\n\\n\".join(batch)\n",
    "#         all_chunks.append(text)\n",
    "    \n",
    "#     # Process all chunks in one batch\n",
    "#     intermediate_summaries = summarizer(\n",
    "#         all_chunks,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )\n",
    "    \n",
    "#     # Extract summary texts\n",
    "#     intermediate = [summary['summary_text'] for summary in intermediate_summaries]\n",
    "    \n",
    "#     # 3) Summarize the intermediate summaries\n",
    "#     joined = \" \".join(intermediate)\n",
    "#     return summarizer(\n",
    "#         joined,\n",
    "#         max_length=max_len,\n",
    "#         min_length=min_len,\n",
    "#         truncation=True,\n",
    "#         do_sample=False\n",
    "#     )[0]['summary_text']\n",
    "\n",
    "# # 4) Apply to each row of final_report with progress bar\n",
    "# quick_summaries = []\n",
    "# for _, row in tqdm(final_report.iterrows(),\n",
    "#                   total=len(final_report),\n",
    "#                   desc=\"Summarizing themes\"):\n",
    "#     revs = row['Reviews']\n",
    "#     quick = hierarchical_summary(revs,\n",
    "#                                  chunk_size=200,\n",
    "#                                  max_len=60,\n",
    "#                                  min_len=20)\n",
    "#     quick_summaries.append(quick)\n",
    "\n",
    "# # 5) Store results in a new column\n",
    "# final_report['QuickSummary'] = quick_summaries\n",
    "\n",
    "# # 6) Inspect\n",
    "# display(final_report[['steam_appid','Theme','QuickSummary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5f8d7",
   "metadata": {},
   "source": [
    "# Tuned for my hardware 1m 50 secs inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe37d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Hardware-optimized GPU summarization with Dask - Tuned for Ryzen 9700X & RTX 4080 Super\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from tqdm.auto import tqdm\n",
    "# import time\n",
    "# import os\n",
    "# import threading\n",
    "\n",
    "# # Create checkpoint directory if it doesn't exist (minimal overhead)\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# # Optimized configuration for your specific hardware\n",
    "# # RTX 4080 Super (12GB usable VRAM) + Ryzen 9700X + 20GB usable RAM\n",
    "# HARDWARE_CONFIG = {\n",
    "#     'worker_count': 6,                # Optimal for Ryzen 9700X\n",
    "#     'memory_per_worker': '3GB',       # 18GB total for workers, leaving headroom\n",
    "#     'gpu_batch_size': 96,             # Aggressive batch size for RTX 4080 Super\n",
    "#     'model_name': 'sshleifer/distilbart-cnn-12-6',  # Best model for your GPU\n",
    "#     'chunk_size': 400,                # Larger chunks for faster processing\n",
    "#     'checkpoint_frequency': 25,       # Less frequent checkpoints for speed\n",
    "#     'cleanup_frequency': 10,          # Less frequent memory cleanup\n",
    "# }\n",
    "\n",
    "# print(f\"Starting optimized Dask cluster for Ryzen 9700X + RTX 4080 Super configuration\")\n",
    "# cluster = LocalCluster(\n",
    "#     n_workers=HARDWARE_CONFIG['worker_count'], \n",
    "#     threads_per_worker=2,\n",
    "#     memory_limit=HARDWARE_CONFIG['memory_per_worker']\n",
    "# )\n",
    "# client = Client(cluster)\n",
    "# print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# # Determine optimal partition sizes - larger for better throughput\n",
    "# @dask.delayed\n",
    "# def prepare_partition(start_idx, end_idx):\n",
    "#     \"\"\"Prepare a partition optimized for high-end hardware\"\"\"\n",
    "#     return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# # Create larger partitions for better throughput\n",
    "# n_workers = HARDWARE_CONFIG['worker_count']\n",
    "# partition_size = len(final_report) // n_workers\n",
    "# partitions = []\n",
    "# for i in range(n_workers):\n",
    "#     start_idx = i * partition_size\n",
    "#     end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "#     partitions.append(prepare_partition(start_idx, end_idx))\n",
    "#     print(f\"Prepared partition {i+1} with {end_idx-start_idx} items\")\n",
    "\n",
    "# # Optimized worker function with aggressive resource usage\n",
    "# @dask.delayed\n",
    "# def process_partition(partition_df, worker_id):\n",
    "#     \"\"\"Optimized worker for RTX 4080 Super\"\"\"\n",
    "#     # Import needed packages\n",
    "#     from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "#     import torch\n",
    "    \n",
    "#     # Load model components with optimal settings for RTX 4080 Super\n",
    "#     print(f\"Worker {worker_id} initializing with optimized settings for RTX 4080 Super\")\n",
    "    \n",
    "#     # Load tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(HARDWARE_CONFIG['model_name'])\n",
    "    \n",
    "#     # Load model with optimized settings for RTX 4080 Super\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#         HARDWARE_CONFIG['model_name'],\n",
    "#         torch_dtype=torch.float16,        # Half precision for speed\n",
    "#         device_map=\"auto\",                # Automatic device placement\n",
    "#         low_cpu_mem_usage=True            # Optimized memory usage\n",
    "#     )\n",
    "    \n",
    "#     # Create optimized pipeline\n",
    "#     summarizer = pipeline(\n",
    "#         task='summarization',\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         framework='pt',\n",
    "#         model_kwargs={\n",
    "#             \"use_cache\": True,            # Enable caching for speed\n",
    "#             \"return_dict_in_generate\": True  # More efficient generation\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     # Report GPU status\n",
    "#     gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "#     print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "#     # Highly optimized batch processing function\n",
    "#     def process_chunks_batched(chunks):\n",
    "#         \"\"\"Process chunks in large batches for RTX 4080 Super\"\"\"\n",
    "#         all_summaries = []\n",
    "        \n",
    "#         # Use large batches for the RTX 4080 Super\n",
    "#         for i in range(0, len(chunks), HARDWARE_CONFIG['gpu_batch_size']):\n",
    "#             batch = chunks[i:i+HARDWARE_CONFIG['gpu_batch_size']]\n",
    "#             batch_summaries = summarizer(\n",
    "#                 batch,\n",
    "#                 max_length=60,\n",
    "#                 min_length=20,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False,\n",
    "#                 num_beams=2  # Use beam search for better quality with minimal speed impact\n",
    "#             )\n",
    "#             all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "#             # Minimal cleanup - only when really needed\n",
    "#             if i % (HARDWARE_CONFIG['gpu_batch_size'] * 3) == 0 and torch.cuda.is_available():\n",
    "#                 torch.cuda.empty_cache()\n",
    "                    \n",
    "#         return all_summaries\n",
    "    \n",
    "#     # Optimized hierarchical summary function\n",
    "#     def hierarchical_summary(reviews):\n",
    "#         \"\"\"Create hierarchical summary with optimized chunk sizes\"\"\"\n",
    "#         # Handle edge cases efficiently\n",
    "#         if not reviews or not isinstance(reviews, list):\n",
    "#             return \"No reviews available for summarization.\"\n",
    "        \n",
    "#         # Fast path for small review sets\n",
    "#         if len(reviews) <= HARDWARE_CONFIG['chunk_size']:\n",
    "#             doc = \"\\n\\n\".join(reviews)\n",
    "#             return summarizer(\n",
    "#                 doc,\n",
    "#                 max_length=60,\n",
    "#                 min_length=20,\n",
    "#                 truncation=True,\n",
    "#                 do_sample=False\n",
    "#             )[0]['summary_text']\n",
    "        \n",
    "#         # Process larger review sets with optimized chunking\n",
    "#         all_chunks = []\n",
    "#         for i in range(0, len(reviews), HARDWARE_CONFIG['chunk_size']):\n",
    "#             batch = reviews[i:i+HARDWARE_CONFIG['chunk_size']]\n",
    "#             text = \"\\n\\n\".join(batch)\n",
    "#             all_chunks.append(text)\n",
    "        \n",
    "#         # Process chunks with optimized batching\n",
    "#         intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "#         # Create final summary\n",
    "#         joined = \" \".join(intermediate_summaries)\n",
    "#         return summarizer(\n",
    "#             joined,\n",
    "#             max_length=60,\n",
    "#             min_length=20,\n",
    "#             truncation=True,\n",
    "#             do_sample=False\n",
    "#         )[0]['summary_text']\n",
    "    \n",
    "#     # Process the partition with minimal overhead\n",
    "#     results = []\n",
    "    \n",
    "#     # Use tqdm for progress tracking\n",
    "#     with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "#         for idx, row in partition_df.iterrows():\n",
    "#             # Process the review\n",
    "#             summary = hierarchical_summary(row['Reviews'])\n",
    "#             results.append((idx, summary))\n",
    "            \n",
    "#             # Minimal cleanup - only every N iterations\n",
    "#             if len(results) % HARDWARE_CONFIG['cleanup_frequency'] == 0:\n",
    "#                 torch.cuda.empty_cache()\n",
    "                \n",
    "#             # Update progress bar\n",
    "#             pbar.update(1)\n",
    "    \n",
    "#     # Final cleanup\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     print(f\"Worker {worker_id} completed successfully\")\n",
    "#     return results\n",
    "\n",
    "# # Schedule tasks\n",
    "# print(f\"Scheduling {n_workers} optimized partitions...\")\n",
    "# delayed_results = []\n",
    "# for i in range(n_workers):\n",
    "#     delayed_result = process_partition(partitions[i], i)\n",
    "#     delayed_results.append(delayed_result)\n",
    "\n",
    "# # Streamlined progress tracking\n",
    "# print(\"\\nStarting optimized computation...\")\n",
    "# main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Minimal checkpoint system - only save occasionally\n",
    "# def update_main_progress(futures):\n",
    "#     while not stop_flag:\n",
    "#         # Count completed futures\n",
    "#         completed_count = sum(f.status == 'finished' for f in futures)\n",
    "#         completed_percentage = completed_count / len(futures)\n",
    "        \n",
    "#         # Update progress bar\n",
    "#         main_progress.n = int(len(final_report) * completed_percentage)\n",
    "#         main_progress.refresh()\n",
    "        \n",
    "#         # Only check every 5 seconds to reduce overhead\n",
    "#         time.sleep(5)\n",
    "\n",
    "# # Submit tasks to cluster\n",
    "# futures = client.compute(delayed_results)\n",
    "\n",
    "# # Start progress monitor with minimal overhead\n",
    "# stop_flag = False\n",
    "# monitor_thread = threading.Thread(target=update_main_progress, args=(futures,))\n",
    "# monitor_thread.daemon = True\n",
    "# monitor_thread.start()\n",
    "\n",
    "# # Wait for computation\n",
    "# try:\n",
    "#     print(\"Computing with optimal settings for RTX 4080 Super...\")\n",
    "#     results = client.gather(futures)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error with futures: {e}\")\n",
    "#     print(\"Falling back to direct computation...\")\n",
    "#     results = dask.compute(*delayed_results)\n",
    "\n",
    "# # Stop progress monitor\n",
    "# stop_flag = True\n",
    "# monitor_thread.join(timeout=3)\n",
    "\n",
    "# # Update progress to completion\n",
    "# main_progress.n = len(final_report)\n",
    "# main_progress.refresh()\n",
    "# main_progress.close()\n",
    "\n",
    "# # Process results efficiently\n",
    "# all_results = []\n",
    "# for worker_results in results:\n",
    "#     all_results.extend(worker_results)\n",
    "\n",
    "# # Sort results\n",
    "# all_results.sort(key=lambda x: x[0])\n",
    "# summaries = [result[1] for result in all_results]\n",
    "\n",
    "# # Store results\n",
    "# final_report['QuickSummary'] = summaries\n",
    "\n",
    "# # Report timing\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"\\nOptimized processing completed in {elapsed_time:.2f} seconds\")\n",
    "# print(f\"Average time per item: {elapsed_time/len(final_report):.2f} seconds\")\n",
    "\n",
    "# # Display results\n",
    "# print(\"\\nResults sample:\")\n",
    "# display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# # Save results\n",
    "# final_report.to_csv('output_csvs/optimized_hardware_report.csv')\n",
    "# print(\"Results saved to output_csvs/optimized_hardware_report.csv\")\n",
    "\n",
    "# # Clean up\n",
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def20d85",
   "metadata": {},
   "source": [
    "# Dynamic Allocation and takes twice as much time but can run on most systems by reading the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Dynamically optimized GPU hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import json\n",
    "import threading\n",
    "\n",
    "# 1. Dynamic resource allocation based on system capabilities\n",
    "def get_system_resources():\n",
    "    \"\"\"Determine optimal system resource allocation\"\"\"\n",
    "    # Get available memory and CPU resources\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)  # GB\n",
    "    available_memory = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "    cpu_count = psutil.cpu_count(logical=False) or psutil.cpu_count(logical=True)\n",
    "    \n",
    "    # Check for GPU presence and memory\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_count = torch.cuda.device_count() if gpu_available else 0\n",
    "    gpu_memory = [torch.cuda.get_device_properties(i).total_memory / (1024**3) for i in range(gpu_count)] if gpu_available else []\n",
    "    \n",
    "    # Determine optimal worker count - leave cores for system and GPU processes\n",
    "    if gpu_available:\n",
    "        # For GPU workloads, fewer workers but more memory per worker\n",
    "        worker_count = min(max(1, cpu_count // 2), gpu_count + 1)\n",
    "    else:\n",
    "        # For CPU workloads, use more workers\n",
    "        worker_count = max(1, cpu_count - 1)\n",
    "    \n",
    "    # Memory per worker (70% of available to leave headroom)\n",
    "    safe_memory = available_memory * 0.7\n",
    "    memory_per_worker = safe_memory / worker_count\n",
    "    \n",
    "    # Dynamic chunk size based on available memory\n",
    "    if memory_per_worker > 8:  # High memory\n",
    "        chunk_size = 300\n",
    "    elif memory_per_worker > 4:  # Medium memory\n",
    "        chunk_size = 200\n",
    "    else:  # Low memory\n",
    "        chunk_size = 100\n",
    "    \n",
    "    print(f\"System resources: {total_memory:.1f}GB total RAM, {available_memory:.1f}GB available\")\n",
    "    print(f\"CPU cores: {cpu_count}, GPU count: {gpu_count}\")\n",
    "    if gpu_count > 0:\n",
    "        for i, mem in enumerate(gpu_memory):\n",
    "            print(f\"GPU {i}: {mem:.1f}GB memory\")\n",
    "    \n",
    "    return {\n",
    "        'worker_count': worker_count,\n",
    "        'memory_per_worker': memory_per_worker,\n",
    "        'chunk_size': chunk_size,\n",
    "        'gpu_available': gpu_available,\n",
    "        'gpu_count': gpu_count,\n",
    "        'gpu_memory': gpu_memory\n",
    "    }\n",
    "\n",
    "# Get system resources\n",
    "resources = get_system_resources()\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Start a local Dask cluster with dynamic resources\n",
    "n_workers = resources['worker_count']\n",
    "print(f\"Starting Dask cluster with {n_workers} workers, {resources['memory_per_worker']:.1f}GB per worker\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=n_workers, \n",
    "    threads_per_worker=2,\n",
    "    memory_limit=f\"{resources['memory_per_worker']:.1f}GB\"\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# 2. Determine model based on available resources\n",
    "def select_model():\n",
    "    \"\"\"Select appropriate model based on available resources\"\"\"\n",
    "    if resources['gpu_available'] and any(mem > 8 for mem in resources['gpu_memory']):\n",
    "        # For high-end GPUs, use more powerful model\n",
    "        return 'sshleifer/distilbart-cnn-12-6'\n",
    "    elif resources['gpu_available']:\n",
    "        # For lower-end GPUs, use smaller model\n",
    "        return 'facebook/bart-large-cnn'\n",
    "    else:\n",
    "        # For CPU-only, use smallest model\n",
    "        return 'facebook/bart-base'\n",
    "\n",
    "# Select model based on resources\n",
    "MODEL_NAME = select_model()\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "\n",
    "# 3. First, load the data and check for existing checkpoints\n",
    "def load_with_checkpoint():\n",
    "    \"\"\"Load data with checkpoint recovery\"\"\"\n",
    "    checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"Found checkpoint with {len(checkpoint)} completed summaries\")\n",
    "            \n",
    "        # Filter the dataframe to only process remaining rows\n",
    "        completed_indices = list(map(int, checkpoint.keys()))\n",
    "        remaining_df = final_report[~final_report.index.isin(completed_indices)].copy()\n",
    "        \n",
    "        print(f\"Resuming processing for {len(remaining_df)} remaining items\")\n",
    "        return remaining_df, checkpoint\n",
    "    else:\n",
    "        print(\"No checkpoint found, processing all items\")\n",
    "        return final_report, {}\n",
    "\n",
    "# Load data with checkpoint support\n",
    "df_to_process, existing_summaries = load_with_checkpoint()\n",
    "\n",
    "# 4. Prepare partitions with optimized distribution\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx, df):\n",
    "    \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "    # Get just this partition\n",
    "    return df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Distribute the remaining work\n",
    "partition_size = len(df_to_process) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(df_to_process)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx, df_to_process))\n",
    "\n",
    "# 5. Worker processing function with dynamic GPU batch sizing\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition with dynamic batch sizes and error recovery\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # Determine optimal GPU batch size based on available memory\n",
    "    def determine_gpu_batch_size():\n",
    "        if not torch.cuda.is_available():\n",
    "            return 8  # Conservative default for CPU\n",
    "            \n",
    "        try:\n",
    "            # Get GPU memory info for this worker\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "            # Reserve 10% for system processes and overhead\n",
    "            usable_mem = total_mem * 0.9\n",
    "            \n",
    "            # Scale batch size based on available GPU memory\n",
    "            if usable_mem > 16:  # High-end GPU with >16GB\n",
    "                return 64\n",
    "            elif usable_mem > 8:  # Mid-range GPU with >8GB\n",
    "                return 32\n",
    "            elif usable_mem > 4:  # Lower-end GPU with >4GB\n",
    "                return 16\n",
    "            else:  # Minimal GPU\n",
    "                return 8\n",
    "        except Exception as e:\n",
    "            print(f\"Error determining GPU batch size: {e}\")\n",
    "            return 8  # Conservative fallback\n",
    "    \n",
    "    # Worker initialization with error handling\n",
    "    try:\n",
    "        # Load tokenizer first\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # Configure device placement based on available resources\n",
    "        if torch.cuda.is_available():\n",
    "            device_map = \"auto\"\n",
    "            dtype = torch.float16  # Use half precision with GPU\n",
    "        else:\n",
    "            device_map = None\n",
    "            dtype = torch.float32  # Use full precision with CPU\n",
    "        \n",
    "        # Load model with appropriate configuration\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device_map,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Create pipeline with model AND tokenizer\n",
    "        summarizer = pipeline(\n",
    "            task='summarization',\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            framework='pt',\n",
    "            model_kwargs={\"use_cache\": True}\n",
    "        )\n",
    "        \n",
    "        # Report worker status\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "            MAX_GPU_BATCH_SIZE = determine_gpu_batch_size()\n",
    "            print(f\"Worker {worker_id}: Using GPU batch size: {MAX_GPU_BATCH_SIZE}\")\n",
    "        else:\n",
    "            MAX_GPU_BATCH_SIZE = 8\n",
    "            print(f\"Worker {worker_id}: Using CPU with batch size: {MAX_GPU_BATCH_SIZE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Worker {worker_id} initialization error: {e}\")\n",
    "        # Fall back to a simpler configuration\n",
    "        try:\n",
    "            print(f\"Falling back to CPU-only mode for worker {worker_id}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "            summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "            MAX_GPU_BATCH_SIZE = 4  # Conservative batch size for fallback mode\n",
    "        except Exception as e2:\n",
    "            print(f\"Critical failure in worker {worker_id}: {e2}\")\n",
    "            return []  # Return empty results to avoid deadlock\n",
    "    \n",
    "    # Efficient batch processing function with memory management\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in batches with dynamic memory management\"\"\"\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Process in dynamically sized batches\n",
    "        for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "            try:\n",
    "                batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "                batch_summaries = summarizer(\n",
    "                    batch,\n",
    "                    max_length=60,\n",
    "                    min_length=20,\n",
    "                    truncation=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "                \n",
    "                # Proactively manage memory\n",
    "                if i % (MAX_GPU_BATCH_SIZE * 2) == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//MAX_GPU_BATCH_SIZE} of worker {worker_id}: {e}\")\n",
    "                # Try smaller batch on failure\n",
    "                if len(batch) > 1:\n",
    "                    print(\"Retrying with smaller batches...\")\n",
    "                    for single_item in batch:\n",
    "                        try:\n",
    "                            summary = summarizer(\n",
    "                                [single_item],\n",
    "                                max_length=60,\n",
    "                                min_length=20,\n",
    "                                truncation=True,\n",
    "                                do_sample=False\n",
    "                            )\n",
    "                            all_summaries.append(summary[0][\"summary_text\"])\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Failed to process single item: {e2}\")\n",
    "                            all_summaries.append(\"Error generating summary.\")\n",
    "                else:\n",
    "                    all_summaries.append(\"Error generating summary.\")\n",
    "                \n",
    "                # Clean up after errors\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        return all_summaries\n",
    "    \n",
    "    # Hierarchical summary function with adaptive chunking\n",
    "    def hierarchical_summary(reviews, base_chunk_size=200):\n",
    "        \"\"\"Create hierarchical summary with adaptive chunk sizing\"\"\"\n",
    "        # Defense against empty or invalid reviews\n",
    "        if not reviews or not isinstance(reviews, list):\n",
    "            return \"No reviews available for summarization.\"\n",
    "        \n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= base_chunk_size:\n",
    "            try:\n",
    "                # Join reviews with clear separation\n",
    "                doc = \"\\n\\n\".join(reviews[:base_chunk_size])\n",
    "                return summarizer(\n",
    "                    doc,\n",
    "                    max_length=60,\n",
    "                    min_length=20,\n",
    "                    truncation=True,\n",
    "                    do_sample=False\n",
    "                )[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                print(f\"Error summarizing small batch: {e}\")\n",
    "                # Try with even smaller batch if original fails\n",
    "                try:\n",
    "                    half_size = len(reviews) // 2\n",
    "                    doc = \"\\n\\n\".join(reviews[:half_size])\n",
    "                    return summarizer(\n",
    "                        doc,\n",
    "                        max_length=60,\n",
    "                        min_length=20, \n",
    "                        truncation=True,\n",
    "                        do_sample=False\n",
    "                    )[0]['summary_text']\n",
    "                except:\n",
    "                    return \"Error generating summary for this batch.\"\n",
    "        \n",
    "        # Adaptively determine chunk size based on review length\n",
    "        # If reviews are very short, use larger chunks\n",
    "        avg_review_len = sum(len(r) for r in reviews[:100]) / min(100, len(reviews))\n",
    "        if avg_review_len < 100:  # Very short reviews\n",
    "            chunk_size = min(base_chunk_size * 2, 500)\n",
    "        elif avg_review_len > 500:  # Very long reviews\n",
    "            chunk_size = max(base_chunk_size // 2, 50)\n",
    "        else:\n",
    "            chunk_size = base_chunk_size\n",
    "            \n",
    "        print(f\"Worker {worker_id}: Using chunk size {chunk_size} for avg review length {avg_review_len:.1f}\")\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with batched processing\n",
    "        try:\n",
    "            intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "            \n",
    "            # Summarize the intermediate summaries\n",
    "            joined = \" \".join(intermediate_summaries)\n",
    "            final_summary = summarizer(\n",
    "                joined,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "            \n",
    "            return final_summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error in hierarchical summarization: {e}\")\n",
    "            # Try to salvage what we can\n",
    "            if intermediate_summaries:\n",
    "                try:\n",
    "                    return f\"Partial summary: {' '.join(intermediate_summaries[:3])}\"\n",
    "                except:\n",
    "                    pass\n",
    "            return \"Error generating hierarchical summary.\"\n",
    "    \n",
    "    # Process the partition with checkpointing\n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            try:\n",
    "                # Skip processing if we already have too many errors in a row\n",
    "                if processed_count > 0 and len(results) == 0:\n",
    "                    # If first N items all failed, skip this worker\n",
    "                    if processed_count >= 5:\n",
    "                        print(f\"Worker {worker_id} failing consistently, aborting\")\n",
    "                        break\n",
    "                \n",
    "                # Process the review with the adaptive chunk size\n",
    "                summary = hierarchical_summary(row['Reviews'], base_chunk_size=resources['chunk_size'])\n",
    "                results.append((idx, summary))\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Clean up every few iterations\n",
    "                if processed_count % 5 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                # Checkpoint every 10 items\n",
    "                if processed_count % 10 == 0:\n",
    "                    print(f\"Worker {worker_id}: Processed {processed_count}/{len(partition_df)} items\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx} in worker {worker_id}: {e}\")\n",
    "                # Still record the error so we know this row was attempted\n",
    "                results.append((idx, f\"Error: Failed to generate summary.\"))\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Final cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Worker {worker_id} completed: {len(results)}/{len(partition_df)} successful\")\n",
    "    return results\n",
    "\n",
    "# 6. Schedule the tasks with the delayed partitions\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# 7. Progress tracking and checkpointing\n",
    "# Create main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(df_to_process), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater with checkpointing\n",
    "def update_main_progress(futures):\n",
    "    \"\"\"Update progress bar and save checkpoints\"\"\"\n",
    "    checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "    summaries_so_far = existing_summaries.copy()\n",
    "    \n",
    "    while not stop_flag:\n",
    "        # Count completed futures\n",
    "        completed_count = sum(f.status == 'finished' for f in futures)\n",
    "        completed_percentage = completed_count / len(futures)\n",
    "        \n",
    "        # Update progress bar\n",
    "        main_progress.n = int(len(df_to_process) * completed_percentage)\n",
    "        main_progress.refresh()\n",
    "        \n",
    "        # Check for newly completed results and update checkpoint\n",
    "        for future in [f for f in futures if f.status == 'finished']:\n",
    "            try:\n",
    "                result = future.result()\n",
    "                for idx, summary in result:\n",
    "                    summaries_so_far[str(idx)] = summary\n",
    "            except:\n",
    "                pass  # Skip failed futures\n",
    "        \n",
    "        # Save checkpoint every 30 seconds\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(summaries_so_far, f)\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "stop_flag = False\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=update_main_progress, args=(futures,))\n",
    "monitor_thread.daemon = True  # Allow program to exit if thread is still running\n",
    "monitor_thread.start()\n",
    "\n",
    "# 8. Wait for computation to complete with robust error handling\n",
    "try:\n",
    "    print(\"Computing all partitions...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    # Fallback to direct computation if future gathering fails\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join(timeout=5)  # Wait for thread to terminate, but with timeout\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(df_to_process)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# 9. Process results with checkpoint recovery\n",
    "all_results = []\n",
    "\n",
    "# Gather results from all workers\n",
    "for worker_results in results:\n",
    "    if worker_results:  # Check if worker returned any results\n",
    "        all_results.extend(worker_results)\n",
    "\n",
    "# Load checkpoint file for any results we already had\n",
    "checkpoint_file = 'checkpoints/summarization_progress.json'\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "        \n",
    "    # Add checkpoint data for any missing indices\n",
    "    result_indices = [idx for idx, _ in all_results]\n",
    "    for idx_str, summary in checkpoint_data.items():\n",
    "        idx = int(idx_str)\n",
    "        if idx not in result_indices:\n",
    "            all_results.append((idx, summary))\n",
    "\n",
    "# Sort by index to maintain order\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create a dictionary mapping of indices to summaries\n",
    "result_dict = {idx: summary for idx, summary in all_results}\n",
    "\n",
    "# Apply to final report\n",
    "final_report['QuickSummary'] = final_report.index.map(\n",
    "    lambda idx: result_dict.get(idx, \"Summary not generated\")\n",
    ")\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Successfully summarized {len(result_dict)}/{len(final_report)} items\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSample results:\")\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# 10. Save the results\n",
    "final_report.to_csv('output_csvs/dynamic_summarized_report.csv')\n",
    "print(\"Results saved to output_csvs/dynamic_summarized_report.csv\")\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc037cd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
