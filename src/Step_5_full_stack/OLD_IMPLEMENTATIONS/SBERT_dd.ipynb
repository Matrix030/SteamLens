{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83b36d8",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0651ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Dask Client\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Start a local Dask client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Theme Dictionary & Precompute Embeddings\n",
    "#only send the theme embeddings for the required worker and not all the embedding at once.\n",
    "import json\n",
    "\n",
    "# Load per-game theme keywords\n",
    "with open('game_themes.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "GAME_THEMES = {int(appid): themes for appid, themes in raw.items()}\n",
    "\n",
    "# Initialize SBERT embedder\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Precompute theme embeddings for each game\n",
    "THEME_EMBEDDINGS = {}\n",
    "for appid, themes in GAME_THEMES.items():\n",
    "    emb_list = []\n",
    "    for theme, seeds in themes.items():\n",
    "        seed_emb = embedder.encode(seeds, convert_to_numpy=True)\n",
    "        emb_list.append(seed_emb.mean(axis=0))\n",
    "    THEME_EMBEDDINGS[appid] = np.vstack(emb_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e133960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Read All Parquet Files into a Dask DataFrame\n",
    "# Assumes all game parquet files are in the same folder\n",
    "ddf = dd.read_parquet(\n",
    "    'parquet_output_theme_combo/*.parquet',\n",
    "    columns=['steam_appid', 'review', 'review_language', 'voted_up']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ff75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Filter & Clean Data\n",
    "# Keep only English reviews and drop missing text\n",
    "ddf = ddf[ddf['review_language'] == 'english']\n",
    "ddf = ddf.dropna(subset=['review'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aea5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define Partition-wise Topic Assignment\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # reused on each worker\n",
    "\n",
    "def assign_topic(df_partition):\n",
    "    # If no rows, return as-is\n",
    "    if df_partition.empty:\n",
    "        df_partition['topic_id'] = []\n",
    "        return df_partition\n",
    "\n",
    "    reviews = df_partition['review'].tolist()\n",
    "    # Compute embeddings in one go\n",
    "    review_embeds = embedder.encode(reviews, convert_to_numpy=True, batch_size=64)\n",
    "    \n",
    "    # Assign each review to its game-specific theme\n",
    "    topic_ids = []\n",
    "    for idx, appid in enumerate(df_partition['steam_appid']):\n",
    "        theme_embs = THEME_EMBEDDINGS[int(appid)]\n",
    "        sims = cosine_similarity(review_embeds[idx:idx+1], theme_embs)\n",
    "        topic_ids.append(int(sims.argmax()))\n",
    "    \n",
    "    df_partition['topic_id'] = topic_ids\n",
    "    return df_partition\n",
    "\n",
    "# Apply to each partition; specify output metadata\n",
    "meta = ddf._meta.assign(topic_id=np.int64())\n",
    "ddf_with_topic = ddf.map_partitions(assign_topic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1450bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Aggregate Counts, Likes, and Collect Reviews per Theme\n",
    "# Count reviews and sum votes per (game, theme)\n",
    "agg = ddf_with_topic.groupby(['steam_appid', 'topic_id']).agg(\n",
    "    review_count=('review', 'count'),\n",
    "    likes_sum=('voted_up', 'sum')\n",
    ")\n",
    "\n",
    "# Also collect reviews into lists per group\n",
    "reviews_series = ddf_with_topic.groupby(['steam_appid', 'topic_id'])['review'] \\\n",
    "    .apply(lambda x: list(x), meta=('review', object))\n",
    "\n",
    "# Compute both in parallel\n",
    "agg_df, reviews_df = dd.compute(agg, reviews_series)\n",
    "\n",
    "# Convert reviews series to DataFrame\n",
    "reviews_df = reviews_df.reset_index().rename(columns={'review': 'Reviews'})\n",
    "\n",
    "# Convert aggregation to DataFrame\n",
    "agg_df = agg_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Construct Final Report DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Merge counts, likes, and reviews\n",
    "report_df = pd.merge(\n",
    "    agg_df,\n",
    "    reviews_df,\n",
    "    on=['steam_appid', 'topic_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Build the final output structure\n",
    "rows = []\n",
    "for _, row in report_df.iterrows():\n",
    "    appid = int(row['steam_appid'])\n",
    "    tid = int(row['topic_id'])\n",
    "    theme_name = list(GAME_THEMES[appid].keys())[tid]\n",
    "    total = int(row['review_count'])\n",
    "    likes = int(row['likes_sum'])\n",
    "    like_ratio = f\"{(likes / total * 100):.1f}%\" if total > 0 else '0%'\n",
    "    rows.append({\n",
    "        'steam_appid': appid,\n",
    "        'Theme': theme_name,\n",
    "        '#Reviews': total,\n",
    "        'LikeRatio': like_ratio,\n",
    "        'Reviews': row['Reviews']\n",
    "    })\n",
    "\n",
    "final_report = pd.DataFrame(rows)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "final_report.to_csv('output_csvs/SBERT_DD_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd04e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 8: View the Report\n",
    "print(final_report.head())\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e07422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 (FULLY OPTIMIZED - FIXED): GPU-optimized hierarchical summarization with Dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Start a local Dask cluster\n",
    "n_workers = 4  # Adjust based on your CPU core count\n",
    "cluster = LocalCluster(n_workers=n_workers, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "# Define model parameters \n",
    "MODEL_NAME = 'sshleifer/distilbart-cnn-12-6'\n",
    "MAX_GPU_BATCH_SIZE = 64  # Large batch size for RTX 4080 Super\n",
    "\n",
    "# First, load the data once and distribute it to avoid repetition\n",
    "@dask.delayed\n",
    "def prepare_partition(start_idx, end_idx):\n",
    "    \"\"\"Prepare a partition without loading the entire DataFrame into each worker\"\"\"\n",
    "    # Get just this partition\n",
    "    return final_report.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Prepare partitions with delayed\n",
    "partition_size = len(final_report) // n_workers\n",
    "partitions = []\n",
    "for i in range(n_workers):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i < n_workers - 1 else len(final_report)\n",
    "    partitions.append(prepare_partition(start_idx, end_idx))\n",
    "\n",
    "# The main processing function - FIXED: Removed dependency on datasets library\n",
    "@dask.delayed\n",
    "def process_partition(partition_df, worker_id):\n",
    "    \"\"\"Process a partition of the data on a worker with batch processing\"\"\"\n",
    "    # Import packages needed in the worker\n",
    "    from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Load tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model with device_map=\"auto\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with model AND tokenizer\n",
    "    summarizer = pipeline(\n",
    "        task='summarization',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework='pt',\n",
    "        model_kwargs={\"use_cache\": True}\n",
    "    )\n",
    "    \n",
    "    # Report worker GPU status\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    print(f\"Worker {worker_id}: GPU Memory: {gpu_mem:.2f}GB allocated\")\n",
    "    \n",
    "    # FIXED: Process chunks in batches without requiring the datasets library\n",
    "    def process_chunks_batched(chunks):\n",
    "        \"\"\"Process chunks in batches for efficient GPU utilization\"\"\"\n",
    "        # Process in large batches to utilize GPU effectively\n",
    "        all_summaries = []\n",
    "        \n",
    "        # Process in batches of MAX_GPU_BATCH_SIZE\n",
    "        for i in range(0, len(chunks), MAX_GPU_BATCH_SIZE):\n",
    "            batch = chunks[i:i+MAX_GPU_BATCH_SIZE]\n",
    "            batch_summaries = summarizer(\n",
    "                batch,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "            all_summaries.extend([s[\"summary_text\"] for s in batch_summaries])\n",
    "            \n",
    "        return all_summaries\n",
    "    \n",
    "    # Define the hierarchical summary function with batch processing\n",
    "    def hierarchical_summary(reviews, chunk_size=200):\n",
    "        # If there are fewer than chunk_size, just do one summary\n",
    "        if len(reviews) <= chunk_size:\n",
    "            doc = \"\\n\\n\".join(reviews)\n",
    "            return summarizer(\n",
    "                doc,\n",
    "                max_length=60,\n",
    "                min_length=20,\n",
    "                truncation=True,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "        \n",
    "        # Prepare all chunks for processing\n",
    "        all_chunks = []\n",
    "        for i in range(0, len(reviews), chunk_size):\n",
    "            batch = reviews[i:i+chunk_size]\n",
    "            text = \"\\n\\n\".join(batch)\n",
    "            all_chunks.append(text)\n",
    "        \n",
    "        # Process chunks with batched processing\n",
    "        intermediate_summaries = process_chunks_batched(all_chunks)\n",
    "        \n",
    "        # Summarize the intermediate summaries\n",
    "        joined = \" \".join(intermediate_summaries)\n",
    "        return summarizer(\n",
    "            joined,\n",
    "            max_length=60,\n",
    "            min_length=20,\n",
    "            truncation=True,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "    \n",
    "    # Process the partition with a progress bar\n",
    "    results = []\n",
    "    # Create a progress bar for this worker\n",
    "    with tqdm(total=len(partition_df), desc=f\"Worker {worker_id}\", position=worker_id) as pbar:\n",
    "        for idx, row in partition_df.iterrows():\n",
    "            summary = hierarchical_summary(row['Reviews'], chunk_size=200)\n",
    "            results.append((idx, summary))\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Clean up every few iterations\n",
    "            if len(results) % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Clean up at the end\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    del summarizer\n",
    "    \n",
    "    # Return the results for this partition\n",
    "    return results\n",
    "\n",
    "# Schedule the tasks with the delayed partitions\n",
    "print(f\"Scheduling {n_workers} partitions for processing...\")\n",
    "delayed_results = []\n",
    "for i in range(n_workers):\n",
    "    delayed_result = process_partition(partitions[i], i)\n",
    "    delayed_results.append(delayed_result)\n",
    "    print(f\"Scheduled partition {i+1}/{n_workers}\")\n",
    "\n",
    "# Create a main progress bar for overall progress\n",
    "print(\"\\nStarting distributed computation with progress tracking:\")\n",
    "main_progress = tqdm(total=len(final_report), desc=\"Overall Progress\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a global progress updater\n",
    "def update_main_progress(future):\n",
    "    # Update main progress bar based on worker progress\n",
    "    completed_tasks = sum(future.status == \"finished\" for future in client.futures.values())\n",
    "    main_progress.n = min(len(final_report), completed_tasks * (len(final_report) // len(delayed_results)))\n",
    "    main_progress.refresh()\n",
    "\n",
    "# Submit the tasks to the cluster\n",
    "futures = client.compute(delayed_results)\n",
    "\n",
    "# Start a loop to update the main progress bar\n",
    "import threading\n",
    "stop_flag = False\n",
    "\n",
    "def progress_monitor():\n",
    "    while not stop_flag:\n",
    "        update_main_progress(futures)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Start the progress monitor in a separate thread\n",
    "monitor_thread = threading.Thread(target=progress_monitor)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for computation to complete - FIXED: Added more reliable computation approach\n",
    "try:\n",
    "    print(\"Computing all partitions...\")\n",
    "    results = client.gather(futures)\n",
    "except Exception as e:\n",
    "    # Fallback to direct computation if future gathering fails\n",
    "    print(f\"Error with futures: {e}\")\n",
    "    print(\"Falling back to direct computation...\")\n",
    "    results = dask.compute(*delayed_results)\n",
    "\n",
    "# Stop the progress monitor\n",
    "stop_flag = True\n",
    "monitor_thread.join()\n",
    "\n",
    "# Update progress bar to completion\n",
    "main_progress.n = len(final_report)\n",
    "main_progress.refresh()\n",
    "main_progress.close()\n",
    "\n",
    "# Flatten the nested list of results\n",
    "all_results = []\n",
    "for worker_results in results:\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "# Sort by index\n",
    "all_results.sort(key=lambda x: x[0])\n",
    "summaries = [result[1] for result in all_results]\n",
    "\n",
    "# Store results in a new column\n",
    "final_report['QuickSummary'] = summaries\n",
    "\n",
    "# Report final timing\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "display(final_report[['steam_appid', 'Theme', 'QuickSummary']].head())\n",
    "\n",
    "# Shut down the client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report.to_csv('output_csvs/summarized_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400fa28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
