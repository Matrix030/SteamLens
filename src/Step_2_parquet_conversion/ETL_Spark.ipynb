{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0f5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b140c00",
   "metadata": {},
   "source": [
    "# Total Number of entries in all the parquet files using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417cec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for Parquet files in ../../parquet_output...\n",
      "Found 20280 Parquet files totaling 7,853,833,200 bytes (7.31 GB)\n",
      "Initializing Spark session...\n",
      "Reading Parquet files...\n",
      "Counting total rows (this may take a while for large datasets)...\n",
      "Total reviews: 35,110,614\n",
      "Total processing time: 23.29 seconds (0.39 minutes)\n",
      "Stopping Spark session...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import time\n",
    "\n",
    "def count_total_reviews():\n",
    "    \"\"\"\n",
    "    Count the total number of reviews across all Parquet files\n",
    "    in the parquet_output directory using PySpark.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1) Gather only non-empty .parquet files\n",
    "    base_dir = \"../../parquet_output\"\n",
    "    good_files = []\n",
    "    total_size = 0\n",
    "    \n",
    "    print(f\"Scanning for Parquet files in {base_dir}...\")\n",
    "    \n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".parquet\"):\n",
    "                path = os.path.join(root, f)\n",
    "                file_size = os.path.getsize(path)\n",
    "                if file_size > 0:\n",
    "                    normalized_path = path.replace(\"\\\\\", \"/\")  # normalize on Windows\n",
    "                    good_files.append(normalized_path)\n",
    "                    total_size += file_size\n",
    "    \n",
    "    if not good_files:\n",
    "        print(\"No valid parquet files found.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(good_files)} Parquet files totaling {total_size:,} bytes ({total_size/(1024*1024*1024):.2f} GB)\")\n",
    "    \n",
    "    # 2) Start Spark with appropriate configurations\n",
    "    print(\"Initializing Spark session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TotalReviews\") \\\n",
    "        .config(\"spark.driver.memory\", \"20g\") \\\n",
    "        .config(\"spark.executor.memory\", \"20g\") \\\n",
    "        .config(\"spark.sql.files.ignoreCorruptFiles\", \"true\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600s\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce noise\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # 3) Read them all at once\n",
    "        print(\"Reading Parquet files...\")\n",
    "        df = spark.read.parquet(*good_files)\n",
    "        \n",
    "        # 4) Count rows\n",
    "        print(\"Counting total rows (this may take a while for large datasets)...\")\n",
    "        total_reviews = df.count()\n",
    "        print(f\"Total reviews: {total_reviews:,}\")\n",
    "        \n",
    "        # Calculate duration\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Total processing time: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
    "        \n",
    "        return total_reviews\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Always stop Spark session when done\n",
    "        print(\"Stopping Spark session...\")\n",
    "        spark.stop()\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    count_total_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c582b",
   "metadata": {},
   "source": [
    "# Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98614ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 35110614\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1) Gather only non-empty .parquet files\n",
    "base_dir = \"parquet_output\"\n",
    "good_files = []\n",
    "for root, _, files in os.walk(base_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".parquet\"):\n",
    "            path = os.path.join(root, f)\n",
    "            if os.path.getsize(path) > 0:\n",
    "                good_files.append(path.replace(\"\\\\\", \"/\"))  # normalize on Windows\n",
    "\n",
    "if not good_files:\n",
    "    print(\"No valid parquet files found.\")\n",
    "    exit(1)\n",
    "\n",
    "# 2) Create Dask dataframe from parquet files\n",
    "# The read_parquet function can take a list of files\n",
    "# df = dd.read_parquet(good_files)\n",
    "df = dd.read_parquet(good_files)\n",
    "# 3) Count rows (compute triggers actual execution)\n",
    "total_reviews = len(df)  # This returns a standard Python integer\n",
    "print(f\"Total reviews: {total_reviews}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8675e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
