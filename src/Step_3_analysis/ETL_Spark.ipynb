{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c582b",
   "metadata": {},
   "source": [
    "# Total Number of entries in all the parquet files using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1) Gather only non-empty .parquet files\n",
    "base_dir = \"cleaned_data_polars\"\n",
    "good_files = []\n",
    "for root, _, files in os.walk(base_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".parquet\"):\n",
    "            path = os.path.join(root, f)\n",
    "            if os.path.getsize(path) > 0:\n",
    "                good_files.append(path.replace(\"\\\\\", \"/\"))  # normalize on Windows\n",
    "\n",
    "if not good_files:\n",
    "    print(\"No valid parquet files found.\")\n",
    "    exit(1)\n",
    "\n",
    "# 2) Create Dask dataframe from parquet files\n",
    "# The read_parquet function can take a list of files\n",
    "df = dd.read_parquet(good_files)\n",
    "# 3) Count rows (compute triggers actual execution)\n",
    "total_reviews = len(df)  # This returns a standard Python integer\n",
    "print(f\"Total reviews: {total_reviews}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4060d3",
   "metadata": {},
   "source": [
    "# Total Number of entries in all the parquet files using Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547cd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, polars as pl\n",
    "\n",
    "# 1) collect non-empty Parquet files\n",
    "base_dir = \"cleaned_data_polars\"\n",
    "good_files = [\n",
    "    os.path.join(root, f).replace(\"\\\\\",\"/\")\n",
    "    for root,_,files in os.walk(base_dir)\n",
    "    for f in files\n",
    "    if f.endswith(\".parquet\") and os.path.getsize(os.path.join(root, f)) > 0\n",
    "]\n",
    "if not good_files:\n",
    "    print(\"No valid parquet files found.\")\n",
    "    exit(1)\n",
    "\n",
    "# 2) eager-read + relaxed concat\n",
    "dfs = [pl.read_parquet(f) for f in good_files]\n",
    "df = pl.concat(dfs, how=\"vertical_relaxed\")\n",
    "\n",
    "# 3) count rows\n",
    "print(f\"Total reviews: {df.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e266a",
   "metadata": {},
   "source": [
    "# Cleaning using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89994864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, as_completed\n",
    "\n",
    "# ─── Suppress non-critical warnings ────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── Your cleaning funcs ───────────────────────────────────────────────────────\n",
    "def clean_review(text):\n",
    "    try:\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    try:\n",
    "        return re.sub(r'<.*?>', '', str(text))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def clean_whitespace(text):\n",
    "    try:\n",
    "        t = str(text)\n",
    "        t = re.sub(r'\\n+', ' ', t)\n",
    "        t = t.replace('\\u3000', ' ')\n",
    "        return re.sub(r'\\s+', ' ', t).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def full_clean(text):\n",
    "    t = remove_html_tags(text)\n",
    "    t = clean_review(t)\n",
    "    return clean_whitespace(t)\n",
    "\n",
    "# ─── Per-file task ─────────────────────────────────────────────────────────────\n",
    "def process_file(path, output_dir, fields):\n",
    "    df = pd.read_parquet(path)\n",
    "    for col in fields:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(full_clean)\n",
    "    out = os.path.join(output_dir, os.path.basename(path))\n",
    "    df.to_parquet(out, compression=\"snappy\")\n",
    "    return True\n",
    "\n",
    "# ─── Main ─────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir   = \"../../parquet_output_2_extras\"\n",
    "    output_dir = \"./cleaned_data_dask\"\n",
    "    html_fields = [\n",
    "        \"detailed_description\",\n",
    "        \"about_the_game\",\n",
    "        \"short_description\",\n",
    "        \"review\",\n",
    "    ]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    total = len(files)\n",
    "    print(f\"Found {total} files to clean\")\n",
    "\n",
    "    # 1) Spin up a local cluster (auto sizing)\n",
    "    client = Client()  \n",
    "\n",
    "    # 2) Submit all tasks\n",
    "    futures = [\n",
    "        client.submit(process_file, f, output_dir, html_fields)\n",
    "        for f in files\n",
    "    ]\n",
    "\n",
    "    # 3) As each future completes, update a single-line counter\n",
    "    cleaned = 0\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "            cleaned += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "        # \\r returns to line start; end='' prevents newline; flush=True forces update\n",
    "        print(f\"\\rCleaned {cleaned}/{total} files\", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n✅ All done!\")  # final newline\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0659b0",
   "metadata": {},
   "source": [
    "# Cleaning Using Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import polars as pl\n",
    "\n",
    "# ─── Suppress noisy warnings ──────────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── Raw regex strings ────────────────────────────────────────────────────────\n",
    "HTML_RX     = r\"<.*?>\"\n",
    "CLEAN_RX    = r\"[^a-zA-Z0-9\\s]\"\n",
    "WS_RX       = r\"\\s+\"\n",
    "TRIM_EDGES  = r\"^\\s+|\\s+$\"\n",
    "\n",
    "# ─── Polars expression for cleaning a single column ───────────────────────────\n",
    "def clean_expr(col_name: str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(col_name).cast(str)\n",
    "          .str.replace_all(HTML_RX, \"\")\n",
    "          .str.replace_all(CLEAN_RX, \"\")\n",
    "          .str.replace_all(WS_RX, \" \")\n",
    "          .str.replace_all(TRIM_EDGES, \"\")\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    data_dir    = \"../../parquet_output_2_extras_with_names\"\n",
    "    output_dir  = \"./cleaned_data_polars\"\n",
    "    html_fields = [\n",
    "        \"detailed_description\",\n",
    "        \"about_the_game\",\n",
    "        \"short_description\",\n",
    "        \"review\",\n",
    "    ]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    total = len(files)\n",
    "    print(f\"Found {total} files to clean\")\n",
    "\n",
    "    cleaned = 0\n",
    "    for path in files:\n",
    "        fname = os.path.basename(path)\n",
    "\n",
    "        # Read, clean, and write\n",
    "        df = pl.read_parquet(path)\n",
    "        exprs = [clean_expr(col) for col in html_fields if col in df.columns]\n",
    "        if exprs:\n",
    "            df = df.with_columns(exprs)\n",
    "        df.write_parquet(os.path.join(output_dir, fname), compression=\"snappy\")\n",
    "\n",
    "        # Update and overwrite the progress line\n",
    "        cleaned += 1\n",
    "        print(f\"\\rCleaned {cleaned}/{total} files\", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n✅ All files cleaned and saved to\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe3569",
   "metadata": {},
   "source": [
    "# EDA with dask and polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46d85fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet files...\n",
      "\n",
      "=== VOLUME PER GAME ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FrameBase.reset_index() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistogram saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaytime_hist.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[19], line 33\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== VOLUME PER GAME ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Count reviews per game\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m game_review_counts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteam_appid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Compute and sort by review count (descending)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m top_games_by_volume \u001b[38;5;241m=\u001b[39m game_review_counts\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: FrameBase.reset_index() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Analyzes Steam game review data using Dask for distributed processing.\n",
    "    Performs three key EDA tasks: review volume analysis, sentiment analysis,\n",
    "    and playtime distribution analysis.\n",
    "    \"\"\"\n",
    "    # Enable progress bar for better visibility of Dask operations\n",
    "    ProgressBar().register()\n",
    "    \n",
    "    # Load all parquet files in one call using Dask\n",
    "    print(\"Reading parquet files...\")\n",
    "    df = dd.read_parquet(\"cleaned_data_polars/*.parquet\")\n",
    "    \n",
    "    # Data preparation\n",
    "    # ---------------\n",
    "    # Handle missing values in key columns\n",
    "    df['author_playtime_forever'] = df['author_playtime_forever'].fillna(0)\n",
    "    # Convert boolean voted_up to integer for aggregation (True=1, False=0)\n",
    "    df['voted_up_int'] = df['voted_up'].astype('bool').astype('int')\n",
    "    # Convert playtime from minutes to hours\n",
    "    df['playtime_hours'] = df['author_playtime_forever'] / 60.0\n",
    "    \n",
    "    # Task 1: Volume per game\n",
    "    # ----------------------\n",
    "    print(\"\\n=== VOLUME PER GAME ===\")\n",
    "    # Count reviews per game\n",
    "    game_review_counts = df.groupby('steam_appid').size().reset_index(name='review_count')\n",
    "    # Compute and sort by review count (descending)\n",
    "    top_games_by_volume = game_review_counts.compute().sort_values('review_count', ascending=False).head(20)\n",
    "    print(\"Top 20 games by review count:\")\n",
    "    print(top_games_by_volume)\n",
    "    \n",
    "    # Task 2: Sentiment proxy – votes-up ratio\n",
    "    # --------------------------------------\n",
    "    print(\"\\n=== SENTIMENT PROXY - VOTES-UP RATIO ===\")\n",
    "    # Calculate sum of positive reviews and total count per game\n",
    "    sentiment_agg = df.groupby('steam_appid').agg({\n",
    "        'voted_up_int': ['sum', 'count']\n",
    "    }).compute()\n",
    "    \n",
    "    # Process results in pandas\n",
    "    sentiment_agg.columns = ['votes_up_sum', 'review_count']\n",
    "    sentiment_agg['positive_ratio'] = sentiment_agg['votes_up_sum'] / sentiment_agg['review_count']\n",
    "    \n",
    "    # Filter games with at least 100 reviews\n",
    "    top_sentiment = (\n",
    "        sentiment_agg[sentiment_agg['review_count'] >= 100]\n",
    "        .sort_values('positive_ratio', ascending=False)\n",
    "        .head(20)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    print(\"Top 20 games by positive ratio (minimum 100 reviews):\")\n",
    "    print(top_sentiment[['steam_appid', 'review_count', 'positive_ratio']])\n",
    "    \n",
    "    # Task 3: Play-time distributions\n",
    "    # -----------------------------\n",
    "    print(\"\\n=== PLAY-TIME DISTRIBUTIONS ===\")\n",
    "    \n",
    "    # Dask requires separate computations for different percentiles\n",
    "    playtime_stats_tasks = {\n",
    "        'mean_hours': df.groupby('steam_appid')['playtime_hours'].mean(),\n",
    "        'median_hours': df.groupby('steam_appid')['playtime_hours'].quantile(0.5),\n",
    "        'percentile_95_hours': df.groupby('steam_appid')['playtime_hours'].quantile(0.95)\n",
    "    }\n",
    "    \n",
    "    # Execute all Dask tasks and collect results\n",
    "    playtime_stats_results = {k: v.compute() for k, v in playtime_stats_tasks.items()}\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    playtime_stats_df = pd.DataFrame(playtime_stats_results)\n",
    "    playtime_stats_df = playtime_stats_df.reset_index()\n",
    "    \n",
    "    print(\"Playtime statistics per game (showing first 20):\")\n",
    "    print(playtime_stats_df.head(20))\n",
    "    \n",
    "    # Generate global histogram of playtimes\n",
    "    # ------------------------------------\n",
    "    print(\"Generating playtime histogram...\")\n",
    "    \n",
    "    # For large datasets, sample to avoid memory issues\n",
    "    estimated_size = df.shape[0].compute()\n",
    "    \n",
    "    if estimated_size > 1_000_000:\n",
    "        # Use a sampling fraction that gives us at most 1M records\n",
    "        sample_frac = min(1_000_000 / estimated_size, 1.0)\n",
    "        print(f\"Sampling {sample_frac:.2%} of data for histogram ({estimated_size:,} records)\")\n",
    "        playtime_data = df['playtime_hours'].sample(frac=sample_frac).compute()\n",
    "    else:\n",
    "        # For smaller datasets, use all data\n",
    "        playtime_data = df['playtime_hours'].compute()\n",
    "    \n",
    "    # Filter outliers for better visualization (keep playtimes under 1000 hours)\n",
    "    filtered_playtime = playtime_data[playtime_data < 1000]\n",
    "    \n",
    "    # Create histogram with 1-hour bins\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.hist(filtered_playtime, bins=np.arange(0, 1000, 1), alpha=0.75)\n",
    "    plt.title('Distribution of Playtime Hours (excluding outliers > 1000 hours)')\n",
    "    plt.xlabel('Playtime (hours)')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add vertical lines for key statistics\n",
    "    global_mean = filtered_playtime.mean()\n",
    "    global_median = filtered_playtime.median()\n",
    "    \n",
    "    plt.axvline(global_mean, color='r', linestyle='--', label=f'Mean: {global_mean:.2f} hours')\n",
    "    plt.axvline(global_median, color='g', linestyle='--', label=f'Median: {global_median:.2f} hours')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the histogram\n",
    "    plt.savefig('playtime_hist.png', dpi=300)\n",
    "    print(\"Histogram saved as 'playtime_hist.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5edbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
