{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0659b0",
   "metadata": {},
   "source": [
    "# Cleaning Using Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ed8201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16732 files to clean\n",
      "Cleaned 16733/16732 files\n",
      "✅ All files cleaned and saved to ./cleaned_data_polars_2_trial\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import polars as pl\n",
    "\n",
    "# ─── Suppress noisy warnings ──────────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── Raw regex strings ────────────────────────────────────────────────────────\n",
    "HTML_RX     = r\"<.*?>\"\n",
    "CLEAN_RX    = r\"[^a-zA-Z0-9\\s]\"\n",
    "WS_RX       = r\"\\s+\"\n",
    "TRIM_EDGES  = r\"^\\s+|\\s+$\"\n",
    "\n",
    "# ─── Polars expression for cleaning a single column ───────────────────────────\n",
    "def clean_expr(col_name: str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(col_name).cast(str)\n",
    "          .str.replace_all(HTML_RX, \"\")\n",
    "          .str.replace_all(CLEAN_RX, \"\")\n",
    "          .str.replace_all(WS_RX, \" \")\n",
    "          .str.replace_all(TRIM_EDGES, \"\")\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    data_dir    = \"../../parquet_output_2_extras_with_names\"\n",
    "    output_dir  = \"./cleaned_data_polars_2_trial\"\n",
    "    html_fields = [\n",
    "        \"detailed_description\",\n",
    "        \"about_the_game\",\n",
    "        \"short_description\",\n",
    "        \"review\",\n",
    "    ]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    total = len(files)\n",
    "    print(f\"Found {total} files to clean\")\n",
    "\n",
    "    cleaned = 1\n",
    "    for path in files:\n",
    "        fname = os.path.basename(path)\n",
    "\n",
    "        # Read, clean, and write\n",
    "        df = pl.read_parquet(path)\n",
    "        exprs = [clean_expr(col) for col in html_fields if col in df.columns]\n",
    "        if exprs:\n",
    "            df = df.with_columns(exprs)\n",
    "        df.write_parquet(os.path.join(output_dir, fname), compression=\"snappy\")\n",
    "\n",
    "        # Update and overwrite the progress line\n",
    "        cleaned += 1\n",
    "        print(f\"\\rCleaned {cleaned}/{total} files\", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n✅ All files cleaned and saved to\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082a0c3",
   "metadata": {},
   "source": [
    "# Using Dask cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77520ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16732 files to clean\n",
      "Cleaned 7500/16732 files (5.5 files/sec)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import cudf\n",
    "import re\n",
    "import numpy as np\n",
    "from dask import compute, delayed\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import psutil\n",
    "\n",
    "# ─── Suppress noisy warnings ──────────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── Raw regex strings ────────────────────────────────────────────────────────\n",
    "HTML_RX     = r\"<.*?>\"\n",
    "CLEAN_RX    = r\"[^a-zA-Z0-9\\s]\"\n",
    "WS_RX       = r\"\\s+\"\n",
    "TRIM_EDGES  = r\"^\\s+|\\s+$\"\n",
    "\n",
    "# ─── Function for cleaning text columns with cuDF ─────────────────────────────\n",
    "def clean_text_series(series):\n",
    "    # Convert to string type if not already\n",
    "    series = series.astype('str')\n",
    "    \n",
    "    # Apply regex replacements (all at once to maximize GPU utilization)\n",
    "    series = series.str.replace(HTML_RX, \"\", regex=True)\n",
    "    series = series.str.replace(CLEAN_RX, \"\", regex=True)\n",
    "    series = series.str.replace(WS_RX, \" \", regex=True)\n",
    "    series = series.str.replace(TRIM_EDGES, \"\", regex=True)\n",
    "    \n",
    "    return series\n",
    "\n",
    "def process_file_batch(file_batch, output_dir, html_fields):\n",
    "    \"\"\"Process multiple files at once to maximize GPU memory usage\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Load multiple files into memory at once\n",
    "    dataframes = []\n",
    "    filenames = []\n",
    "    \n",
    "    for path in file_batch:\n",
    "        try:\n",
    "            fname = os.path.basename(path)\n",
    "            gdf = cudf.read_parquet(path)\n",
    "            dataframes.append(gdf)\n",
    "            filenames.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError loading {path}: {str(e)}\")\n",
    "            results.append(False)\n",
    "    \n",
    "    # Process all loaded dataframes\n",
    "    for idx, gdf in enumerate(dataframes):\n",
    "        try:\n",
    "            fname = filenames[idx]\n",
    "            output_path = os.path.join(output_dir, fname)\n",
    "            \n",
    "            # Clean the HTML fields that exist\n",
    "            for col in html_fields:\n",
    "                if col in gdf.columns:\n",
    "                    if gdf[col].dtype == np.dtype('O') or gdf[col].dtype == 'string':\n",
    "                        gdf[col] = clean_text_series(gdf[col])\n",
    "            \n",
    "            # Write the cleaned DataFrame\n",
    "            gdf.to_parquet(output_path, compression=\"snappy\")\n",
    "            results.append(True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {filenames[idx]}: {str(e)}\")\n",
    "            results.append(False)\n",
    "    \n",
    "    # Clear references to free GPU memory\n",
    "    dataframes = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    data_dir    = \"../../parquet_output_2_extras_with_names\"\n",
    "    output_dir  = \"./cleaned_data_cudf\"\n",
    "    html_fields = [\n",
    "        \"detailed_description\",\n",
    "        \"about_the_game\",\n",
    "        \"short_description\",\n",
    "        \"review\",\n",
    "    ]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    total = len(files)\n",
    "    print(f\"Found {total} files to clean\")\n",
    "\n",
    "    # Start with large batch size to maximize GPU utilization\n",
    "    batch_size = 500\n",
    "    max_workers = psutil.cpu_count(logical=False)  # Use physical cores\n",
    "    \n",
    "    cleaned = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(files), batch_size):\n",
    "            try:\n",
    "                batch_files = files[i:i+batch_size]\n",
    "                \n",
    "                # Process this batch of files\n",
    "                results = process_file_batch(batch_files, output_dir, html_fields)\n",
    "                \n",
    "                # Update progress with timing information\n",
    "                successful = sum(1 for r in results if r)\n",
    "                cleaned += successful\n",
    "                elapsed = time.time() - start_time\n",
    "                files_per_second = cleaned / elapsed if elapsed > 0 else 0\n",
    "                \n",
    "                print(f\"\\rCleaned {cleaned}/{total} files ({files_per_second:.1f} files/sec)\", end=\"\", flush=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If we run out of memory, reduce batch size and retry\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    old_batch_size = batch_size\n",
    "                    batch_size = max(int(batch_size * 0.7), 10)\n",
    "                    print(f\"\\nOut of memory with batch size {old_batch_size}. Reducing to {batch_size} and retrying...\")\n",
    "                    \n",
    "                    # Force memory cleanup\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Retry this batch\n",
    "                    i -= old_batch_size\n",
    "                else:\n",
    "                    print(f\"\\nError processing batch: {str(e)}\")\n",
    "            \n",
    "            # Force memory cleanup between batches\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n✅ All files cleaned and saved to {output_dir}\")\n",
    "    print(f\"⏱️ Total processing time: {total_time:.2f} seconds ({total/total_time:.1f} files/sec average)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16396f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
